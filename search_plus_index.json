{"./":{"url":"./","title":"序言","keywords":"","body":"istio-handbook — Istio 实践手册 Istio 实践手册，从服务网格概念出发，将逐步渗透到 Istio 具体细节中来，旨在帮助 Istio 学习者、使用者快速掌握相关知识点，可作为 Istio 学习、实践手册，建议收藏！ （不断更新中……） 如果你在学习过程中，有什么建议、或者需要帮助，欢迎通过 Issues 方式一起讨论。 Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-11-14 00:48:58 "},"microservice/new-generation-microservices-architecture.html":{"url":"microservice/new-generation-microservices-architecture.html","title":"迎接新一代微服务架构","keywords":"","body":"迎接新一代微服务架构 微服务是近些年来软件架构中的热名词，也是一个很大的概念，不同人对它的理解都各不相同，甚至在早期微服务架构中出现了一批四不像的微服务架构产品，有人把单纯引入 Spring Boot、Spring Cloud 等框架的应用服务也称之为微服务架构，但这却只是将它作为服务的 Web 容器而已。 随着微服务的火热，越来越多的团队开始实践，将微服务纷纷落地，并投入生产。但随着微服务规模的不断壮大，每增加一个微服务，就可能会增加一些依赖的基础设施和第三方的配置，比如 Kafka 、Redis 实例等，相应 CI/CD 的配置也会增加或调整。 同时随着微服务数量增多、业务复杂性的提升及需求的多样性等（如，对接第三方异构系统等），服务间通信的错综复杂，一步步地将微服务变得更加臃肿，服务治理也是难上加难，而这些问题在单体架构中是很容易解决的。为此，有人开始怀疑当初微服务化是否是明智之选，甚至考虑回归到传统单体应用。 正如下图所示，PPT 中的微服务总是美好的，但现实中的微服务却是一团糟糕，想甩甩不掉，越看越糟心。难道就没有办法了么？ 图 2.1.1：现实中和PPT中的微服务对比 1、传统微服务架构面临的挑战 面对上述暴露出的问题，并在传统微服务架构下，经过实践的不断冲击，面临了更多新的挑战，综上所述，产生这些问题的原因有以下这几点： 过于绑定特定技术栈。 当面对异构系统时，需要花费大量精力来进行代码的改造，不同异构系统可能面临不同的改造。 代码侵入度过高。 开发者往往需要花费大量的精力来考虑如何与框架或 SDK 结合，并在业务中更好的深度融合，对于大部分开发者而言都是一个高曲线的学习过程。 多语言支持受限。 微服务提倡不同组件可以使用最适合它的语言开发，但是传统微服务框架，如 Spring Cloud 则是 Java 的天下，多语言的支持难度很大。这也就导致在面对异构系统对接时的无奈，或选择退而求其次的方案了。 老旧系统维护难。 面对老旧系统，很难做到统一维护、治理、监控等，在过度时期往往需要多个团队分而管之，维护难度加大。 上述这些问题在传统微服务架构中都是在所难免，我们都知道技术演进来源于实践中不断的摸索，将功能抽象、解耦、封装、服务化。 随着传统微服务架构暴露出的这些问题，将迎来新的挑战，让大家纷纷寻找其他解决方案。 2、迎来新一代微服务架构 为了解决传统微服务面临的问题，以应对全新的挑战，微服务架构也进一步演化，最终催生了Service Mesh 的出现，迎来了新一代微服务架构，也被称为“下一代微服务”。为了更好地理解 Service Mesh 的概念和存在的意义，我们来回顾一下这一演进过程。 1.1 耦合阶段 在微服务架构中，服务发现、负载均衡、熔断等能力是微服务架构中重要的组成部分。微服务化之后，服务更加的分散，复杂度变得更高，起初开发者将诸如熔断、超时等功能和业务代码封装在一起，使服务具备了网络管控的能力，如下图所示。 图 2.1.2：耦合阶段 这种方案虽然易于实现，但从设计角度来讲却存在一定的缺陷。 基础设施功能（如，服务发现，负载均衡、熔断等）和业务逻辑高度耦合。 每个微服务都重复实现了相同功能的代码。 管理困难。如果某个服务的负载均衡发生变化，则调用它的相关服务都需要更新变化。 开发者不能集中精力只关注于业务逻辑开发。 1.2 公共库 SDK 基于上面存在的问题，很容易会想到将基础设施功能设计为一个公共库 SDK，让服务的业务逻辑与这些公共功能降低耦合度，提高重复利用率，更重要的是开发者只需要关注公共库 SDK 的依赖及使用，而不必关注实现这些公共功能，从而更加专注于业务逻辑的开发，比如 Spring Cloud 框架是类似的方式。如下图所示： 图 2.1.3：公共库SDK阶段 实际上即便如此，它仍然有一些不足之处。 这些公共库 SDK 存在较为陡峭的学习成本，需要耗费开发人员一定的时间和人力与现有系统集成，甚至需要考虑修改现有代码进行整合。 这些公共库 SDK 一般都是通过特定语言实现，缺乏多语言的支持，在对现有系统整合时有一定的局限性。 公共库 SDK 的管理和维护依然需要耗费开发者的大量精力，并需专门人员来进行管理维护。 1.3 Sidecar 模式 有了上面公共库 SDK 的启发，加上跨语言问题、更新后的发布和维护等问题，人们发现更好的解决方案是把它作为一个代理，服务通过这个透明的代理完成所有流量的控制。 这就是典型的 Sidecar 代理模式，也被翻译为\"边车\"代理，它作为与其他服务通信的桥梁，为服务提供额外的网络特性，并与服务独立部署，对服务零侵入，更不会受限于服务的开发语言和技术栈，如下图所示。 图 2.1.4：Sidecar模式阶段 以 Sidecar 模式进行通信代理，实现了基础实施层与业务逻辑的完全隔离，在部署、升级时带来了便利，做到了真正的基础设施层与业务逻辑层的彻底解耦。另一方面，Sidecar 可以更加快速地为应用服务提供更灵活的扩展，而不需要应用服务的大量改造。Sidecar 可以实现以下主要功能： 服务注册。 帮助服务注册到相应的服务注册中心，并对服务做相关的健康检查。 服务路由。 当应用服务调用其它服务时，Sidecar 可以帮助从服务发现中找到相应的服务地址，完成服务路由功能。 服务治理。 Sidecar 可以完全拦截服务进出的流量，并对其进行相应的调用链跟踪、熔断、降级、日志监控等操作，将服务治理功能集中在 Sidecar 中实现。 集中管控。 整个微服务架构体系下的所有服务完全可以通过 Sidecar 来进行集中管控，完成对服务的流控、下线等。 于是，应用服务终于可以做到跨语言开发、并更专注于业务逻辑的开发。 1.4 Service Mesh 把 Sidecar 模式充分应用于一个庞大的微服务架构系统，为每个应用服务配套部署一个 Sidecar 代理，完成服务间复杂的通信，最终就会得到一个如下图所示的网络拓扑结构，这就是 Service Mesh，又称之为“服务网格“。 图 2.1.5：Service Mesh阶段 至此，迎来了新一代微服务架构——Service Mesh，它彻底解决了传统微服务架构所面临的问题。 Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-09-05 13:52:03 "},"servicemesh/introduction.html":{"url":"servicemesh/introduction.html","title":"服务网格介绍","keywords":"","body":"服务网格介绍 在上一节《迎接新一代微服务架构》中，我们知道服务网格经历了 4 个重要阶段： 图 3.1.1：服务网格的演进历程 耦合阶段：高度耦合、重复实现、维护困难，在耦合架构设计中体现的最为突出，单体架构就是典型的代表。 公共 SDK：让基础设施功能设计成为公共 SDK，提高利用率，是解藕最有效的途径，比如 Spring Cloud 就是类似的方式。但学习成本高、特定语言实现，却将一部分人拦在了门外。 Sidecar 模式：再次深度解藕，不单单功能解藕，更从跨语言、更新发布和运维等方面入手，实现对业务服务的零侵入，更解藕于开发语言和单一技术栈，实现了完全隔离，为部署、升级带来了便利，做到了真正的基础设施层与业务逻辑层的彻底解耦。另一方面，Sidecar 可以更加快速地为应用服务提供更灵活的扩展，而不需要应用服务的大量改造。 Service Mesh：把 Sidecar 模式充分应用到一个庞大的微服务架构系统中来，为每个应用服务配套部署一个 Sidecar 代理，完成服务间复杂的通信，最终就会得到一个的网络拓扑结构，这就是服务网格，又称之为“Service Mesh“。它从本质上解决了传统微服务所面临的问题。 接下来，让我们一起全面、真正的开始了解服务网格吧！ 1、云原生定义 在正式开始服务网格了解之前，我们先来看看另外一个与之相关的名词——“云原生”，因为在服务网格的技术圈子里，与之密不可分。 CNCF（Cloud Native Computing Foundation（云原生计算基金会））对云原生的定义： 云原生技术有利于各组织在公有云、私有云和混合云等新型动态环境中，构建和运行可弹性扩展的应用。云原生的代表技术包括容器、服务网格、微服务、不可变基础设施和声明式API。 图 3.1.2：云原生技术 从上图可知，服务网格是云原生体系的具体实现，是承载微服务架构理念的云原生技术形态。 微服务源自服务化架构设计理念，与敏捷开发 DevOps 理念的结合：微、小、快、独。 经过四代的技术演进，随着云计算发展到云原生阶段，服务网格则成为承载微服务理念的新一代技术形态。 当你开启服务网格的学习之路后，也就意味着已经踏入了云原生的领域。在这里，服务治理与业务逻辑逐步解耦，服务治理能力下沉到基础设施，服务网格以基础设施的方式提供无侵入的连接控制、安全、可监测性、灰度发布等治理能力，如华为云的 ASM、蚂蚁金服的 SOFAMesh 等，都是对服务网格的最佳实践。 2、服务网格定义 服务网格，又称之为 Service Mesh，作为服务间通信的基础设施层。轻量级高性能网络代理，提供安全的、快速的、可靠地服务间通讯，与实际应用部署一起，但对应用透明。应用作为服务的发起方，只需要用最简单的方式将请求发送给本地的服务网格代理，然后网格代理会进行后续的操作，如服务发现，负载均衡，最后将请求转发给目标服务。 服务网格目的是解决系统架构微服务化后的服务间通信和治理问题。服务网格由 Sidecar 节点组成，这个模式的精髓在于实现了数据面（业务逻辑）和控制面的解耦。具体到微服务架构中，即给每一个微服务实例同步部署一个 Sidecar。 图 3.1.3：服务网格部署网络结构图 在服务网格部署网络结构图中，绿色方块为应用服务，蓝色方块为 Sidecar，应用服务之间通过 Sidecar 进行通信，整个服务通信形成图中的蓝色网络连线，图中所有蓝色部分就形成了。其具备如下主要特点： 应用程序间通讯的中间层 轻量级网络代理 应用程序无感知 解耦应用程序的重试/超时、监控、追踪和服务发现 服务网格的出现解决了传统微服务框架中的痛点，使得开发人员专注于业务本身，同时，将服务通信及相关管控功能从业务中分离到基础设施层。 从云原生的视角来看，服务网格是一种云原生的、应用层的网络技术，具体体现如下： 云原生：面向弹性、（微）服务化、去中心化业务场景。 应用层：以应用为中心，关注应用的发布、监控、恢复等。 网络：关注应用组件之间的接口、流量、数据、访问安全等。 3、服务网格的功能 服务网格作为微服务架构中负责网络通信的基础设施层，具备网络处理的大部分功能。下面列举了一些主要的功能： 动态路由。 可通过路由规则来动态路由到所请求的服务，便于不同环境、不同版本等的动态路由调整。 故障注入。 通过引入故障来模拟网络传输中的问题（如延迟）来验证系统的健壮性，方便完成系统的各类故障测试。 熔断。 通过服务降级来终止潜在的关联性错误。 安全。 在服务网格上实现安全机制（如 TLS），并且很容易在基础设施层完成安全机制更新。 多语言支持。 作为独立运行且对业务透明的 Sideca 代理，很轻松地支持多语言的异构系统。 多协议支持。 同多语言一样，也支持多协议。 指标和分布式链路追踪。 概括起来，服务网格主要体现在以下 4 个方面： 可见性： 运行时指标遥测、分布式跟踪。 可管理性： 服务发现、负载均衡、运行时动态路由等。 健壮性： 超时、重试、熔断等弹性能力。 安全性： 服务间访问控制、TLS 加密通信。 4、服务网格解决的问题 从上述服务网格的定义看： 基础设施层是服务网格的定位，致力于解决微服务基础设施标准化、配置化、服务化和产品化的问题。 服务间通信是服务网格技术层面对的问题，对微服务屏蔽通信的复杂度，解决微服务的通信治理问题。 请求可靠传递是服务网格的目标。 轻量级网络代理是服务网格的部署方式。 对应用程序透明是服务网格的亮点和特色，实现对业务无侵入。 综合上述，服务网格主要解决用户如下 3 个维度的痛点需求： 完善的微服务基础设施 通过将微服务通信下沉到基础设施层，屏蔽了微服务处理各种通信问题的复杂度，形成微服务之间的抽象协议层。开发者无需关心通信层的具体实现，也无需关注 RPC 通信（包含服务发现、负载均衡、流量调度、流量降级、监控统计等）的一切细节，真正像本地调用一样使用微服务，通信相关的一起工作直接交给服务网格。 语言无关的通信和链路治理 功能上，服务网格并没有提供任何新的特性和能力，服务网格提供的所有通信和服务治理能力在服务网格之前的技术中均能找到，比如 Spring Cloud 就实现了完善的微服务 RPC 通信和服务治理支持。 服务网格改变的是通信和服务治理能力提供的方式，通过将这些能力实现从各语言业务实现中解耦，下沉到基础设施层面，以一种更加通用和标准化的方式提供，屏蔽不同语言、不同平台的差异性，有利于通信和服务治理能力的迭代和创新，使得业务实现更加方便。 服务网格避免了多语言服务治理上的重复建设，通过服务网格语言无关的通信和服务治理能力，助力于多语言技术栈的效率提升。 通信和服务治理的标准化 微服务治理层面，服务网格是标准化、体系化、无侵入的分布式治理平台。 标准化方面，Sidecar 成为所有微服务流量通信的约束标准，同时服务网格的数据平台和控制平面也通过标准协议进行交互。 体系化方面，从全局考虑，提供多维度立体的微服务可观测能力（Metric、Trace、Logging），并提供体系化的服务治理能力，如限流、熔断、安全、灰度等。 通过标准化，带来一致的服务治理体验，减少多业务之间由于服务治理标准不一致带来的沟通和转换成本，提升全局服务治理的效率。 5、服务网格的原理 服务网格的核心是数据平面（Sidecar）与控制平面（Control Plane），如下图： 图 3.1.4：Service Mesh架构图 数据平面： Sidecar，与服务部署在一起的轻量级网络代理，用于实现服务框架的各项功能（如，服务发现、负载均衡、限流熔断等），让服务回归业务本质。 数据平台可以认为是将 Spring Cloud、Dubbo 等相关的微服务框架中通信和服务治理能力独立出来的一个语言无法的进程，并且更注重通用性和扩展性。在 服务网格 中，不再将数据平面代理视为一个个独立的组件，而是将这些代理连接在一起形成一个全局的分布式网格。 在传统的微服务架构中，各种服务框架的功能（如，服务发现、负载均衡、限流熔断等）代码逻辑或多或少的都需要耦合到服务实例的代码中，给服务实例增加了很多无关业务的代码，同时带来了一定的复杂度。 有了 Sidecar 之后，服务节点只做业务逻辑自身的功能，服务之间的调用只需交给 Sidecar，由 Sidecar 完成注册服务、服务发现、请求路由、熔断限流、日志统计等业务无关功能。 在这种新的微服务架构中，所有的 Sidecar 组成在一起，就形成了服务网格。那么这个大型的服务网格并不是完全自治的，它还需要一个统一的控制节点 Control Plane。 控制平面： 是用来从全局的角度上控制 Sidecar，相当于服务网格整体的大脑，控制着 Sidecar 来实现服务治理的各项功能。比如，它负责所有 Sidecar 的注册，存储统一的路由表，帮助各个 Sidecar 进行负载均衡和请求调度；它收集所有 Sidecar 的监控信息和日志数据。 6、总结 至此，关于服务网格的介绍就到此结束，更深入的理解可结合后续具体应用、实践来加深吧。 参考资料： 全方位解读服务网格（Service Mesh）的背景和概念 云原生第9课：Istio服务网格快速入门 Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-11-21 01:22:02 "},"servicemesh/framework-contrast.html":{"url":"servicemesh/framework-contrast.html","title":"服务网格框架对比","keywords":"","body":"服务网格框架对比 当前，业界主要有以下主要几种 Service Mesh 框架，下面进行详细的说明及对比。 1、Linkerd Linkerd 是 Buoyant 公司 2016 年率先开源的高性能网络代理，是业界的第一款 Service Mesh 框架。其主要用于解决分布式环境中服务之间通信面临的一些问题，如网络不可靠、不安全、延迟丢包等问题。 Linkerd 使用Scala 语言编写，运行于 JVM，底层基于 Twitter 的 Finagle 库，并对其做了相应的扩展。最主要的是 Linkerd 具有快速、轻量级、高性能等特点，每秒以最小的延迟及负载处理万级请求，易于水平扩展。除此之外，还有以下功能： 支持多平台：可运行于多种平台，比如 Kubernetes、DC/OS、Docker，甚至虚拟机或物理机。 无缝集成多种服务发现工具。 支持多协议，如 gRPC、HTTP/1.x、HTTP/2，甚至可通过 linkerd-tcp 支持 TCP 协议。 支持与第三方分布式追踪系统 Zipkin 集成。 灵活性、扩展性高，可通过其提供的接口开发自定义插件。 目前，Linkerd和Linkerd2并行开发，其情况如下： Linkerd：Linkerd使用Scala语言编写，运行于JVM，底层基于 Twitter 的Finagle库，并对其做了相应的扩展。 Linkerd2：使用Go语言和Rust语言完全重写了Linkerd，专门用于Kubernetes。 Linkerd本身是数据平面，负责将数据路由到目标服务，同时保证数据在分布式环境中传输是安全、可靠、快速的。另外，Linkerd还包括控制平面组件Namerd，通过控制平面Namerd实现中心化管理和存储路由规则、中心化管理服务发现配置、支持运行时动态路由以及暴露Namerd API管理接口。 图 3.2.1：Linkerd架构图 控制平面 是在Kubernetes特定命名空间中运行的一组服务。这些服务可以完成各种事情：聚集遥测数据，提供面向用户的 API，向数据平面代理提供控制数据等。 由以下部分组成： Controller：由public-api容器组成，该容器为CLI和dashboard提供接口 API。 Destination：数据平面中的每个代理都使用此组件来查找将请求发送到哪里。还用于获取服务配置信息，如：路由指标，重试和超时等。 Identity：该组件提供了证书的颁发，接受来自代理的CSRs并返回正确身份签名的证书。这些证书由代理在启动时获取，并且必须在代理准备就绪之前发出。随后，它们可用于Linkerd代理之间的任何连接以实现mTLS。 Proxy Injector：是一个注入程序，每次创建一个pod时，它都会接收一个webhook请求。该注入程序检查资源以查找特定于Linkerd的注释（linkerd.io/inject: enabled）。当存在该注释时，注入器将更改容器的规范，并添加 initContainer包含代理本身的以及附属工具。 Service Profile Validator：用于在保存新服务配置文件之前先对其进行验证。 Tap：从CLI和dashboard接收请求，以实时监视请求和响应。 数据平面 由轻量级代理组成，这些代理作为sidecar容器与服务代码的每个实例一起部署。为了将服务“添加”到Linkerd服务网格，必须重新部署该服务的Pod，以在每个 Pod 中包含数据平面代理。 2、Envoy 同Linkerd一样，Envoy也是一款高性能的网络代理，于 2016 年 10 月份有 Lyft 公司开源，为云原生应用而设计，可作为边界入口，处理外部流量，此外，也作为内部服务间通信代理，实现服务间可靠通信。Envoy的实现借鉴现有生产级代理及负载均衡器，如Nginx、HAProxy、硬件负载均衡器及云负载均衡器的实践经验，同时基于C++编写及 Lyft 公司生产实践证明，Envoy性能非常优秀、稳定。 Envoy既可用作独立代理层运行，也可作为Service Mesh架构中数据平面层，因此通常Envoy跟服务运行在一起，将应用的网络功能抽象化，Envoy提供通用网络功能，实现平台及语言无法性。除此之外，还有以下功能： 优先支持HTTP/2和gRPC，同时支持Websocket和 TCP 代理。 API 驱动的配置管理方式，支持动态管理、更新配置以及无连接和请求丢失的热重启功能。 L3/L4层过滤器形成Envoy核心的连接管理功能。 通过与多种指标收集工具及分布式追踪系统集成，实现运行时指标收集、分布式追踪，提供整个系统及服务的运行时可见性。 内存资源使用率低，Sidecar是Envoy最常用的部署模式。 3、Istio Istio是由Google、IBM和Lyft发起的开源的Service Mesh框架。该项目在 2017 年推出，并在 2018 年 7 月发布了 1.0 版本。 Istio是Service Mesh目前的实现的典型代表，如果Sidecar是整个Service Mesh的数据面，那么Istio主要在控制面上做了更多的改进，Istio使用Envoy作为Sidecar，控制面相关全部使用Golang编写，性能上有了很大的提升。 Istio 首先是一个服务网格，但是Istio又不仅仅是服务网格：在 Linkerd，Envoy 这样的典型服务网格之上，Istio提供了一个完整的解决方案，为整个服务网格提供行为洞察和操作控制，以满足微服务应用程序的多样化需求。 Istio在服务网络中统一提供了许多关键功能： 流量管理：控制服务之间的流量和 API 调用的流向，使得调用更可靠，并使网络在恶劣情况下更加健壮。 可观察性：了解服务之间的依赖关系，以及它们之间流量的本质和流向，从而提供快速识别问题的能力。 策略执行：将组织策略应用于服务之间的互动，确保访问策略得以执行，资源在消费者之间良好分配。策略的更改是通过配置网格而不是修改应用程序代码。 服务身份和安全：为网格中的服务提供可验证身份，并提供保护服务流量的能力，使其可以在不同可信度的网络上流转。 除此之外，Istio针对可扩展性进行了设计，以满足不同的部署需要。 平台支持：Istio旨在在各种环境中运行，包括跨云， 预置，Kubernetes，Mesos等。最初专注于Kubernetes，但很快将支持其他环境。 集成和定制：策略执行组件可以扩展和定制，以便与现有的ACL，日志，监控，配额，审核等解决方案集成。 这些功能极大的减少了应用程序代码，底层平台和策略之间的耦合，使微服务更容易实现。 图 3.2.2：Istio架构图 Istio架构图中各个子模块功能如下： Envoy：负责各个应用服务之间通信。 Pilot：管理和配置Envoy，提供服务发现、负载均衡和智能路由，保证弹性服务（服务超时次数、重试、熔断策略）。 Mixer：信息监控检查。 Istio-Auth：提供服务和服务、用户和服务之间的认证服务，实现访问控制，解决是谁访问的是哪个 API 的问题。 其中，图中的通信代理组件为Envoy，这是Istio原生引入的，但Linkerd也能够集成对接Istio。 4、Conduit Conduit于 2017 年 12 月发布，作为由 Buoyant 继Linkerd后赞助的另外一个开源项目，作为Linkerd面向Kubernetes的独立版本。Conduit旨在彻底简化用户在Kubernetes使用服务网格的复杂度，提高用户体验，而不是像Linkerd一样针对各种平台进行优化。 Conduit的主要目标是轻量级、高性能、安全并且非常容易理解和使用。同Linkerd和Istio，Conduit也包含数据平面和控制平面，其中数据平面由Rust语言开发，使得Conduit使用极少的内存资源，而控制平面由Go语言开发。Conduit依然支持Service Mesh要求的功能，而且还包括以下功能： 超级轻量级和极快的性能。 专注于支持Kubernetes平台，提高运行在Kubernetes平台上服务的可靠性、可见性及安全性。 支持gRPC、HTTP/2和HTTP/1.x请求及所有 TCP 流量。 Conduit以极简主义架构，以零配置理念为中心，旨在减少用户与Conduit的交互，实现开箱即用。 5、对比总结 下面对上述各种 Service Mesh 框架进行简单的比较汇总，见下表所示： 功能 Linkerd Envoy Istio Conduit 代理 Finagle + Jetty Envoy Envoy Conduit 熔断 支持。基于连接的熔断器Fast Fail和基于请求的熔断器Failure Accrual。 支持。通过特定准则，如最大连接数、 最大请求数、最大挂起请求数或者最大重试数的设定。 支持。通过特定准则，如最大连接数和最大请求数等的设定。 暂不支持。 动态路由 支持。通过设置Linkerd的dtab规则实现不同版本服务请求的动态路由。 支持。通过服务的版本或环境信息实现。 支持。通过服务的版本或环境信息实现。 暂不支持。 流量分流 支持。以增量和受控的方式实现分流。 支持。以增量和受控的方式实现分流。 支持。以增量和受控的方式实现分流。 暂不支持。 服务发现 支持。支持多种服务发现机制，如基于文件的服务发现、Consul、Zookeeper、Kubernetes等。 支持。通过提供平台无关的服务发现接口实现与不同服务发现工具集成。 支持。通过提供平台无关的服务发现接口实现与不同服务发现工具集成。 只支持Kubernetes。 负载均衡 支持。提供多种负载均衡算法。 支持。提供多种负载均衡算法，如Round Robin、加权最小请求、哈希环、Maglev等。 支持。提供多种负载均衡算法，如Round Robin、加权最小请求、哈希环、Maglev等。 支持。当前只有 HTTP 请求支持基于P2C + least-loaded的负载均衡算法。 安全通信 支持 TLS。 支持 TLS。 支持 TLS。 支持TLS。 访问控制 不支持。 不支持。 支持。基于RBAC的访问控制。 暂不支持。 可见性 分布式追踪(Zipkin)、运行时指标(InfluxDB、Prometheus、statsd) 分布式追踪(Zipkin)、运行时指标(statsd) 分布式追踪(Zipkin)、运行时指标(Prometheus、statsd)、监控(NewRepic、Stackdriver) 运行时指标(Prometheus) 部署模式 Sidecar 或者 per-host 模式 Sidecar 模式 Sidecar 模式 Sidecar 模式 控制平面 Namerd 没有，但可通过 API 实现。 Pilot、Mixer、Citadel Conduit 协议支持 HTTP/1.x、HTTP/2、gRPC HTTP/1.x、HTTP/2、gRPC、TCP HTTP/1.x、HTTP/2、gRPC、TCP HTTP/1.x、HTTP/2、gRPC、TCP 运行平台 平台无关 平台无关 目前支持Kubernetes，平台无关是最终实现目标。 只支持Kubernetes。 上述任何一个 Service Mesh 框架都能够满足您的基本需求。到⽬前为⽌，Istio 具有这几个服务⽹格框架中最多的功能和灵活性，灵活性意味着复杂性，因此需要团队更为充⾜的准备。如果只想使⽤基本的 Service Mesh 治理功能，Linkerd 可能是最佳选择。如果您想⽀持同时包含 Kubernetes 和 VM 的异构环境，并且不需要 Istio 的复杂性，那么 Conduit 可能是您的最佳选择，⽬前 Istio 也提供了同时包含 Kubernetes 和 VM 的异构环境的⽀持。 Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-11-21 01:22:02 "},"architecture/istio-architecture.html":{"url":"architecture/istio-architecture.html","title":"Istio 整体架构","keywords":"","body":"Istio 整体架构 Istio 从逻辑上分为数据平面（Data Plane）和控制平面（Control Plane）。 数据平面：由整个网格内的 Sidecar 代理组成，这些代理以 Sidecar 的形式和应用服务一起部署。这些代理负责协调和控制应用服务之间的所有网络通信。每一个 Sidecar会接管进入和离开服务的流量，并配合控制平面完成流量控制等方面的功能。 控制平面：控制和管理数据平面中的 Sidecar 代理，完成配置分发、服务发现、流量路由、授权鉴权等功能，以达到对数据平面的统一管理。在 Istio 1.5 版本中，控制平面由原来分散的、独立部署的几个组件整合为一个独立的 istiod，变成了一个单进程、多模块的组织形态。同时，在 Istio 1.5 版本中，基于性能、部署方便考虑，废弃了Mixer，转而将这些功能放到了 Sidecar 中。 下图展示了组成每个平面的不同组件： 图 4.1.1：Istio架构图 1、核心组件 下面将简单的介绍一下 Istio 架构中几个核心组件的主要功能。 1.1 Proxy Proxy 位于数据平面，即：常说的 Sidecar 代理，与应用服务以 Sidecar 方式部署在同一个 Pod 中。Proxy 实际上包括 istio-proxy 和 pilot-agent 两部分，它们以两个不同的进程部署在同一个容器 istio-proxy 中。 istio-proxy Istio 的数据平面默认使用 Envoy 的扩展版本作为 Sidecar 代理（即：istio-proxy），istio-proxy 是基于 Envoy 新增了一些扩展，其代码仓库位于 istio/proxy。 注：理论上，Istio 是支持多种 Sidecar 代理，其中 Envoy 作为默认提供的数据平面，如无特殊说明在 Istio 中通常所说的 Envoy 就是 istio-proxy。 Envoy 是用 C++ 开发的高性能代理，用于协调服务网格中所有服务的入站和出站流量，是唯一与数据平面流量交互的组件。主要包括三部分能力： 动态服务发现、负载均衡、路由、流量转移。 弹性能力：如超时重试、熔断等。 调试功能：如故障注入、流量镜像等。 polit-agent pilot-agent，负责管理 istio-proxy 的整个生命周期，具体包括 istio-proxy 准备启动参数和配置文件，负责管理 istio-proxy 的启动过程、运行状态监控以及重启等。其代码仓库位于 istio/istio/pilot/cmd/pilot-agent。 部署上，isito-proxy 不是单独构建镜像，而是和 polit-agent 一起打包构建成一个镜像 istio/proxyv2，poilt-agent 将会以子进程的方式启动 istio-proxy，并监控 istio-proxy 的运行状态。 1.2 Istiod 自 Istio 1.5 版本开始，控制平面由原来分散、独立部署的三个组件（Pilot、Citadel、Galley）整合为一个独立的 istiod，变成了一个单进程、多模块的组织形态，极大的降低了原来部署的复杂度。 Pilot 负责 Istio 数据平面的 xDS 配置管理，具体包括： 服务发现、配置规则发现：为 Sidecar 提供服务发现、用于智能路由的流量管理功能（例如，A/B 测试、金丝雀发布等）以及弹性功能（超时、重试、熔断器等）。通过提供通用的流量管理模型和服务发现适配器（Service Discovery Adapter），来对接不同平台的适配层。 xDS 配置下发：提供统一的 xDS API，供 Sidecar 调用。将路由规则等配置信息转换为 Sidecar 可以识别的信息，并下发给数据平面。 注：这里实际上是指 pilot-discovery，代码仓库位于 istio/istio/pilot/cmd/pilot-discovery Citadel 负责安全证书的管理和发放，可以实现授权和认证等操作。 Citadel 并不是唯一的证书管理方式，Istio 当前支持 Citadel、Vault 和 Google 等多种证书管理方式，Citadel 是当前默认的证书管理方式。 Galley Galley 是 Istio 1.1 版本中新引入的配置管理组件，主要负责配置的验证、提取和处理等功能。其目的是将 Istio 和底层平台（如 Kubernetes）进行解耦。 在引入 Galley 之前，Istio 控制平面的各个组件需要分别对 Kubernetes 资源进行管理，包括资源的配置验证，监控资源配置变化，并针对配置变更采取相应的处理等。 2、设计目标 几个关键的设计目标形成了 Istio 的架构，这些目标对于使系统能够大规模和高性能地处理服务是至关重要的。 对应用透明性：从本质上说，对应用透明是 Service Mesh 的特性，一个合格的 Service Mesh 产品都应该具有这一特性，否则也就失去了网格产品的核心竞争力。为此，Istio 自动将自己注入到服务之间的所有网络路径中，做到对应用的透明性。Istio 使用 Sidecar 代理来捕获流量，并在不更改已部署应用程序代码的情况下，自动对网络层进行配置，以实现通过这些代理来路由流量。 可扩展性：Istio 认为，运维和开发人员随着深入使用 Istio 提供的功能，会逐渐涌现更多的需求，主要集中在策略方面。因此，为策略系统提供足够的扩展性，成为了 Istio 的一个主要的设计目标。 可移植性：考虑到现有云生态的多样性，Istio 被设计为可以支持不同的底层平台，也支持本地、虚拟机、云平台等不同的部署环境。不过从目前的情况来看，Istio 和 Kubernetes 还是有着较为紧密的依赖关系，平台无关性、可移植性将是 Istio 最终实现目标。 策略一致性：Istio 使用自己的 API 将策略系统独立出来，而不是集成到 Sidecar 中，从而允许服务根据需要直接与之集成。同时，Istio 在配置方面也注重统一和用户体验的一致性。一个典型的例子是路由规则都统一由虚拟服务来配置，可在网格内、外以及边界的流量控制中复用。 3、Istio 架构演进 从 2017 年 5 月发布以来，Istio 经历了四个重要的版本和由此划分的三个发展阶段。在不到三年的产品迭代过程中，出现了两次重大的架构变动。 0.1 版本：2017 年 5 月发布。作为第二代 Service Mesh 的开创者，宣告了 Istio 的诞生，也燃起了网格市场的硝烟与战火。 1.0 版本：发布于 2018 年 7 月，对外宣传生产环境可用。从 0.1 到 1.0 版本，开发时间经历了一年多，但持续的发布了多个 0.x 版本，这一阶段处于快速迭代期。 1.1 版本：发布于 2019 年 3 月，号称企业级可用的版本。一个小的版本号变化居然耗费了半年之久，其主要原因是出现了第一次架构重构，这一阶段算是调整期。 1.5 版本：发布于 2020 年 3 月，再次进行架构的重建，将多组件整合为单体形态的 istiod。从 1.1 到 1.5 版本的一年中，Istio 开始遵循季节性发布，进入了产品的稳定发展期。 图 4.1.2：Istio架构演进 接下来，我们将会针对 Istio 数据平面和控制平面展开具体说明。 Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-09-30 17:35:45 "},"architecture/dataplane.html":{"url":"architecture/dataplane.html","title":"数据平面","keywords":"","body":"数据平面 Istio 数据平面的核心是以 Sidecar 方式存在的代理，该 Sidecar 将数据平面核心组件部署到单独的流程或容器中，以提供隔离和封装。Sidecar 与应用服务共享相同的生命周期，与应用服务一起创建、退出。Sidecar 与参与服务网格的所有应用服务实例一起运行，但不在同一个容器进程中，形成了服务网格的数据平面。只要应用服务想要与其他服务通信，就会通过服务 Sidecar 代理进行。 如下图所示，数据平面的 Sidecar 代理可以调节和控制应用服务之间所有的网络通信，每个应用服务 Pod 启动时会伴随启动 istio-init 和 istio-proxy 容器。其中 istio-init 容器主要功能是初始化 Pod 网络和对 Pod 设置 iptable 规则，设置完成后自动结束。istio-proxy 容器会启动两个进程：pilot-agent 以及 Sidecar 代理（如：Envoy）。pilot-agent 的作用是同步管理数据，启动并管理 Sidecar 代理服务进程，上报遥测数据，Sidecar 代理则根据管理策略完成流量管控、生成遥测数据。 图 4.2.1：Istio数据平面架构图 在 Istio 中，数据平面主要负责提供以下能力： 服务发现：探测所有可用的上游或下游服务实例。 健康检测：探测上游或下游服务实例是否健康，是否准备好接收网络流量。 流量路由：将网络请求路由到正确的上游或下游服务。 负载均衡：在对上游或下游服务进行请求时，选择合适的服务实例接收请求，同时负责处理超时、断路、重试等情况。 身份验证和授权：在 istio-agent 与 istiod 配合下，对网络请求进行身份验证、权限验证，以决定是否响应以及如何响应，使用 mTLS 或其他机制对链路进行加密等。 链路追踪：对于每个请求，生成详细的统计信息、日志记录和分布式追踪数据，以便操作人员能够理解调用路径并在出现问题时进行调试。 目前常见的数据平面实现有： Envoy：Istio 默认使用的开箱即用 Sidecar 代理，使用 C++ 开发，性能较高。 MOSN：阿里巴巴公司开源，设计类似 Envoy，使用 Go 语言开发，对多协议进行了支持。 Linkerd：一个提供弹性云端原生应用服务网格的开源项目，也是面向微服务的开源 RPC 代理，使用 Scala 开发。它的核心是一个透明代理，因此也可作为典型的数据平面的实现。 下面将对数据平面涉及到的容器或组件进行具体说明。 1、isito-init 2、istio-proxy 和 pilot-agent 3、Envoy Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-09-30 17:35:45 "},"architecture/controlplane.html":{"url":"architecture/controlplane.html","title":"控制平面","keywords":"","body":"控制平面 控制平面是控制和管理数据平面中的 Sidecar 代理，完成配置分发、服务发现、流量路由、授权鉴权等功能，以达到对数据平面的统一管理。 在 Istio 1.5 版本之前，Mixer 组件负责遥测统计（istio-telemetry）和策略控制（istio-policy），其中提供了两个接口 check 和 report，在每次 Sidecar 代理发送请求时都会去 Mixer 里 check 一次，然后 report 观测信息给 Mixer，Mixer 加工后发送给 Prometheus。因此，基于性能、部署方便的考虑，在 Istio 1.5 版本中废弃了 Mixer 组件，转而将这些功能放到了 Sidecar 中。 在 Istio 1.5 版本中，控制平面由原来分散、独立部署的几个组件整合为一个独立的 istiod，变成了一个单进程、多模块的组织形态。istiod 是新版本中最大的变化，以一个单体组件替代了原有的架构，在降低复杂度和维护难度的同时，也让易用性得到提升。需要注意的一点是，原有的多组件并不是被完全移除，而是在重构后以模块的形式整合在一起组成了 istiod。 目前，控制平面依旧延续之前组件的功能，但以模块的形式呈现在 istiod 中，包括 Pilot、Citadel、Galley 三个模块，本节将会详细介绍它们。 1、Pilot Pilot 是 Istio 中的核心组件，用于管理和配置部署在特定 Istio 服务网格中的所有 Sidecar 代理实例。它管理 Sidecar 代理之间的路由流量规则，并配置故障恢复功能，如超时、重试和熔断等。 1.1 Pilot 架构 图 4.3.1：Pilot架构图 根据上图，Pilot 有几个关键模块如下： 抽象模型（Abstract model） 为了实现对不同服务注册中心 （如，Kubernetes、Consul） 的支持，Pilot 需要对不同输入来源的数据有一个统一的存储格式，也就是抽象模型。 抽象模型中定义的关键成员包括 HostName（service 名称）、Ports（service 端口）、Address（service ClusterIP）、Resolution （负载均衡策略） 等。 平台适配器 （Platform adapters） Pilot 的实现是基于平台适配器（Platform adapters） 的，借助平台适配器 Pilot 可以实现服务注册中心数据到抽象模型之间的数据转换。 例如，Pilot 中的 Kubernetes 适配器通过 Kubernetes API Server 获得 Kubernetes 中 service 和 Pod 的相关信息，然后翻译为抽象模型提供给 Pilot 使用。 通过平台适配器模式，Pilot 还可以从 Consul 等平台中获取服务信息，还可以开发适配器将其他提供服务发现的组件集成到 Pilot 中。 xDS API Pilot 使用了一套源于 Envoy 项目的标准数据平面 API 来将服务信息和流量规则下发到数据面的 Sidecar 中。这套标准数据平面 API，也叫 xDS。 Sidecar 通过 xDS API 可以动态获取 Listener （监听器）、Route （路由）、Cluster（集群）及 Endpoint （集群成员）及 Secret（安全）配置： LDS：Listener 发现服务。Listener 监听器控制 Sidecar 启动端口的监听（目前只支持 TCP 协议），并配置 L3/L4 层过滤器，当网络连接达到后，配置好的网络过滤器堆栈开始处理后续事件。 RDS：Route 发现服务，用于 HTTP 连接管理过滤器动态获取路由配置，路由配置包含 HTTP 头部修改（增加、删除 HTTP 头部键值），virtual hosts （虚拟主机），以及 virtual hosts 定义的各个路由条目。 CDS：Cluster 发现服务，用于动态获取 Cluster 信息。 EDS：Endpoint 发现服务。用于动态维护端点信息，端点信息中还包括负载均衡权重、金丝雀状态等，基于这些信息，Sidecar 可以做出智能的负载均衡决策。 SDS：Secret 发现服务，用于运行时动态获取 TLS 证书。 通过采用该标准 API， Istio 将控制平面和数据平面进行了解耦，为多种数据平面 Sidecar 实现提供了可能性。例如蚂蚁金服开源的 Golang 版本的 Sidecar MOSN (Modular Observable Smart Network)。 用户 API（User API） Pilot 还定义了一套用户 API， 用户 API 提供了面向业务的高层抽象，可以被运维人员理解和使用。 运维人员使用该 API 定义流量规则并下发到 Pilot ，这些规则被 Pilot 翻译成数据平面的配置，再通过 xDS API 分发到 Sidecar 实例，可以在运行期对微服务的流量进行控制和调整。 通过运用不同的流量规则，可以对网格中微服务进行精细化的流量控制，如按版本分流、断路器、故障注入、灰度发布等。 1.2 Pilot 实现 图 4.3.2：Pilot实现 图中实线连线表示控制流，虚线连线表示数据流。带 [pilot] 的组件表示为 Pilot 组件，图中关键的组件如下： Discovery service：即 pilot-discovery，主要功能是从注册中心（如 Kubernetes 或者 Consul ）中获取服务信息，从 Kubernetes API Server 中获取流量规则（Kubernetes CRD Resource），并将服务信息和流量规则转化为数据平面可以处理的格式，通过标准的数据平面 API 下发到网格中的各个 Sidecar 中。 Agent：即 pilot-agent 组件，该进程根据 Kubernetes API Server 中的配置信息生成 Envoy 的配置文件，负责启动、监控 Sidecar 进程。 Proxy：既 Sidecar Proxy，是所有服务的流量代理，直接连接 pilot-discovery ，间接地从 Kubernetes 等服务注册中心获取集群中微服务的注册情况。 Service A/B：使用了 Istio 的应用，如 Service A/B，进出网络流量会被 Proxy 接管。 其中，Pilot 实际上包括 pilot-discovery 和 pilot-agent 两个组件，分别位于控制平面和数据平面。 pilot-discovery pilot-discovery 位于控制平面，扮演服务发现、控制平面到 Sidecar 之间的桥梁作用。pilot-discovery 的主要功能如下： 监控服务注册中心（如 Kubernetes）的服务注册情况。在 Kubernetes 环境下，会监控 service、endpoint、pod、node 等资源信息。 监控 Istio 控制平面信息变化，在 Kubernetes 环境下，会监控包括 RouteRule、 VirtualService、Gateway、EgressRule、ServiceEntry 等以 Kubernetes CRD 形式存在的 Istio 控制面配置信息。 将上述两类信息合并组合为 Sidecar 可以理解的配置信息，并将这些信息以 gRPC 协议提供给 Sidecar。 pilot-agent pilot-agent 位于数据平面，是一个本地代理，与 Sidecar 代理部署在一起，负责 Sidecar 服务的整个生命周期。pilot-agent 的主要功能如下： 生成 Sidecar 的配置。 负责 Sidecar 的启动与监控。 2、Citadel Citadel 是 Istio 中负责身份认证和证书管理的核心安全组件，1.5 版本之后取消了独立进程，作为一个功能模块被整合在 istiod 中。 2.1 Citadel 基本功能 总体来说，Istio 在安全架构方面主要包括以下内容： 证书签发机构（CA）负责密钥和证书管理。 API 服务器将安全配置分发给数据平面。 客户端、服务端通过代理安全通信。 Istio 的身份标识模型使用一级服务标识来确定请求的来源，它可以灵活的标识终端用户、工作负载等。在平台层面，Istio 可以使用类似于服务名称来标识身份，或直接使用平台提供的服务标识。比如 Kubernetes 的 ServiceAccount，AWS IAM 用户、角色账户等。 在身份和证书管理方面，Istio 使用 X.509 证书，并支持密钥和证书的自动轮换。从 1.1 版本开始，Istio 开始支持安全发现服务器（SDS），随着不断的完善和增强，1.5 版本 SDS 已经成为默认开启的组件。Citadel 以前有两个功能：将证书以 Secret 的方式挂载到命名空间里；通过 SDS gRPC 接口与 nodeagent（已废弃）通信。目前 Citadel 只需要完成与 SDS 相关的工作，其他功能被移动到了 istiod 中。 2.2 Citadel 工作原理 Citadel 主要包括 CA 服务器、SDS 服务器、证书密钥控制器等模块，它们的工作原理如下： CA 服务器 Citadel 中的 CA 签发机构是一个 gRPC 服务器，启动时会注册两个 gRPC 服务，一个是 CA 服务，用来处理 CSR 请求（certificate signing request）；另外一个是证书服务，用来签发证书。CA 首先通过 HandleCSR 接口处理来自客户端的 CSR 请求，对客户端进行身份验证（包括 TLS 认证和 JWT 认证），验证成功后会调用 CreateCertificate 进行证书签发。 SDS 服务器 SDS， 即安全发现服务（Secret Discovery Service），它是一种在运行时动态获取证书私钥的 API，Envoy 代理通过 SDS 动态获取证书私钥。Istio 中的 SDS 服务器负责证书管理，并实现了安全配置的自动化。相比传统的方式，使用 SDS 主要有以下优点： 无需挂载 Secret 卷； 动态更新证书，无需重启； 可以监听多个证书密钥对。 图 4.3.3：Citadel身份认证流程 目前的版本中，SDS 是默认开启的，它的工作流程如下： Envoy 通过 SDS API 发送证书和密钥请求； istio-agent 作为 Envoy 的代理，创建一个私钥和证书签名请求（CSR），并发送给 istiod； CA 机构验证收到的 CSR 并生成证书； istio-agent 将私钥和从 istiod 收到的证书通过 SDS API 发送给 Envoy； 以上流程周期性执行实现密钥和证书轮换。 证书密钥控制器 证书密钥控制器（CaSecretController）监听 istio.io/key-and-cert 类型的 Secret 资源，它会周期性的检查证书是否过期，并更新证书。 证书轮换 如果没有自动证书轮换功能，当证书过期时，就不得不重启签发，并重启代理。证书轮换解决了这一问题，提高了服务的可用性。Istio 里通过一个轮换器（Rotator）自动检查自签名的根证书，并在证书即将过期时进行更新，它本质上是一个协程（goroutine）在后台轮询实现的： 获取当前证书，解析证书的有效期并获取下一次轮换时间。 启动定时器，如果发现证书到达轮换时间，从 CA 获取最新的证书密钥对。 更新证书。 3、Galley Galley 是不直接向数据平面提供业务能力，而是在控制平面上向其他模块提供支持。 Galley 作为负责配置管理的模块，验证配置信息的格式和内容的正确性，并将这些配置信息提供给控制平面的 Pilot 使用，这样控制平面的其他模块只用和 Galley 交互，从而与底层平台解耦。使用简单的接口和 Galley 进行交互，由 Galley 负责配置验证、配置变更管理、配置源管理、多平台适配等工作。 Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-09-30 17:35:45 "},"install/istio-install.html":{"url":"install/istio-install.html","title":"Istio 安装","keywords":"","body":"Istio 安装 Istio 安装方式很多，本文采用 istioctl 命令安装，更多安装方式参考 Installation Guides 以 Istio 1.9.0 版本为例说明。 1、Istio 下载 在 Istio release 页面 https://github.com/istio/istio/releases/tag/1.9.0 下载 Istio 安装文件。 将 istio-1.9.0-linux-amd64.tar.gz 上传到安装服务器上，并解压。 tar -xvf istio-1.9.0-linux-amd64.tar.gz 将 istioctl 客户端路径增加到 path 环境变量中，使得能够直接执行 istioctl 命令。 修改当前用户的 .bash_profile 文件，将 istio 目录下的 bin 文件夹添加到 path 环境变量中，并使其生效（source .bash_profile）： # 进入当前用户目录 $ cd ~ # 修改.bash_profile文件，将istio目录下的bin文件夹添加到path中 $ vi .bash_profile # 使其生效 $ source .bash_profile 2、Istio 安装 对于本次安装，我们采用 demo 安装配置。 选择它是因为它包含了一组专为测试准备的功能集合，另外还有用于生产或性能测试的配置组合。 2.1 在线安装 安装命令如下： istioctl install --set profile=demo -y istioctl install 安装过程中需要下载相关镜像（最好能够科学上网），需耐心等待安装完成即可。 （安装失败，大多都是下载镜像失败所致，可确保能够正常下载镜像的情况下，再次执行上述安装命令。） 安装配置： 在安装 Istio 时所能够使用的内置配置文件，通过命令 istioctl profile list 可以查看有哪些内置配置。这些配置文件提供了对 Istio 控制平面和 Istio 数据平面 Sidecar 的定制内容。 您可以从 Istio 内置配置文件的其中一个开始入手，然后根据您的特定需求进一步自定义配置文件。当前提供以下几种内置配置文件： default: 根据默认的安装选项启用组件 (建议用于生产部署)。 demo: 这一配置具有适度的资源需求，旨在展示 Istio 的功能。它适合运行 Bookinfo 应用程序和相关任务。 这是通过快速开始指导安装的配置，但是您以后可以通过自定义配置 启用其他功能来探索更高级的任务。此配置文件启用了高级别的追踪和访问日志，因此不适合进行性能测试。 minimum：与默认配置文件相同，但仅安装控制平面组件。这允许您使用单独的配置文件配置控制平面和数据平面组件（例如，网关）。 external: 用于配置一个远程集群，由一个外部控制平面或通过控制平面主集群的多集群网格。 empty：不部署任何东西。这可以用作自定义配置的基本配置文件。 preview：预览配置文件包含实验性功能。这是为了探索 Istio 的新功能。不保证稳定性、安全性和性能 - 使用风险自负。 组件对应关系表： default demo minimal external empty preview istio-egressgateway ✅ istio-ingressgateway ✅ ✅ ✅ istiod ✅ ✅ ✅ ✅ 2.2 离线安装 在受网络限制的环境下，需进行离线安装。 准备所需镜像 tar 包。 在具备下载镜像的环境下，通过 docker pull 、docker save 命令制作 docker 镜像 tar 包。 导入镜像。 在 Docker 私有镜像仓库，通过 docker load 、 docker tag 、 docker push 命令，将镜像 tar 包导入私有镜像仓库。 离线安装。 执行 istioctl install 命令，并指定镜像仓库参数： istioctl install --set profile=demo --set values.global.hub=192.168.161.100/istio -y 参数 --set values.global.hub=xx，用于设置 istio 安装所需镜像的私有仓库。更多 --set 参数值参考https://istio.io/latest/docs/reference/config/istio.operator.v1alpha1/#IstioOperatorSpec 对于ARM架构，截止2022年8月，isito 官方暂未提供相应的镜像，可将 values.global.hub 设置为 ghcr.io/resf/istio 进行安装。 参考资料： Install with Istioctl Installation Configuration Profiles Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2022-08-08 18:24:11 "},"install/istio-uninstall.html":{"url":"install/istio-uninstall.html","title":"Istio 卸载","keywords":"","body":"Istio 卸载 在某些场景下，我们需要卸载 Istio,可参考本文进行卸载。 要从集群中完整卸载 Istio，运行下面卸载命令： istioctl x uninstall --purge 可选的 --purge 参数将删除所有的 Istio 资源，包括可能被其他 Istio 控制平面共享的、集群范围的资源。 或者，只删除指定的 Istio 控制平面，运行以下命令： istioctl x uninstall 或 istioctl manifest generate | kubectl delete -f - 控制平面的命名空间（例如：istio-system）默认不会删除， 如果确认不再需要，用下面命令删除它： kubectl delete namespace istio-system Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-10-12 23:03:24 "},"install/deploy-bookinfo-sample.html":{"url":"install/deploy-bookinfo-sample.html","title":"部署 Bookinfo 示例","keywords":"","body":"部署 Bookinfo 示例 部署官方 Bookinfo 示例应用。 该示例部署了一个用于演示多种 Istio 特性的应用，该应用由四个单独的微服务构成。 这个应用模仿在线书店的一个分类，显示一本书的信息。 页面上会显示一本书的描述，书籍的细节（ISBN、页数等），以及关于这本书的一些评论。 Bookinfo 应用分为四个单独的微服务： productpage：这个微服务会调用 details 和 reviews 两个微服务，用来生成页面。 details：这个微服务中包含了书籍的信息。 reviews：这个微服务中包含了书籍相关的评论。它还会调用 ratings 微服务。 ratings：这个微服务中包含了由书籍评价组成的评级信息。 reviews 微服务有 3 个版本： v1 版本不会调用 ratings 服务。 v2 版本会调用 ratings 服务，并使用 1 到 5 个黑色星形图标来显示评分信息。 v3 版本会调用 ratings 服务，并使用 1 到 5 个红色星形图标来显示评分信息。 下图展示了这个应用的端到端架构。 图 5.3.1：Bookinfo部署图 1、部署服务 进入 Istio 安装目录。 Istio 默认 自动注入 sidecar。请为 default 命名空间打上标签 istio-injection=enabled： $ kubectl label namespace default istio-injection=enabled namespace/default labeled 使用 kubectl apply -f 命令部署应用： kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml 确认所有的服务和 Pod 都已经正确的定义和启动： kubectl get service kubectl get pod 要确认 Bookinfo 应用是否正在运行，请在某个 Pod 中用 curl 命令对应用发送请求，例如 ratings： kubectl exec -it $(kubectl get pod -l app=ratings -o jsonpath='{.items[0].metadata.name}') -c ratings -- curl productpage:9080/productpage | grep -o \".*\" 2、确定 Ingress 的 IP 和端口 现在 Bookinfo 中的所有服务都启动并运行中，您需要使应用程序可以从外部访问，例如使用浏览器。可以通过 Istio Gateway 和 istio-ingress 来实现。 为应用程序定义 Ingress 网关 kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml 确认网关创建完成 $ kubectl get gateway NAME AGE bookinfo-gateway 32s 确认 Ingress 的 IP 和端口 执行如下命令，明确自身 Kubernetes 集群环境支持外部负载均衡： $ kubectl get svc istio-ingressgateway -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-ingressgateway LoadBalancer 10.108.112.232 localhost 15021:32133/TCP,80:31412/TCP,443:31507/TCP,31400:32717/TCP,15443:32369/TCP 22h 默认端口为 80。 3、确认可以从集群外部访问应用 用浏览器打开网址 http:///productpage，来浏览应用的 Web 页面。如果刷新几次应用的页面，就会看到 productpage 页面中会随机展示 reviews 服务的不同版本的效果（红色、黑色的星形或者没有显示）。reviews 服务出现这种情况是因为我们还没有使用 Istio 来控制版本的路由。 图 5.3.2：BookInfo应用页面 接下来的 Istio 学习中，可以使用此示例来验证 Istio 的流量路由、故障注入等功能。 4、卸载示例应用 当完成 Bookinfo 示例的实验后，如有需要可按照以下说明进行卸载和清理： 删除路由规则，并终止应用程序容器 samples/bookinfo/platform/kube/cleanup.sh 确认卸载 kubectl get virtualservices #-- there should be no virtual services kubectl get destinationrules #-- there should be no destination rules kubectl get gateway #-- there should be no gateway kubectl get pods #-- the Bookinfo pods should be deleted Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2022-05-24 20:43:22 "},"install/deploy-kiali.html":{"url":"install/deploy-kiali.html","title":"部署 Kiali","keywords":"","body":"部署 Kiali Istio 和几个遥测应用做了集成。 能帮你了解服务网格的结构、展示网络的拓扑结构、分析网格的健康状态等。 使用下面说明部署 Kiali 仪表板、 以及 Prometheus、 Grafana、 还有 Jaeger。 安装 Kiali 和其他插件，等待部署完成。 如果在安装插件时出错，再运行一次命令。 有一些和时间相关的问题，再运行就能解决。 $ kubectl apply -f samples/addons $ kubectl rollout status deployment/kiali -n istio-system Waiting for deployment \"kiali\" rollout to finish: 0 of 1 updated replicas are available... deployment \"kiali\" successfully rolled out 访问 Kiali 仪表板。 istioctl dashboard kiali --address 192.168.161.233 --address：Kiali 仪表板监听地址，即：用于访问的IP。 用浏览器打开网址 http://:20001/kiali，就可以访问 Kiali 仪表板。在左侧的导航菜单，选择 Graph ，然后在 Namespace 下拉列表中，选择 default 。 Kiali 仪表板展示了网格的概览、以及 Bookinfo 示例应用的各个服务之间的关系。 它还提供过滤器来可视化流量的流动。 图 5.4.1：Kiali Graph Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2022-05-24 20:43:22 "},"install/istio-upgrade.html":{"url":"install/istio-upgrade.html","title":"升级","keywords":"","body":"升级 众所周知，Istio 目前属于快速发展时期，版本的更新也是很快，Istio 框架升级也是必须要考虑的一个重要环节。目前，Istio 官方也给出多种升级方法供大家根据实际情况选择。 以 Istio 1.9.0 版本向 1.10.0 版本升级为例进行说明。 1、金丝雀升级 金丝雀升级，是一种渐进式的升级方式，可以让新老版本的 istiod 同时存在，并可通过流量控制先将一小部分流量路由到新版本的 istiod 上管控，逐步完成新版本的升级。 该种方式比较安全，也是大家比较推荐的升级方法。 首先需 下载新版本的 Istio，将其上传至服务器，并切换到新版本的目录。 注意：接下来一定要使用新版本的 istioctl 命令，否则将会升级失败！（可参考 Istio 安装 中修改 istioctl path 环境变量，或根据 istioctl 的路径使用，如：./bin/istioctl） 1.1 控制平面升级 安装灰度 canary 版本，将 revision 字段设置为 canary： istioctl install --set revision=canary --set values.global.hub=192.168.162.47/istio -y 根据 istiotcl 版本来决定升级的 canary 版本，所以需确保执行 istiotcl 命令的版本为要升级 istio 的版本。 上述执行成功后，会部署一个新的 istiod-canary，即：新版本的控制平面，该新的控制平面并不会对原有的控制平面产生影响，此时会有新、旧两个控制平面同时存在： $ kubectl get pods -n istio-system -l app=istiod NAME READY STATUS RESTARTS AGE istiod-786779888b-p9s5n 1/1 Running 0 114m istiod-canary-6956db645c-vwhsk 1/1 Running 0 1m 此外，也会同时存在 2 个 service 和 sidecar-injector： $ kubectl get svc -n istio-system -l app=istiod NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istiod ClusterIP 10.32.5.247 15010/TCP,15012/TCP,443/TCP,15014/TCP 33d istiod-canary ClusterIP 10.32.6.58 15010/TCP,15012/TCP,443/TCP,15014/TCP,53/UDP,853/TCP 12m $ kubectl get mutatingwebhookconfigurations NAME WEBHOOKS AGE istio-sidecar-injector 1 7m56s istio-sidecar-injector-canary 1 3m18s 1.2 数据平面升级 只安装 canary 版本的控制平面 istio-canary 并不会对现有的代理造成影响，要升级数据平面，需将他们指向新的控制平面，需要在 namespace 中插入 istio.io/rev 标签。 例如，想要升级 default namespace 的数据平面，需要添加 istio.io/rev 标签以指向 canary 版本的控制平面，并删除 istio-injection 标签： kubectl label namespace default istio-injection- istio.io/rev=canary 注意：istio-injection 标签必须删除，因为该标签的优先级高于 istio.io/rev 标签，保留该标签将导致无法升级数据平面。 在 namespace 的标签更新成功后，需要重启 Pod 来重新注入 Sidecar： kubectl rollout restart deployment -n default 当重启成功后，该 namespace 的 Pod 将被配置指向新的 istiod-canary 控制平面，使用如下命令查看启用新代理的 Pod： kubectl get pods -n default -l istio.io/rev=canary 同时可以使用如下命令 istioctl proxy-status 查看新 Pod 的控制平面是否为 istiod-canary 及是否为新版本号： istioctl proxy-status 目前 Istio 1.10.0 版本在金丝雀升级时，istio-egressgateway 并未升级，可能是该版本存在的 bug，期待后续版本更新。 1.3 卸载旧的控制平面 升级控制平面和数据平面之后，您可以卸载旧的控制平面。例如，以下命令可以卸载旧的控制平面 1-9-0： istioctl x uninstall --revision 1-9-0 卸载前提是，旧版本的控制平面没有被使用。 如果旧的控制平面没有 revision 版本标签，请使用其原始安装选项将其卸载，例如： istioctl x uninstall -f manifests/profiles/default.yaml 通过以下方式可以确认旧的控制平面已被移除，并且集群中仅存在新的控制平面： $ kubectl get pods -n istio-system -l app=istiod NAME READY STATUS RESTARTS AGE istiod-canary-55887f699c-t8bh8 1/1 Running 0 27m 请注意，以上说明仅删除了用于指定控制平面修订版的资源，而未删除与其他控制平面共享的群集作用域资源。要完全卸载 istio，请参阅卸载。 1.4 卸载金丝雀控制平面（回滚） 如果您想要回滚到旧的控制平面，而不是完成金丝雀升级，则可以卸载 Canary 版本： istioctl x uninstall --revision=canary 但是，在这种情况下，您必须首先手动重新安装先前版本的网关，因为卸载命令不会自动还原先前升级的网关。 确保使用与 istioctl 旧控制平面相对应的版本来重新安装旧网关，并且为避免停机，请确保旧网关已启动并正在运行，然后再进行金丝雀卸载。 2、原地升级 通过 istioctl upgrade 命令将对 Istio 进行升级。在执行升级之前，它会检查 Istio 安装是否满足升级资格标准。另外，如果它检测到 Istio 版本之间的配置文件默认值有任何更改，也会警告用户。 目前原地升级有很大的概率通不过升级检测，导致无法升级，不推荐这种升级方式。期待后续版本更好的支持。 官方原地升级文档：https://istio.io/latest/docs/setup/upgrade/in-place/ Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-10-27 00:01:31 "},"traffic/":{"url":"traffic/","title":"概述","keywords":"","body":"流量管理 流量管理，是对整个系统流量的管控，其中包括了对服务网格的入口流量、出口流量以及网格内部服务间的流量管控。 Istio 的流量路由规则可以让您轻松地控制服务之间的流量和 API 调用。 Istio 简化了服务级别属性的配置(如，熔断器、超时和重试等)，并使设置重要任务(如，A/B 测试、canary 部署和基于百分比的流量分割的分阶段部署)变得容易。 它还提供了开箱即用的故障恢复特性，帮助您的应用程序更健壮地应对依赖服务或网络的故障。 本章节将针对 Istio 中流量管理展开详细说明，通过 Istio 提供丰富的资源配置项来解决流量管控方面的需求。 Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-10-27 00:50:10 "},"traffic/crd/traffic-crd.html":{"url":"traffic/crd/traffic-crd.html","title":"资源配置","keywords":"","body":"资源配置 Istio 的流量管理是通过一系列的 CRD（Kubernetes 的自定义资源）来实现的，包括以下这些资源： VirtualService：虚拟服务，用来定义路由规则，控制请求如何被路由到某个服务。 DestinationRule：目标规则，用来配置请求策略。 Gateway：网关，在网格的入口设置负载、控制流量等。 ServiceEntry：服务入口，用来定义外部如何访问服务网格。 EnvoyFilter： Sidecar： Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-10-23 21:28:30 "},"traffic/crd/virtual-service.html":{"url":"traffic/crd/virtual-service.html","title":"VirtualService","keywords":"","body":"VirtualService VirtualService 与 DestinationRule 是流量控制最关键的两个自定义资源。在 VirtualService 中定义了一组路由规则，当流量进入时，逐个规则进行匹配，直到匹配成功后将流量转发给指定的路由地址。 图 6.2.1.1：VirtualService流程图 配置项 下图是 VirtualService 的资源配置项： 图 6.2.1.2：VirtualService 资源配置项 示例 （以 Bookinfo 示例，将 Reviews 服务路由到v1版本） apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - route: - destination: host: reviews subset: v1 --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: reviews spec: host: reviews subsets: - name: v1 labels: version: v1 配置项说明： hosts：用来配置下游访问的可寻址地址。配置一个 String[] 类型的值，可以配置多个。指定了发送流量的目标主机， 可以使用FQDN（Fully Qualified Domain Name - 全限定域名）或者短域名， 也可以一个前缀匹配的域名格式或者一个具体的 IP 地址。 match：这部分用来配置路由规则，通常情况下配置一组路由规则，当请求到来时，自上而下依次进行匹配，直到匹配成功后跳出匹配。它可以对请求的 uri、method、authority、headers、port、queryParams 以及是否对 uri 大小写敏感等进行配置。 route：用来配置路由转发目标规则，可以指定需要访问的 subset （服务子集），同时可以对请求权重进行设置、对请求头、响应头中数据进行增删改等操作。subset （服务子集）是指同源服务而不同版本的 Pod，通常在 Deployment 资源中设置不同的 label 来标识。 更多详细配置项说明参考：https://istio.io/latest/zh/docs/reference/config/networking/virtual-service/#VirtualService Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2022-05-28 21:47:56 "},"traffic/crd/destination-rule.html":{"url":"traffic/crd/destination-rule.html","title":"DestinationRule","keywords":"","body":"DestinationRule DestinationRule 是 Istio 中定义的另外一个比较重要的资源，它定义了网格中某个 Service 对外提供服务的策略及规则，包括负载均衡策略、异常点检测、熔断控制、访问连接池等。 负载均衡策略支持简单的负载策略（ROUND_ROBIN、LEAST_CONN、RANDOM、PASSTHROUGH）、一致性 Hash 策略和区域性负载均衡策略。 异常点检测配置在服务连续返回了5xx的错误时进行及时的熔断保护，避免引起雪崩效应。DestinationRule 也可以同 VirtualService 配合使用实现对同源服务不同子集服务的访问配置。 Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-10-23 21:28:30 "},"traffic/crd/gateway.html":{"url":"traffic/crd/gateway.html","title":"Gateway","keywords":"","body":"Gateway Gateway，一个运行在网格边缘的负载均衡器，定义了所有 HTTP/TCP 流量进出服务网格的统一进出口。它描述了一组对外公开的端口、协议、负载均衡、以及 SNI 配置。 Istio Gateway 包括 Ingress Gateway 与 Egress Gateway，分别用来配置网格的入口流量与出口流量。Ingress Gateway 使用 istio-ingressgateway 负载均衡器来代理流量，而 istio-ingressgateway 实际上是一个 Envoy 代理。 图 6.2.3.1：Gateway流程图 示例 一个简单示例如下： apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: my-gateway namespace: some-config-namespace spec: selector: app: my-gateway-controller servers: - port: number: 80 name: http protocol: HTTP hosts: - uk.bookinfo.com - eu.bookinfo.com tls: httpsRedirect: true # sends 301 redirect for http requests - port: number: 443 name: https-443 protocol: HTTPS hosts: - uk.bookinfo.com - eu.bookinfo.com tls: mode: SIMPLE # enables HTTPS on this port serverCertificate: /etc/certs/servercert.pem privateKey: /etc/certs/privatekey.pem - port: number: 9443 name: https-9443 protocol: HTTPS hosts: - \"bookinfo-namespace/*.bookinfo.com\" tls: mode: SIMPLE # enables HTTPS on this port credentialName: bookinfo-secret # fetches certs from Kubernetes secret - port: number: 9080 name: http-wildcard protocol: HTTP hosts: - \"ns1/*\" - \"ns2/foo.bar.com\" - port: number: 2379 # to expose internal service via external port 2379 name: mongo protocol: MONGO hosts: - \"*\" 该示例中， Gateway 被引用在 some-config-namespace 这个 Namespace 下，并使用 label my-gateway-controller 来关联部署网络代理的 Pod ，对外公开了 80、443、9443、9080、2379 端口的服务。 80 端口：附属配置的 host 为uk.bookinfo.com，eu.bookinfo.com，同时在 tls 中配置了 httpsRedirect。如果使用 HTTP1.1 协议访问将会返回 301，要求使用 HTTPS 访问，通过这种配置变相的禁止了对 uk.bookinfo.com，eu.bookinfo.com 域名的 HTTP1.1 协议的访问入口。 443 端口：提供TLS/HTTPS 的访问，表示接受 uk.bookinfo.com，eu.bookinfo.com 域名的 HTTPS 协议的访问，protocol 属性指定了协议类型。在 tls 的配置中指定了会话模式为单向 TLS（mode: SIMPLE） ，同时指定了服务端证书和私钥的存放地址。 9443 端口：提供TLS/HTTPS 的访问，与 443 端口不同的是证书不是指定存放证书文件的地址，而是通过 credentialName 属性配置从 Kubernetes 的证书管理中心拉取证书。 9080 端口：提供简单的 HTTP1.1 协议的访问。 hosts 中配置了 ns1/* 与 ns2/foo.bar.com，表示只允许ns1 Namespace 下的 VirtualService 绑定它以及 ns2 Namespace下配置了 host 为 foo.bar.com 的 VirtualService 绑定它。 2379 端口：提供 MONGO 协议的访问，允许所有 host 绑定它。 Egress Gateway 提供了对网格的出口流量进行统一管控的功能，在安装 Istio 时默认是不开启的。可以使用以下命令查看是否开启： kubectl get pod -l istio=egressgateway -n istio-system 若没有开启，使用以下命令添加。 istioctl manifest apply --set values.global.istioNamespace=istio-system \\ --set values.gateways.istio-egressgateway.enabled=true Egress Gateway 的一个简单示例如下： apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: istio-egressgateway spec: selector: istio: egressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - edition.cnn.com 可以看出，与 Ingress Gateway不同，Egress Gateway 使用有 istio: egressgateway 标签的 Pod 来代理流量，实际上这是一个 Envoy 代理。当网格内部需要访问 edition.cnn.com 这个地址时，流量将会统一先转发到 Egress Gateway 上，再由 Egress Gateway 将流量转发到 edition.cnn.com 上。 Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-10-23 21:28:30 "},"traffic/crd/service-entry.html":{"url":"traffic/crd/service-entry.html","title":"ServiceEntry","keywords":"","body":"ServiceEntry ServiceEntry ，将网格外的服务注册到 Istio 的注册表中，这样就可以把外部服务当做网格内部的服务一样进行管理和操作。包括服务发现、路由控制等，在 ServiceEntry 中可以配置 hosts，vips，ports，protocols，endpoints等。 图 6.2.4.1：ServiceEntry流程图 示例 它的一个简单示例如下： apiVersion: networking.istio.io/v1alpha3 kind: ServiceEntry metadata: name: external-svc-https spec: hosts: - api.dropboxapi.com - www.googleapis.com - api.facebook.com location: MESH_EXTERNAL ports: - number: 443 name: https protocol: TLS resolution: DNS 该示例中，定义了在网格内部使用 HTTPS 协议访问外部的几个服务的配置。通过上面配置，网格内部的服务就可以把 api.dropboxapi.com，www.googleapis.com, www.googleapis.com 这几个外部的服务当做网格内部服务去访问。MESH_EXTERNAL 表示是网格外服务，该参数会影响到服务间调用的 mTLS 认证、策略执行等。 Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-10-23 21:28:30 "},"traffic/crd/envoy-filter.html":{"url":"traffic/crd/envoy-filter.html","title":"EnvoyFilter","keywords":"","body":"EnvoyFilter EnvoyFilter 提供了一种机制来定制 Istio Pilot 生成的 Envoy 配置。使用 EnvoyFilter 来修改某些字段的值，添加特定的过滤器，甚至添加全新的 listener、cluster 等。这个功能必须谨慎使用，因为不正确的配置可能破坏整个网格的稳定性。与其他 Istio 网络对象不同，EnvoyFilter 对应用是累加生效的。对于特定命名空间中的特定工作负载，可以存在任意数量的 EnvoyFilter，并累加生效。这些 EnvoyFilter 的生效顺序如下：配置根命名空间中的所有 EnvoyFilter，其次是工作负载命名空间中的所有匹配 EnvoyFilter。 注意： 该 API 的某些方面与 Istio 网络子系统的内部实现以及 Envoy 的 xDS API 有很深的关系。虽然 EnvoyFilter API 本身将保持向后兼容，但通过该机制提供的任何 Envoy 配置应在 Istio 代理版本升级时仔细审查，以确保废弃的字段被适当地删除和替换。 当多个 EnvoyFilter 被绑定到特定命名空间的同一个工作负载时，所有补丁将按照创建顺序处理。如果多个 EnvoyFilter 的配置相互冲突，则其行为将无法确定。 要将 EnvoyFilter 资源应用于系统中的所有工作负载（sidecar 和 gateway）上，请在 config 根命名空间中定义该资源，不要使用 workloadSelector。 1、配置项 下图是 EnvoyFilter 的资源配置项： 图 6.2.5.1：EnvoyFilter 资源配置项 2、示例 2.1 前提准备 2.1.1 开启注入 服务需注入边车，对其运行命名空间添加自动注入标签 istio-injection=enabled： kubectl label namespace samples istio-injection=enabled 2.1.2 服务准备 通过服务 sleep 发起请求，调用服务 helloworld 的 /hello 完成相应功能的验证。 sleep。 kubectl apply -f samples/sleep/sleep.yaml -n samples helloworld。 1）部署服务 helloworld。 kubectl apply -f samples/helloworld/helloworld.yaml -n samples 2）部署 helloworld gateway。 kubectl apply -f samples/helloworld/helloworld-gateway.yaml -n samples 2.2 添加 HTTP 响应头 在应用程序中添加 HTTP 响应头可以提高 Web 应用程序的安全性。本示例介绍如何通过定义 EnvoyFilter 添加HTTP 响应头。 OWASP(Open Web Application Security Project) 提供了最佳实践指南和编程框架，描述了如何使用安全响应头保护应用程序的安全。HTTP 响应头的基准配置如下： HTTP响应头 默认值 描述 Content-Security-Policy frame-ancestors none; 防止其他网站进行Clickjacking攻击。 X-XSS-Protection 1;mode=block 激活浏览器的XSS过滤器（如果可用），检测到XSS时阻止渲染。 X-Content-Type-Options Nosniff 禁用浏览器的内容嗅探。 Referrer-Policy no-referrer 禁用自动发送引荐来源。 X-Download-Options noopen 禁用旧版本IE中的自动打开下载功能。 X-DNS-Prefetch-Control off 禁用对页面上的外部链接的推测性DNS解析。 Server envoy 由Istio的入口网关自动设置。 X-Powered-by 无默认值 去掉该值来隐藏潜在易受攻击的应用程序服务器的名称和版本。 Feature-Policy camera ‘none’; microphone ‘none’;geolocation ‘none’;encrypted-media ‘none’;payment ‘none’;speaker ‘none’;usb ‘none’; 控制可以在浏览器中使用的功能和API。 2.2.1 边车 具体服务上生效。 创建 Envoyfilter。 kubectl apply -f samples/envoyfilter/ef-add-response-headers-into-sidecar.yaml -n samples 注：与服务在相同的 namespace。 apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: ef-add-response-headers-into-sidecar spec: workloadSelector: # select by label in the same namespace labels: app: helloworld configPatches: # The Envoy config you want to modify - applyTo: HTTP_FILTER match: context: SIDECAR_INBOUND listener: filterChain: filter: name: \"envoy.filters.network.http_connection_manager\" subFilter: name: \"envoy.filters.http.router\" patch: operation: INSERT_BEFORE value: # lua filter specification name: envoy.lua typed_config: \"@type\": \"type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\" inlineCode: |- function envoy_on_response(response_handle) function hasFrameAncestors(rh) s = rh:headers():get(\"Content-Security-Policy\"); delimiter = \";\"; defined = false; for match in (s..delimiter):gmatch(\"(.-)\"..delimiter) do match = match:gsub(\"%s+\", \"\"); if match:sub(1, 15)==\"frame-ancestors\" then return true; end end return false; end if not response_handle:headers():get(\"Content-Security-Policy\") then csp = \"frame-ancestors none;\"; response_handle:headers():add(\"Content-Security-Policy\", csp); elseif response_handle:headers():get(\"Content-Security-Policy\") then if not hasFrameAncestors(response_handle) then csp = response_handle:headers():get(\"Content-Security-Policy\"); csp = csp .. \";frame-ancestors none;\"; response_handle:headers():replace(\"Content-Security-Policy\", csp); end end if not response_handle:headers():get(\"X-Frame-Options\") then response_handle:headers():add(\"X-Frame-Options\", \"deny\"); end if not response_handle:headers():get(\"X-XSS-Protection\") then response_handle:headers():add(\"X-XSS-Protection\", \"1; mode=block\"); end if not response_handle:headers():get(\"X-Content-Type-Options\") then response_handle:headers():add(\"X-Content-Type-Options\", \"nosniff\"); end if not response_handle:headers():get(\"Referrer-Policy\") then response_handle:headers():add(\"Referrer-Policy\", \"no-referrer\"); end if not response_handle:headers():get(\"X-Download-Options\") then response_handle:headers():add(\"X-Download-Options\", \"noopen\"); end if not response_handle:headers():get(\"X-DNS-Prefetch-Control\") then response_handle:headers():add(\"X-DNS-Prefetch-Control\", \"off\"); end if not response_handle:headers():get(\"Feature-Policy\") then response_handle:headers():add(\"Feature-Policy\", \"camera 'none';\".. \"microphone 'none';\".. \"geolocation 'none';\".. \"encrypted-media 'none';\".. \"payment 'none';\".. \"speaker 'none';\".. \"usb 'none';\"); end if response_handle:headers():get(\"X-Powered-By\") then response_handle:headers():remove(\"X-Powered-By\"); end end 验证。 1）进入 sleep 服务容器内。 $ kubectl exec -it $(kubectl get pod -n samples -l app=sleep -o jsonpath='{.items[0].metadata.name}') -c sleep -n samples sh 2）调用 /hello 接口。接口响应头中包含额外添加的响应头，则说明创建的 EnvoyFilter 生效。 $ curl -i helloworld:5000/hello HTTP/1.1 200 OK content-type: text/html; charset=utf-8 content-length: 60 server: envoy date: Wed, 03 Aug 2022 08:06:59 GMT x-envoy-upstream-service-time: 163 content-security-policy: frame-ancestors none; x-frame-options: deny x-xss-protection: 1; mode=block x-content-type-options: nosniff referrer-policy: no-referrer x-download-options: noopen x-dns-prefetch-control: off feature-policy: camera 'none';microphone 'none';geolocation 'none';encrypted-media 'none';payment 'none';speaker 'none';usb 'none'; Hello version: v1, instance: helloworld-v1-6874cd9dcd-ddnrh 2.2.2 istio-ingress ingress 上生效。 创建 Envoyfilter。 kubectl apply -f samples/envoyfilter/ef-add-response-headers-into-ingressgateway.yaml -n istio-system apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: ef-add-response-headers-into-ingressgateway spec: workloadSelector: # select by label in the same namespace labels: istio: ingressgateway configPatches: # The Envoy config you want to modify - applyTo: HTTP_FILTER match: context: GATEWAY listener: filterChain: filter: name: \"envoy.filters.network.http_connection_manager\" subFilter: name: \"envoy.filters.http.router\" patch: operation: INSERT_BEFORE value: # lua filter specification name: envoy.lua typed_config: \"@type\": \"type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\" inlineCode: |- function envoy_on_response(response_handle) function hasFrameAncestors(rh) s = rh:headers():get(\"Content-Security-Policy\"); delimiter = \";\"; defined = false; for match in (s..delimiter):gmatch(\"(.-)\"..delimiter) do match = match:gsub(\"%s+\", \"\"); if match:sub(1, 15)==\"frame-ancestors\" then return true; end end return false; end if not response_handle:headers():get(\"Content-Security-Policy\") then csp = \"frame-ancestors none;\"; response_handle:headers():add(\"Content-Security-Policy\", csp); elseif response_handle:headers():get(\"Content-Security-Policy\") then if not hasFrameAncestors(response_handle) then csp = response_handle:headers():get(\"Content-Security-Policy\"); csp = csp .. \";frame-ancestors none;\"; response_handle:headers():replace(\"Content-Security-Policy\", csp); end end if not response_handle:headers():get(\"X-Frame-Options\") then response_handle:headers():add(\"X-Frame-Options\", \"deny\"); end if not response_handle:headers():get(\"X-XSS-Protection\") then response_handle:headers():add(\"X-XSS-Protection\", \"1; mode=block\"); end if not response_handle:headers():get(\"X-Content-Type-Options\") then response_handle:headers():add(\"X-Content-Type-Options\", \"nosniff\"); end if not response_handle:headers():get(\"Referrer-Policy\") then response_handle:headers():add(\"Referrer-Policy\", \"no-referrer\"); end if not response_handle:headers():get(\"X-Download-Options\") then response_handle:headers():add(\"X-Download-Options\", \"noopen\"); end if not response_handle:headers():get(\"X-DNS-Prefetch-Control\") then response_handle:headers():add(\"X-DNS-Prefetch-Control\", \"off\"); end if not response_handle:headers():get(\"Feature-Policy\") then response_handle:headers():add(\"Feature-Policy\", \"camera 'none';\".. \"microphone 'none';\".. \"geolocation 'none';\".. \"encrypted-media 'none';\".. \"payment 'none';\".. \"speaker 'none';\".. \"usb 'none';\"); end if response_handle:headers():get(\"X-Powered-By\") then response_handle:headers():remove(\"X-Powered-By\"); end end 验证。 1）确认 ingress 地址。 $ kubectl cluster-info Kubernetes control plane is running at https://192.168.1.1:16443 To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. $ kubectl get svc -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-egressgateway ClusterIP 10.111.176.85 80/TCP,443/TCP,15443/TCP 5d4h istio-ingressgateway LoadBalancer 10.101.141.144 15021:31873/TCP,80:31064/TCP,443:30191/TCP,31400:30943/TCP,15443:31029/TCP 5d4h istiod ClusterIP 10.105.147.56 15010/TCP,15012/TCP,443/TCP,15014/TCP 5d4h 从上述结果中得知，ingress 地址为 http://192.168.1.1:31064。 2）通过 ingress 访问 hello 接口。接口响应头中包含额外添加的响应头，则说明创建的 EnvoyFilter 生效。 $ curl -i http://192.168.1.1:31064/hello HTTP/1.1 200 OK content-type: text/html; charset=utf-8 content-length: 60 server: istio-envoy date: Wed, 03 Aug 2022 07:43:03 GMT x-envoy-upstream-service-time: 159 content-security-policy: frame-ancestors none; x-frame-options: deny x-xss-protection: 1; mode=block x-content-type-options: nosniff referrer-policy: no-referrer x-download-options: noopen x-dns-prefetch-control: off feature-policy: camera 'none';microphone 'none';geolocation 'none';encrypted-media 'none';payment 'none';speaker 'none';usb 'none'; Hello version: v1, instance: helloworld-v1-6874cd9dcd-ddnrh 2.3 添加直接响应 对于发往指定服务的指定路径的http请求，不再向服务转发请求，而是立即返回固定的响应内容。 创建EnvoyFilter。 kubectl apply -f samples/envoyfilter/ef-envoy-direct-response.yaml -n samples apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: ef-envoy-direct-response spec: workloadSelector: labels: app: helloworld configPatches: - applyTo: NETWORK_FILTER match: context: SIDECAR_INBOUND listener: filterChain: filter: name: \"envoy.filters.network.http_connection_manager\" patch: operation: REPLACE value: name: envoy.filters.network.http_connection_manager typed_config: \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager stat_prefix: hello route_config: name: my_first_route virtual_hosts: - name: direct_response_service domains: [\"helloworld.samples\"] routes: - match: prefix: \"/\" direct_response: status: 200 body: inline_string: \"envoy direct response.\" http_filters: - name: envoy.filters.http.router typed_config: \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router 验证。 1）进入 sleep 服务容器内。 $ kubectl exec -it $(kubectl get pod -n samples -l app=sleep -o jsonpath='{.items[0].metadata.name}') -c sleep -n samples sh 2）调用 /hello 接口。返回结果为 envoy direct response，则说明创建的 EnvoyFilter 生效。 $ curl -i http://192.168.1.1:31064/hello HTTP/1.1 200 OK content-type: text/html; charset=utf-8 content-length: 60 server: istio-envoy date: Wed, 03 Aug 2022 07:43:03 GMT x-envoy-upstream-service-time: 3 envoy direct response. 参考 Envoy Filter 在ASM中通过EnvoyFilter添加HTTP响应头 Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2022-08-13 12:59:03 "},"traffic/crd/sidecar.html":{"url":"traffic/crd/sidecar.html","title":"Sidecar","keywords":"","body":"Sidecar 默认情况下，Istio 让每个 Envoy 代理都可以访问来自和它关联的应用服务的所有端口请求，然后转发到对应的应用服务。而通过 Sidecar 资源配置可以做更多的事情，如： 调整 Envoy 代理接受的端口和协议集。 限制 Envoy 代理可以访问的服务集合。 例如，下面的 Sidecar 配置将 bookinfo 命名空间中的所有服务配置为仅能访问运行在相同命名空间和 Istio 控制平面中的服务： apiVersion: networking.istio.io/v1alpha3 kind: Sidecar metadata: name: default namespace: bookinfo spec: egress: - hosts: - \"./*\" - \"istio-system/*\" Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-10-23 21:28:30 "},"traffic/route.html":{"url":"traffic/route.html","title":"路由","keywords":"","body":"路由 本章节用来验证服务的路由功能。 在 Istio 中服务路由完全由 VirtualService 资源完成，通过定义请求匹配方式、路由规则来完成。 基于请求内容路由 基于多版本权重路由 Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2022-06-06 14:36:49 "},"traffic/load-balancing.html":{"url":"traffic/load-balancing.html","title":"负载均衡","keywords":"","body":"负载均衡 Istio 中的负载均衡是基于 Sidecar 实现，并通过 DestinationRule 中的 loadBalancer 完成负载均衡的配置，目前支持以下负载均衡算法： 标准算法： ROUND_ROBIN：轮询算法，默认。 LEAST_CONN：权重最小请求算法。该算法选择两个随机的健康主机，并选择活动请求较少的主机。 RANDOM：随机算法。该算法选择一个随机的健康主机。如果未配置运行状况检查策略，则随机负载均衡器的性能通常比轮询更好。 PASSTHROUGH：该算法将连接转发到调用者请求的原始 IP 地址，而不进行任何形式的负载平衡。需谨慎使用，它适用于特殊场景。 consistentHash：一致 Hash 算法。该算法可提供基于 HTTP 头、Cookie 等一致 Hash 算法，仅适用于HTTP 请求，常作为基于会话保持的负载均衡算法。 localityLbSetting：地域负载均衡。提供了地域感知的能力，简单说来，就是在分区部署的较大规模的集群，或者公有云上，Istio 负载均衡可以根据节点的区域标签，对调用目标做出就近选择。这些区域是使用任意标签指定的，这些标签以{region} / {zone} / {sub-zone}形式指定区域的层次结构。 更多关于 DestinationRule 中的 loadBalancer 具体配置说明可参考：LoadBalancerSettings。 例如，采用 ROUND_ROBIN 轮询负载均衡策略将流量转发到 ratings 服务。 apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: bookinfo-ratings spec: host: ratings.prod.svc.cluster.local trafficPolicy: loadBalancer: simple: ROUND_ROBIN Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-10-30 22:37:19 "},"traffic/traffic-shadow.html":{"url":"traffic/traffic-shadow.html","title":"流量镜像","keywords":"","body":"流量镜像 流量镜像（Mirroring / traffic-shadow），也称为影子流量，是一个以尽可能低的风险为生产带来变化的强大的功能。镜像会将实时流量的副本发送到镜像服务。镜像流量发生在主服务的关键请求路径之外。 …… Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-10-30 22:37:19 "},"traffic/ingress.html":{"url":"traffic/ingress.html","title":"Ingress","keywords":"","body":"Ingress Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-10-27 00:50:10 "},"traffic/egress.html":{"url":"traffic/egress.html","title":"Egress","keywords":"","body":"Egress Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-10-27 00:50:10 "},"traffic/circuit-breaking.html":{"url":"traffic/circuit-breaking.html","title":"熔断","keywords":"","body":"熔断 熔断（Circuit Breaker），是指当服务到达系统负载阈值时，为避免整个软件系统不可用，而采取的一种主动保护措施。例如，熔断应用于金融领域，指当股指波幅达到规定的熔断点时，交易所为控制风险采取的暂停交易措施。 对于微服务系统而言，熔断尤为重要，它可以使系统在遭遇某些模块故障时，通过服务降级等方式来提高系统核心功能的可用性，得以应对来自故障、潜在峰值或其他未知网络因素的影响。 在 Istio 中也具备了熔断的基本功能。 1、熔断实现 通过配置上游主机的 DestinationRule 来实现，当我们在DestinationRule中配置了熔断(ConnectionPoolSettings、OutlierDetection）时，上游服务的Sidecar代理将会根据配置进行阻止。 分为两种类型： 高并发熔断（connectionPool）： 限制最大并发访问数，超过并发限制后，返回503请求过多的错误异常。 控制请求的最大数量，挂起请求，重试或者超时等。通过设置连接池 connectionPool的各项配置，来完成熔断，其中分为 TCP 和 HTTP 两类： TCP：HTTP 和 TCP 上游连接的设置。 MaxConnections：到目标主机的 HTTP/TCP 最大连接数量。 ConnectTimeout：TCP 连接超时时间，默认单位秒。 TcpKeepalive HTTP：用于 TCP1.1/HTTP2/gRPC 连接的设置。 Http1MaxPendingRequests：HTTP 请求 pending 状态的最大请求数，从应用容器发来的 HTTP 请求的最大等待转发数，默认是1024。 Http2MaxRequests：后端请求的最大请求数，默认是1024。 MaxRequestsPerConnection：在一定时间内限制对后端服务发起的最大请求数，如果超过了这个限制，就会开启限流。如果将这一参数设置为 1 则会禁止 keepalive 特性。 MaxRetries：最大重试次数，默认为3。 IdleTimeout：上游连接池连接的空闲超时。空闲超时被定义为没有活动请求的时间段。如果未设置，则没有空闲超时。当达到空闲超时时，连接将被关闭。注意，基于请求的超时意味着HTTP/2 ping 将无法保持有效连接。适用于 HTTP1.1 和 HTTP2 连接。 H2UpgradePolicy 异常服务熔断（outlierDetection）： 根据服务异常访问次数，在时间窗口 baseEjectionTime 内超过异常次数 consecutive5xxErrors 后，触发熔断，将服务实例剔除。在下个时间窗口，再次判断异常访问次数，触发新一轮的熔断。 用来控制从负载均衡池中剔除不健康的实例，可以设置最小逐出时间和最大逐出百分比。 consecutive5xxErrors interval baseEjectionTime 高并发熔断示例 验证超过设定并发后，触发的熔断现象。（返回503请求过多的错误异常） 参考：https://istio.io/latest/zh/docs/tasks/traffic-management/circuit-breaking/ 以官方httpbin 示例验证。 部署httpbin 示例。 $ kubectl apply -f samples/httpbin/httpbin.yaml serviceaccount/httpbin unchanged service/httpbin created deployment.apps/httpbin created 配置熔断器。 创建一个DestinationRule，并配置熔断策略： $ kubectl apply -f - 测试客户端。 创建客户端程序以发送流量到 httpbin 服务。把 Fortio 作为负载测试的客户端，其可以控制连接数、并发数及发送 HTTP 请求的延迟。通过 Fortio 能够有效的触发前面在 DestinationRule 中设置的熔断策略。 （1）部署 Fortio： $ kubectl apply -f samples/httpbin/sample-client/fortio-deploy.yaml service/fortio created deployment.apps/fortio-deploy created （2）登入客户端 Pod 并使用 Fortio 工具调用 httpbin 服务，验证是否请求成功。 $ kubectl exec -it $(kubectl get pod | grep fortio | awk '{ print $1 }') -c fortio -- /usr/bin/fortio curl -quiet http://httpbin:8000/get HTTP/1.1 200 OK server: envoy date: Tue, 13 Apr 2021 :47:00 GMT content-type: application/json access-control-allow-origin: * access-control-allow-credentials: true content-length: 445 x-envoy-upstream-service-time: 36 { \"args\": {}, \"headers\": { \"Content-Length\": \"0\", \"Host\": \"httpbin:8000\", \"User-Agent\": \"istio/fortio-0.6.2\", \"X-B3-Sampled\": \"1\", \"X-B3-Spanid\": \"824fbd828d809bf4\", \"X-B3-Traceid\": \"824fbd828d809bf4\", \"X-Ot-Span-Context\": \"824fbd828d809bf4;824fbd828d809bf4;0000000000000000\", \"X-Request-Id\": \"1ad2de20-806e-9622-949a-bd1d9735a3f4\" }, \"origin\": \"127.0.0.1\", \"url\": \"http://httpbin:8000/get\" } 触发熔断。 在 DestinationRule 配置中，定义了 maxConnections: 1 和 http1MaxPendingRequests: 1。 这些规则意味着，如果并发的连接和请求数超过一个，在 istio-proxy 进行进一步的请求和连接时，后续请求或连接将被拒绝。 发送并发数为 3 的连接（-c 3），请求 30 次（-n 30）： $ kubectl exec -it $(kubectl get pod | grep fortio | awk '{ print $1 }') -c fortio -- /usr/bin/fortio load -c 3 -qps 0 -n 30 -loglevel Warning http://httpbin:8000/get Fortio 0.6.2 running at 0 queries per second, 2->2 procs, for 5s: http://httpbin:8000/get Starting at max qps with 3 thread(s) [gomax 2] for exactly 30 calls (10 per thread + 0) 23:51:51 W http.go:617> Parsed non ok code 503 (HTTP/1.1 503) 23:51:51 W http.go:617> Parsed non ok code 503 (HTTP/1.1 503) 23:51:51 W http.go:617> Parsed non ok code 503 (HTTP/1.1 503) 23:51:51 W http.go:617> Parsed non ok code 503 (HTTP/1.1 503) 23:51:51 W http.go:617> Parsed non ok code 503 (HTTP/1.1 503) 23:51:51 W http.go:617> Parsed non ok code 503 (HTTP/1.1 503) 23:51:51 W http.go:617> Parsed non ok code 503 (HTTP/1.1 503) 23:51:51 W http.go:617> Parsed non ok code 503 (HTTP/1.1 503) 23:51:51 W http.go:617> Parsed non ok code 503 (HTTP/1.1 503) 23:51:51 W http.go:617> Parsed non ok code 503 (HTTP/1.1 503) 23:51:51 W http.go:617> Parsed non ok code 503 (HTTP/1.1 503) Ended after 71.05365ms : 30 calls. qps=422.22 Aggregated Function Time : count 30 avg 0.0053360199 +/- 0.004219 min 0.000487853 max 0.018906468 sum 0.160080597 # range, mid point, percentile, count >= 0.000487853 0.001 0.002 0.003 0.004 0.005 0.006 0.007 0.008 0.009 0.014 0.018 从上述结果来看，只有63.3 %的请求成功，其余全部熔断拦截。 Code 200 : 19 (63.3 %) Code 503 : 11 (36.7 %) 另外，通过查询 istio-proxy 状态以了解更多熔断详情: $ kubectl exec $(kubectl get pod | grep fortio | awk '{ print $1 }') -c istio-proxy -- pilot-agent request GET stats | grep httpbin | grep pending cluster.outbound|80||httpbin.springistio.svc.cluster.local.upstream_rq_pending_active: 0 cluster.outbound|80||httpbin.springistio.svc.cluster.local.upstream_rq_pending_failure_eject: 0 cluster.outbound|80||httpbin.springistio.svc.cluster.local.upstream_rq_pending_overflow: 11 cluster.outbound|80||httpbin.springistio.svc.cluster.local.upstream_rq_pending_total: 20 可以看到 upstream_rq_pending_overflow 值 11，这意味着，目前为止已有 11 个调用被标记为熔断。 异常服务熔断 验证在设定熔断窗口内，服务异常次数超过设定值时，触发的异常服务熔断现象。 Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2023-06-13 17:53:22 "},"traffic/ratelimit.html":{"url":"traffic/ratelimit.html","title":"限流","keywords":"","body":"限流 参考资料： https://www.aboutwayfair.com/tech-innovation/understanding-envoy-rate-limits https://istio.io/latest/docs/tasks/policy-enforcement/rate-limit/ Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2022-08-13 13:54:16 "},"traffic/fault-injection.html":{"url":"traffic/fault-injection.html","title":"故障注入","keywords":"","body":"故障注入 故障注入（Fault Injection），即：故障测试，是 Istio 提供了一种无侵入式的故障注入机制，让开发测试人员在不用调整服务程序的前提下，通过配置即可完成对服务的异常模拟，完成对系统的定向错误测试。 通过引入故障来模拟网络传输中的问题（如延迟）来验证系统的健壮性，方便完成系统的各类故障测试。通过配置上游主机的 VirtualService 来实现，当我们在 VirtualService 中配置了故障注入时，上游服务的 Sidecar 代理在拦截到请求之后就会做出相应的响应。 目前，Istio 提供两种类型的故障注入，abort 类型与 delay 类型。 abort：非必配项，配置一个 Abort 类型的对象。用来注入请求异常类故障。简单的说，就是用来模拟上游服务对请求返回指定异常码时，当前的服务是否具备处理能力。它对应于 Envoy 过滤器中的 config.filter.http.fault.v2.FaultAbort 配置项，当 VirtualService 资源应用时，Envoy 将会该配置加载到过滤器中并处理接收到的流量。 delay：非必配项，配置一个 Delay 类型的对象。用来注入延时类故障。通俗一点讲，就是人为模拟上游服务的响应时间，测试在高延迟的情况下，当前的服务是否具备容错容灾的能力。它对应于 Envoy 过滤器中的 config.filter.fault.v2.FaultDelay 配置型，同样也是在应用 Istio 的 VirtualService 资源时，Envoy 将该配置加入到过滤器中。 实际上，Istio 的故障注入是基于 Envoy 的 config.filter.http.fault.v2.HTTPFault 过滤器实现的，它的局限性也来自于 Envoy 故障注入机制的局限性。对于 Envoy 的 HttpFault 的详细介绍请参考Envoy 文档。对比 Istio 故障注入的配置项与 Envoy 故障注入的配置项，不难发现，Istio 简化了对于故障控制的手段，去掉了 Envoy 中通过 HTTP header 控制故障注入的配置。 HTTPFaultInjection.Abort： httpStatus：必配项，是一个整型的值。表示注入 HTTP 请求的故障状态码。 percentage：非必配项，是一个 Percent 类型的值。表示对多少请求进行故障注入。如果不指定该配置，那么所有请求都将会被注入故障。 HTTPFaultInjection.Delay： fixedDelay：必配项，表示请求响应的模拟处理时间。格式为：1h/1m/1s/1ms， 不能小于 1ms。 percentage：非必配项，是一个 Percent 类型的值。表示对多少请求进行故障注入。如果不指定该配置，那么所有请求都将会被注入故障。 Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-10-30 22:37:19 "},"traffic/multi-protocol.html":{"url":"traffic/multi-protocol.html","title":"多协议","keywords":"","body":"多协议 Istio 支持代理任何的 TCP 流量，其中包括 HTTP、HTTPS、gRPC 以及 TCP 协议。但为了提供其他的能力，比如路由和丰富的指标，使用什么协议必须被确定。协议可通过自动检测和手动配置方式来确定。 基于非 TCP 的协议（例如，UDP），不能被代理直接使用。在不受 Istio 代理的任何拦截下，这些协议仍可正常继续使用，但不能在仅代理的组件中（例如，Ingress 或 Egress）使用。 可通过以下两种方式配置确定： 自动协议选择： Istio 可以自动检测 HTTP 和 HTTP/2 的流量。如果无法自动确定协议，则流量将会被当作普通 TCP 协议的流量对待。 这个特性是默认开启的。通过设置这些安装选项可以将其关闭： （位于istiod Pod 中的 pilot-discovery 容器） --set values.pilot.enableProtocolSniffingForOutbound=false 为出站监听器禁用协议检测。 --set values.pilot.enableProtocolSniffingForInbound=false 为入站监听器禁用协议检测。 手动协议选择： 可以在 Service 定义中手动指定协议。可以通过两种方式进行配置： 通过端口的名称：name: [-]。 在 Kubernetes 1.18+ 中，按 appProtocol 字段：appProtocol: 。 Service 定义中支持以下协议： http http2 https tcp tls grpc grpc-web mongo mysql* redis* udp 标注 * 的这些协议默认是处于禁用状态，要启用它们，需配置相应的Pilot环境变量。 下面是一个 Service 定义示例，通过 appProtocol 定义 mysql 的端口、通过名称定义 http 的端口： kind: Service metadata: name: myservice spec: ports: - number: 3306 name: database appProtocol: mysql - number: 80 name: http-web 1、协议实现 Istio 的多协议是通过定义 Service 完成配置，并由 Sidecar （例如，Envoy）完成具体协议的实现。 2、示例 Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2022-08-13 13:54:16 "},"traffic/gray-release.html":{"url":"traffic/gray-release.html","title":"灰度发布","keywords":"","body":"灰度发布 灰度发布，又叫做“金丝雀发布”，是指选择一小部分用户作为新版本的测试对象，当新版本运行稳定后，再逐步将更多的流量切换到新版本，直到将 100% 的流量都切换到新版本上，最后关闭剩下的老版本服务，完成灰度发布。 灰度发布，用来实现业务从老版本到新版本的平滑过渡，并避免升级过程中出现的问题对用户造成的影响。 “金丝雀发布”，来源于矿工们用金丝雀对矿井进行空气测试的做法。以前矿工挖煤的时候，矿工下矿井前会先把金丝雀放进去，或者挖煤的时候一直带着金丝雀。金丝雀对甲烷和一氧化碳浓度比较敏感，会先报警。所以大家都用“金丝雀”来搞最先的测试。 例如，下图中，左下方的少部分用户就被当作“金丝雀”来用于测试新上线的 1.1 版本。如果新版本出现问题，“金丝雀”们会报警，但不会影响其他用户业务的正常运行。 图 6.14.1：灰度发布 灰度发布的流程如下： 准备和生产环境隔离的“金丝雀”服务器。 将新版本的服务部署到“金丝雀”服务器上。 对“金丝雀”服务器上的服务进行自动化和人工测试。 测试通过后，将“金丝雀”服务器连接到生产环境，将少量生产流量导入到“金丝雀”服务器中。 如果在线测试出现问题，则通过把生产流量从“金丝雀”服务器中重新路由到老版本的服务的方式进行回退，修复问题后重新进行发布。 如果在线测试顺利，则逐渐把生产流量按一定策略逐渐导入到新版本服务器中。 待新版本服务稳定运行后，删除老版本服务。 1、Istio 中灰度发布的实现 灰度发布的实现，核心技术是要提供一种机制满足多不版本同时在线，并能够灵活配置规则给不同的版本分配流量。 Istio 本身并没有关于灰度发布的规则定义，灰度发布只是流量管控规则的一种典型应用，在进行灰度发布时，只要写个简单的流量规则配置（ VirtualService 规则配置）即可。 Istio 在每个 Pod 里都注入了一个 Envoy，因而只要在控制面配置分流策略，对目标服务发起访问的每个 Envoy 便都可以执行流量策略，完成灰度发布功能。 Istio 灰度发布支持两类灰度策略： 基于流量比例的策略：配置 VirtualService 中的路由权重 weight 来实现基于流量比例的灰度发布。 基于请求内容的策略：配置 VirtualService 中的路由请求头 headers 来实现基于请求内容的策略。该策略是非常灵活的，比如某个特性是专门为 Mac 操作系统开发的，则在该版本的流量策略中需要匹配请求方的操作系统。浏览器、请求的 Headers 等请求内容在 Istio 中都可以作为灰度发布的特征条件。 采用 kubernetes 的滚动升级(rolling update)功能也可以实现不中断业务的应用升级，但滚动升级是通过逐渐使用新版本的服务来替换老版本服务的方式对应用进行升级，在滚动升级不能对应用的流量分发进行控制，因此无法采用受控地把生产流量逐渐导流到新版本服务中，也就无法控制服务升级对用户造成的影响。 2、灰度发布示例 下面采用 Istio 官方提供的 BookInfo 示例来验证灰度发布的流程，实现将 reviews 服务从 v1 版本逐渐灰度替换到 v2 版本，如将流量从 20% 逐步路由到 v2 版本，如下图所示： 图 6.14.2：灰度发布示例图 部署 v1 版本的服务 部署 BookInfo 示例中所有服务（productpage、details、reviews-v1、ratings） 1）只部署 revices 服务的 v1 版本，则需将 samples/bookinfo/platform/kube/bookinfo.yaml 中关于 reviews 服务的 v2 、v3版本去除，通过 kubectl apply 命令部署： kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml 2）通过 kubectl 命令行确认 pod 部署，可以看到只有 V1 版本的服务： $ kubectl get pods …… 3）为了能够外部访问，则为应用程序定义 Ingress 网关： kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml 4）用浏览器打开网址 http:///productpage，（EXTERNAL-IP 是 istio-ingress 的 External IP）来浏览应用的 Web 页面。由于 V1 版本的 reviews 服务并不会调用 rating 服务，因此可以看到 Product 页面显示的是不带星级的评价信息。 部署 v2 版本的 reviews 服务 1）部署 reviews 服务的 v2 版本之前，需要创建路由规则（VirtualService、DestinationRule），确保将所有流量都路由到 v1，避免新版本 v2 对线上用户造成影响。 ① 根据 samples/bookinfo/networking/virtual-service-all-v1.yaml 创建 VirtualService，将请求都路由到所有服务的 v1 版本： apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: productpage spec: hosts: - productpage http: - route: - destination: host: productpage subset: v1 --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - route: - destination: host: reviews subset: v1 --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: ratings spec: hosts: - ratings http: - route: - destination: host: ratings subset: v1 --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: details spec: hosts: - details http: - route: - destination: host: details subset: v1 --- 通过 kubectl apply 命令创建该 VirtualService： kubectl apply -f samples/bookinfo/networking/virtual-service-all-v1.yaml ② 根据 samples/bookinfo/networking/destination-rule-all.yaml 创建 DestinationRule，配合 VirtualService 实现流量路由到 reviews 服务的 v2 版本，将其中无关内容删除，最终配置如下： apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: productpage spec: host: productpage subsets: - name: v1 labels: version: v1 --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: reviews spec: host: reviews subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: ratings spec: host: ratings subsets: - name: v1 labels: version: v1 --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: details spec: host: details subsets: - name: v1 labels: version: v1 --- 通过 kubectl apply 命令创建该 DestinationRule： kubectl apply -f samples/bookinfo/networking/destination-rule-all.yaml 2）部署 reviews 服务的 v2 版本： kubectl apply -f samples/bookinfo/platform/kube/bookinfo-reviews-v2.yaml 此时系统中部署了 v1 和 v2 两个版本的 reviews 服务，但所有的业务流量都被路由到 reviews 服务的 v1 版本。 将测试流量导入到 v2 版本的 reviews 服务 基于 istio 请求内容的路由规则策略，将部分用户的流量切换到 reviews 服务的 v2 版本，以最小化模拟测试对已上线业务的影响。 1）修改 reviews 的 VirtualService的路由规则，将用户名为 jason 的流量路由到 reviews 服务的 v2 版本，即：可使用示例中的 samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml，内容如下： apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - match: - headers: end-user: exact: jason route: - destination: host: reviews subset: v2 - route: - destination: host: reviews subset: v1 通过 kubectl apply 命令创建该 VirtualService： kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml 2）以用户 jason 登录 productpage 页面，可以看到 reviews 服务的 V2 版本带星级的评价页面。而注销用户 jason，即：不登录用户，则是 v1 版本无星级的评价页面。由此，说明基于 istio 请求内容的路由规则策略生效，实现了将部分用户的流量切换到了 v2 版本。 将部分流量路由到 v2 版本的 reviews 服务 在线上模拟测试完成后，如果系统测试情况良好，可以通过规则将一部分用户流量导入到 v2 版本的服务中，进行小规模的灰度测试。 修改 reviews 的 VirtualService的路由规则，将 20% 的流量路由到 v2 版本。 备注：本例只是描述原理，因此为简单起见，将 20% 流量路由v2 版本，在实际操作中，更可能是先导入较少流量，然后根据监控的新版本运行情况将流量逐渐导入，如采用 5%，10%，20%，50% …的比例逐渐导入。 即：可使用示例中的 samples/bookinfo/networking/virtual-service-reviews-80-20.yaml，内容如下： apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - route: - destination: host: reviews subset: v1 weight: 80 - destination: host: reviews subset: v2 weight: 20 通过 kubectl apply 命令创建该 VirtualService： kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-80-20.yaml 此时，会有 20% 的流量路由到 v2 版本。 将所有生产流量路由到 v2 版本的 reviews 服务 如果新版本 v2 的服务运行正常，则可以将所有流量路由到 v2 版本。 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - route: - destination: host: reviews subset: v2 删除 v1 版本的 reviews 服务 待 v2 版本上线稳定运行后，删除 v1 版本的 reviews 服务。 （ 指 v1 版本的 reviews 服务的 Pod 名。 ） kubectl delete pod Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-10-30 22:37:19 "},"security/":{"url":"security/","title":"安全","keywords":"","body":"安全 将单体应用拆分为多个微服务，使系统具备了更好的灵活性、可伸缩性及功能复用的能力，但是服务间的调用却变得更加复杂、频繁。因此，微服务面对安全问题有着特殊的需求： 为了抵御中间人攻击，需要流量加密。 为了提供灵活的服务访问控制，需要双向 TLS 和细粒度的访问策略。 要确定谁在什么时候做了什么，需要审计工具。 对于 Istio 在安全性方面，也提供了全面的安全解决方案来解决上述这些问题，以确保服务能够在任何地方、任何场景下安全的运行。 Istio 安全功能提供强大的身份，强大的策略，透明的 TLS加密，认证，授权和审计工具来保护你的服务和数据。Istio 安全的目标是： 默认安全：应用程序代码和基础设施无需更改。 深度防御：与现有安全系统集成以提供多层防御。 零信任网络：在不受信任的网络上构建安全解决方案。 1、安全基础 在开始 Istio 安全性讨论之前，有必要先了解一下关于 Istio 安全层面的一些基础知识及相关概念，便于后续更深入的理解。 认证和授权： 在通信安全方面有两个最基本的概念：认证（Authentication）与授权（Authorization）。 认证是解决身份确认的问题，认证过程类似于身份证验证的过程。授权是解决权限管理的问题，只有拥有相应的权限才能进行权限范围内的操作。 身份： 身份（identity）是通信安全的基础，安全认证和授权过程中都会用到。在服务间通信开始阶段，通信双方需要交换包含各自身份标识的证书，并进行证书的双向验证。在客户端通过校验证书中的身份标识来确认这是否是它要访问的目标服务，而服务端则在认证客户端身份的同时，还能对该用户进行授权、审计等更为复杂的操作。 Istio 身份模型使用 service identity （服务身份）来确定一个请求源端的身份。这种模型有极好的灵活性和粒度，可以用服务身份来标识用户、单个工作负载或一组工作负载。在没有服务身份的平台上，Istio 可以使用其它可以对服务实例进行分组的身份，例如服务名称。 Istio 身份模型非常开放，可以复用业界主流云平台的服务身份，例如： Kubernetes：Kubernetes Service Account GCP：GCP Service Account AWS： AWS IAM user/role account 用户自定义服务账户。 2、安全实现 Istio中安全性主要是由控制平面中的 Citadel 实现，涉及有多个组件： 用于密钥和证书管理的证书颁发机构（CA）。 配置 API 服务器分发给代理： 认证策略 授权策略 安全命名信息 Sidecar 和边缘代理作为 Policy Enforcement Points(PEPs) 以保护客户端和服务器之间的通信安全。 一组 Envoy 代理扩展，用于管理遥测和审计。 控制平面处理来自 API server 的配置，并且在数据平面中配置 PEPs，PEPs 用 Envoy 实现。Istio 安全架构图如下所示： Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2022-08-13 13:54:16 "},"security/crd/security-crd.html":{"url":"security/crd/security-crd.html","title":"资源配置","keywords":"","body":"资源配置 Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-10-23 21:28:30 "},"security/crd/request-authentication.html":{"url":"security/crd/request-authentication.html","title":"RequestAuthentication","keywords":"","body":"RequestAuthentication Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-10-23 21:28:30 "},"security/crd/peer-authentication.html":{"url":"security/crd/peer-authentication.html","title":"PeerAuthentication","keywords":"","body":"PeerAuthentication Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-10-23 21:28:30 "},"security/crd/authorization-policy.html":{"url":"security/crd/authorization-policy.html","title":"AuthorizationPolicy","keywords":"","body":"AuthorizationPolicy Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-10-23 21:28:30 "},"security/authentication.html":{"url":"security/authentication.html","title":"认证","keywords":"","body":"认证 Istio 基于服务维度和用户维度，提供了两种类型的认证： PeerAuthentication：用于服务到服务的认证，以验证连接的客户端服务。Istio 提供双向 TLS 作为传输认证的全栈解决方案，无需更改服务代码就可以启用它。这个解决方案： 为每个服务提供强大的身份，表示其角色，以实现跨群集和云的互操作性。 保护服务到服务的通信。 提供密钥管理系统，以自动进行密钥和证书的生成，分发和轮换。 RequestAuthentication：对请求的终端用户进行认证，以验证附加到请求的凭据。具体可通过 JWT（Json Web Token）、Google Auth 和自定义身份认证等方式认证。 在所有情况下，Istio 都通过自定义 Kubernetes API 将认证策略存储在 Istio config store。Istiod 使每个代理保持最新状态，并在适当时提供密钥。此外，Istio 的认证机制支持宽容模式（permissive mode，同时支持密文传输（双向 TLS）和明文传输），以帮助您了解策略更改在实施之前如何影响您的安全状况。 1、双向TLS认证 双向 TLS 认证是指客户端和服务端完成相互校验。 在 Istio 中的双向 TLS 认证，实际上是两个 Sidecar 之间首先进行双向 TLS 认证，认证通过后将后续的数据流进行加密传输。 例如，服务 A、服务 B 间采用双向 TLS 认证，当服务 A 向服务 B 发送请求时，该请求处理流程如下图所示： 图 7.3.1：Istio双向TLS认证流程 流程说明： Istio 将出站流量从客户端服务 A 重新路由到客户端服务 A 的 Sidecar（如，Envoy）。 客户端服务 A 的 Sidecar 与服务端服务 B 的 Sidecar 开始双向 TLS 握手。在握手期间，客户端 Sidecar 还做了安全命名检查，以验证服务端证书中显示的服务帐户是否授权目标服务。 客户端 Envoy 与服务器端 Envoy 开始双向 TLS 握手。在握手期间，客户端 Envoy 还做了安全命名检查，以验证服务器证书中显示的服务帐户是否被授权运行目标服务。 客户端 Sidecar 和服务端 Sidecar 建立一个双向 TLS 连接，Istio 将流量从客户端 Sidecar转发到服务端 Sidecar。 授权后，服务端服务 B 的 Sidecar 通过本地 TCP 连接将流量转发到服务端服务B。 （Sidecar 间的双向 TLS 认证对于服务 A、B 而言是无感知的。） 2、认证策略 根据实际应用场景，认证策略分为 PeerAuthentication 和 RequestAuthentication 两种方式，下面将针对这两种认证策略展开详细说明。 2.1 PeerAuthentication PeerAuthentication，又叫做“对等认证”，用于服务到服务的认证，以验证连接的客户端服务，对目标服务强制执行双向 TLS 认证模式。Istio 提供如下四种双向 TLS 身份认证模式，在不同的场景下进行使用，可通过PeerAuthentication.MutualTLS.Mode配置。 PERMISSIVE：同时支持密文传输（双向 TLS）和明文传输。不管是在 Istio 管理下的 Pod 还是在 Istio 管理外的 Pod，相互之间的通信畅通无阻。PERMISSIVE 模式的主要用途是在用户迁移的过程中，服务与服务之间仍旧能够通信，例如部分服务并未注入 sidecar。对于刚接触 Istio 的用户而言非常友好，官方建议在完成迁移之后调整为 STRICT 模式。 STRICT：只支持密文传输（双向 TLS）。 DISABLE：关闭双向 TLS。从安全的角度而言，官方并不建议在没有其他安全措施的情况下使用该模式。 UNSET： 具体的策略将从父级配置中继承（命名空间或网格层面），如果父级没有进行相应的配置，则使用 PERMISSIVE 模式。 例如，下面定义的 PeerAuthentication 认证策略：命名空间 foo 下所有服务都采用双向 TLS 认证。 apiVersion: \"security.istio.io/v1beta1\" kind: \"PeerAuthentication\" metadata: name: \"example-policy\" namespace: \"foo\" spec: mtls: mode: STRICT 使用 PeerAuthentication 认证策略，可以为不同端口指定不同的 mTLS 模式。例如，下面定义的 PeerAuthentication 认证策略：app: example-app 上的 80 端口禁用双向 TLS，其他端口采用双向 TLS。 apiVersion: \"security.istio.io/v1beta1\" kind: \"PeerAuthentication\" metadata: name: \"example-workload-policy\" namespace: \"foo\" spec: selector: matchLabels: app: example-app portLevelMtls: 80: mode: DISABLE 2.2 RequestAuthentication RequestAuthentication，又叫做\"请求认证\"，对请求的终端用户进行认证，以验证附加到请求的凭据。如果请求包含无效的身份验证信息，则它将基于配置的身份验证规则拒绝该请求。不包含任何身份验证凭据的请求将被接受，但不具有任何身份验证。如需对请求进行身份验证，则应附有授权规则。 具体可通过 JWT（Json Web Token）、Google Auth 和自定义身份认证等方式认证，这里以 JWT 方式展开说明。 JWT，是一种多方传递可信 JSON 数据的方案，一个 JWT token 由 . 分隔的三部分组成：{Header}.{Payload}.{Signature}，其中： Header： 是 Base64 编码的 JSON 数据，包含令牌类型 typ、签名算法 alg 以及秘钥 ID kid 等信息。 Payload： 是需要传递的 claims 数据，也是 Base64 编码的 JSON 数据，其中有些字段是 JWT 标准已有的字段如：exp、iat、iss、sub 和 aud 等，也可以根据需求添加自定义字段。 Signature： 是对前两部分的签名，防止数据被篡改，以此确保 token 信息是可信的，更多参考 Introduction to JSON Web Tokens。Istio 中验签所需公钥由 RequestAuthentication 资源的 JWKS 配置提供。 可通过 RequestAuthentication 中的 jwtRules配置 JWT 认证。 例如: apiVersion: security.istio.io/v1beta1 kind: RequestAuthentication metadata: name: httpbin namespace: foo spec: selector: matchLabels: app: httpbin jwtRules: - issuer: \"issuer-foo\" jwksUri: https://example.com/.well-known/jwks.json Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2022-08-13 13:54:16 "},"security/authorization.html":{"url":"security/authorization.html","title":"授权","keywords":"","body":"授权 授权，是按照预定义的配置针对特定的请求进行匹配，匹配成功之后会执行对应的动作，例如放行请求或者拒绝请求。 就像实现流量控制功能一样，Istio 中授权功能的实现也是非侵入式的，可以在不影响现有业务逻辑的情况下，通过一系列自定义的授权策略在 Istio 集群中启用授权功能，实现业务的安全加固。 用户可以通过配置授权策略来实现授权功能。授权策略按照作用域大小，可以分为三类： 网格级别：作用于整个服务网格集群的全局策略。 命名空间级别：作用于某个 namespace 的局部策略。 服务级别：作用于某些 pod 的具体策略。 例如： apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: allow-read namespace: default spec: selector: matchLabels: app: products action: ALLOW rules: - to: - operation: methods: [\"GET\", \"HEAD\"] 这条授权策略会找出 命名空间 default 中含有 app: products 标签的 pod，针对发送到这些 pod 的请求进行匹配，如果这些请求使用 HTTP 协议，且请求方法为 \"GET\" 或者 \"HEAD\"，则放行这些请求。 1、授权实现 授权策略对服务器端 Envoy 代理中的入站流量实施访问控制。每个 Envoy 代理都运行一个授权引擎，该引擎在运行时对请求进行授权。当请求到达代理时，授权引擎将根据当前授权策略评估请求上下文，并返回授权结果ALLOW 或 DENY。运维人员使用.yaml 文件配置指定 Istio 授权策略。 图 7.4.1：Istio授权架构图 2、授权策略 Istio 中的授权策略已隐式开启，无需显式开启，只需将授权策略应用于工作负载即可实施访问控制。对于未应用授权策略的工作负载，Istio 不会强制执行允许所有请求的访问控制。 授权策略支持 ALLOW，DENY 和 CUSTOM 动作。该策略动作优先级顺序为 CUSTOM、DENY、ALLOW。下图详细显示了策略优先级： 图 7.4.2：istio授权策略动作优先级 授权功能是通过授权策略（AuthorizationPolicy）完成配置和使用，一个授权策略包括 选择器（selector），动作（action） 和规则列表（rules）： selector：指定授权策略的目标。 action：指定允许还是拒绝请求。 rules：策略规则，指定何时触发动作。 from： 指定请求的来源 to：指定请求的操作 when：指定应用规则所需的条件 这里的 rules 是一个 rule 列表，可以指定多个规则。每一条 rule 规则包括三部分： from 、 to 和 when ，类似于防火墙规则。from 和 to 表示匹配当前请求从哪里来、到哪里去，when 会增加一些额外的匹配条件，当这些条件都满足时，就会认为当前规则匹配成功。如果其中某一部分未配置，则认为其可以匹配成功。 在 rule 中进行配置时，所有的字符串类型都支持类似于通配符的匹配模式，例如 abc* 匹配 \"abc\" 和 \"abcd\" 等，*xyz 匹配 \"xyz\" 和 \"axyz\" 等，单独的 * 匹配非空的字符串。 下面针对具体的字段详细进行说明。 from：针对请求的发送方进行匹配，指定请求的来源，主要包括 principals 、 requestPrincipals 、 namespaces 和 ipBlocks 四个部分。 principals：匹配请求方的身份，在 Kubernetes 中是 pod 的 Service Account。使用这个字段时，首先需要开启 mTLS 功能。例如，当前请求是从命名空间 default 中的 pod 发出，且 pod 使用的 Service Account 名为 sleep，针对这个请求进行匹配，可将 principals 配置为 [cluster.local/ns/default/sa/sleep]。 requestPrincipals：匹配请求中的 JWT Token 的 / 字段组合。 namespaces：匹配请求方 pod 所在的 namespace。 ipBlocks：匹配请求的源 IP 地址段。 to：针对请求的接收方进行匹配，指定请求的操作。除了请求接收方，还会对请求本身进行匹配。包括以下字段： hosts：请求目标的 host。 ports：请求目标的 port。 methods：是指当前请求执行的 HTTP Method。针对 gRPC 服务，这个字段需要设置为 POST。注意这个字段必须在 HTTP 协议时才进行匹配，如果请求不是 HTTP 协议，则认为匹配失败。 paths：当前请求执行的 HTTP URL Path。针对 gRPC 服务，需要配置为 /package.service/method 格式。 when：指定应用规则所需的条件，这是一个 key/value 格式的 list 。这个字段会针对请求进行一些额外的匹配条件，当这些条件全部匹配时才会认证当前规则匹配成功。例如 key: request.headers[User-Agent] 可以匹配 HTTP Header 中的 User-Agent 字段。所有可配置项可参见 Authorization Policy Conditions 说明。 针对以上字段，还有对应的反向匹配操作，即“取反”匹配，包括 notPrincipals、notNamespaces 等。例如 notNamespaces: [\"bar\"] 表示当发送请求的 pod 不位于命名空间 bar 中的时候匹配成功。 例如，下面定义的授权策略：允许来自命名空间 dev 的服务 cluster.local/ns/default/sa/sleep ，访问命名空间 foo 中 app: httpbin 和 version: v1 标签的服务，并且请求中包含由 https://accounts.google.com 签发的有效 JWT token。 apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: httpbin namespace: foo spec: selector: matchLabels: app: httpbin version: v1 action: ALLOW rules: - from: - source: principals: [\"cluster.local/ns/default/sa/sleep\"] - source: namespaces: [\"dev\"] to: - operation: methods: [\"GET\"] when: - key: request.auth.claims[iss] values: [\"https://accounts.google.com\"] 例如，下面定义的授权策略：对于来源不是命名空间 foo 的请求，将进行拒绝请求： apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: httpbin-deny namespace: foo spec: selector: matchLabels: app: httpbin version: v1 action: DENY rules: - from: - source: notNamespaces: [\"foo\"] 拒绝策略 DENY 优先允许策略 ALLOW。 3、示例 参考：https://istio.io/latest/docs/tasks/security/ Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2022-08-13 13:54:16 "},"observability/":{"url":"observability/","title":"可观测性","keywords":"","body":"可观测性 微服务架构中最大的挑战之一是如何通过简单方法就能够了解各服务间的关系。一次请求可能会流经多个甚至几十个独立部署的微服务，因此，发现哪里有性能瓶颈或错误变得尤为重要。 在 Istio 中， 为服务网格内所有的服务通信生成了详细的遥测数据，这种遥测技术提供了服务行为的可观察性，使得运维人员能够更快的排查故障、维护和优化服务，而不会给服务的开发人员带来任何额外的负担。通过 Istio，运维人员可以全面了解到受监控的服务是如何与其他服务以及 Istio 组件进行交互。 Istio 生成了以下三种类型的遥测数据，以提供对整个服务网格的可观测性： 指标：Istio 基于 4 个监控维度（延迟、流量、错误、饱和度）生成了一系列的服务指标。此外，还提供了控制平面级更详细的指标，并基于这些指标默认提供了一些监控面板，如：Grafana。 （饱和度是指服务每秒可以处理的请求总数，以及距离达到该阈值有多近） 分布式追踪： Istio 为每个服务生成分布式追踪 span，并默认提供Jaeger 面板，方便运维人员更直观的查看网格内服务的依赖和调用流程。 访问日志：当流量流入网格中的服务时，Istio 可以生成每个请求的完整记录，包括源和目标的元数据。此信息使运维人员能够将服务行为的审查控制到单个工作负载实例的级别。 在 Istio 中，已经和常见的几个Dashboard（ Prometheus 、Grafana、Jaeger、Kiali）做了对接， 能够帮助我们了解服务网格的结构、展示网络的拓扑结构、分析网格的健康状态等，最大幅度的实现了对服务网格中服务的可观测性。Dashborard 需要手动安装，安装方法参照3.3 部署dashboard。 本章中，我们会通过 Jaeger 实现调用链跟踪、通过 Prometheus 和 Grafana 实现遥测数据收集和展示，通过Kiali 生成服务可视化图。 Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2022-08-13 13:54:16 "},"observability/metrics.html":{"url":"observability/metrics.html","title":"指标、监控和可视化","keywords":"","body":"指标、监控和可视化 指标（metrics）是数据的总体汇总，它能让你了解正在发生的事情和需要深入挖掘的地方，提供了一种以聚合的方式监控和理解行为的方法。服务不断产生消费指标，这些指标是服务健康状况的持续衡量标准。 为了监控服务行为，Istio 为服务网格中所有出入的服务流量都生成了指标。这些指标提供了关于行为的信息，例如总流量数、错误率和请求响应时间。 在 Istio 中指标分为三类指标： 服务指标：面向服务的指标，包含了四个监控维度：延迟、流量、错误和饱和度。 代理指标：面向 Sidecar（如：Envoy）的指标，为所有进出流量提供了丰富的指标。 控制面板指标：面向控制平面的指标，用于采集 Istiod 自己的指标，已有的指标参考pilot-discovery。 各类指标通过事先埋点，通过暴露 HTTP 接口的方式让 Prometheus 定时抓取，将其指标数据存储至时间序列数据库（TSDB），且提供了多维度的数据模型、强大的查询语言（PQL）和简单的面板来生成被监控资源的报表，并结合 Grafana 将监控指标数据进行可视化展示。 1、Prometheus Prometheus 是一款开源的、自带时序数据库的监控告警系统。目前，Prometheus 已成为 Kubernetes 集群中监控告警系统的标配。Prometheus 的架构如下图所示： 图 8.2.1：Prometheus架构图 Prometheus 通过规则对 Kubernetes 集群中的数据源做服务发现（Service Dicovery），再从数据源中抓取数据，保存在它的时序数据库 TSDB 中。再根据配置的告警规则，将数据推给 AlertManager 服务，做告警信息的推送。同时，Prometheus 中也暴露了 HTTP 指标查询接口，通过 PromQL（一种特定的查询语法）可以将收集的数据查询并展示出来。 从上图可以看出，Prometheus 主要从两种数据源抓取指标：PushGateway 和 Exporters。PushGateway 指的是服务将指标数据主动推给 PushGateway 服务，Prometheus 再异步从 PushGateway 服务中抓取。而 Exporters 则主动暴露了 HTTP 服务接口，Prometheus 定时从接口中抓取指标。 在 Istio 中，各个组件是通过暴露 HTTP 接口的方式让 Prometheus 定时抓取的（采用了 Exporters 的方式）。在 Kubernetes 集群中，Istio 安装完成后，需单独执行 $ kubectl apply -f samples/addons/prometheus.yaml 来安装 Prometheus，此时会在 istio-system 的命名空间中部署 Prometheus，并将 Istio 组件各相关指标的数据源默认配置在 Prometheus 中。 关于 Prometheus 采集哪些数据源的指标，可直接在浏览器中打开 Prometheus 页面的 Targets 页签（http://:9090/targets），查看数据源： 图 8.2.2：prometheus-targets (也可通过查看默认 Prometheus 配置文件，即：samples/addons/prometheus.yaml 中 ConfigMap配置。) 其中，数据源 kubernetes-pods 提供了集群中 Pod 的相关指标，包括 Istio 的 Sidecar 及控制平面的相关指标。 Prometheus 基本配置 在 istio 中已经提供了关于 Prometheus的部署 samples/addons/prometheus.yaml，其默认的 prometheus 配置已经能够从各 Istio 组件中正常抓取指标数据，其默认配置是 samples/addons/prometheus.yaml 中 ConfigMap 配置。具体配置内如如下： （由于配置文件内容太多，这里截取其中的关键部分进行说明，# 为注释说明部分） # 全局配置 global: evaluation_interval: 1m scrape_interval: 15s # 采样数据时间间隔 scrape_timeout: 10s # 指定规则文件，如告警规则 rule_files: - /etc/config/recording_rules.yml - /etc/config/alerting_rules.yml - /etc/config/rules - /etc/config/alerts # 抓取指标的数据源配置，称为target，每个target用job_name命名。有静态配置和服务发现两种配置方式。 scrape_configs: # 命名为prometheus的Targets，从localhost:9090采集指标 - job_name: prometheus static_configs: - targets: - localhost:9090 …… # 命名为kubernetes-pods的Target，从Kubernetes服务发现配置采集指标 - job_name: kubernetes-pods # Kubernetes的服务发现配置：发现Kubernetes集群中所有命名空间下的所有pod kubernetes_sd_configs: - role: pod # 在Prometheus抓取指标前做一些内置标签的聚合、替换、去除等操作。 relabel_configs: # 采集标签__meta_kubernetes_pod_annotation_prometheus_io_scrape值为true的 - action: keep regex: true source_labels: - __meta_kubernetes_pod_annotation_prometheus_io_scrape # 采集标签__meta_kubernetes_pod_annotation_prometheus_io_path的值，并将其替换给内置标签__metrics_path__，即：作为采集地址 - action: replace regex: (.+) source_labels: - __meta_kubernetes_pod_annotation_prometheus_io_path target_label: __metrics_path__ # 把标签值分别匹配([^:]+)(?::\\d+)?和(\\d+)正则表达式的内置标签__address__和 __meta_kubernetes_pod_annotation_prometheus_io_port的值，并合并成__address__:port的格式，替换掉原来的__address__标签值。如：内置标签__address__:10.244.1.23、__meta_kubernetes_pod_annotation_prometheus_io_port:15020，则合并后并替换完的target_label是10.244.1.23:15020 - action: replace regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2 source_labels: - __address__ - __meta_kubernetes_pod_annotation_prometheus_io_port target_label: __address__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - action: replace source_labels: - __meta_kubernetes_namespace target_label: kubernetes_namespace - action: replace source_labels: - __meta_kubernetes_pod_name target_label: kubernetes_pod_name - action: drop regex: Pending|Succeeded|Failed source_labels: - __meta_kubernetes_pod_phase …… 关于更多 Kubernetes的服务发现配置和可用元标签的说明，可参考kubernetes_sd_config。 其中，relabel_configs：重定义标签，是在拉取(scraping)阶段前，修改 target 和它的 labels action：指定relabel 的动作，有如下值： replace：正则匹配 source_labels 的值来替换 target_labels； keep：正则没有匹配到 source_labels ，则删除 targets； drop：正则匹配到 source_labels，则删除 targets； hashmod：设置 target_labels 值为source_labels 值的 hash值； labelmap：正则匹配所有标签名，将匹配的标签的值复制到由 replace 提供的标签名； labeldrop：正则匹配所有标签名，匹配则移除标签； labelkeep：正则匹配所有标签名，不匹配的标签会被移除。 regex：正则表达式，用于匹配 source_labels 。 source_labels：指定需要处理的源标签。 target_labels：指定要处理后的标签。 例如，下图 Pod 中的元数据如下，结合上述 Prometheus 的数据源配置 kubernetes-pods，可知：Prometheus 将采集该 Pod 中的指标，采集指标地址为：http://10.144.1.23:15020/stats/prometheus。 图 8.2.3：reviews-pod-metadata 2、Grafana Grafana 是一款开源的指标数据可视化工具，有着功能齐全的度量仪表盘、图表等时序数据展示面板，支持 Zabbix、InfluentDB、Prometheus、Elasticsearch、MySQL 等数据源的指标展示。 在 Istio 中，也引入了 Grafana 这样一款提供了将时间序列数据库（TSDB）数据转换为精美的图形和可视化面板的工具。Grafana 让用户能够更直观地观测到集群中各项数据指标的变化趋势（网格流量变化、组件资源使用情况等），是 Isito 实现可观测性最重要的组件之一。 在 Istio 安装时，我们可以通过配置将 Grafana 服务默认安装在 Istio-system 命名空间下，Istio 安装完成后，默认配置了 Istio 中的 Prometheus 作为数据源，定时地从 Prometheus 中采集 Istio 各组件的指标数据，进行可视化展示。 3、Kiali Kiali 最初是由 Red Hat 开源的，用于解决 Service Mesh 中可观察性即微服务的可视性问题。目前已获得 Istio 社区的官方支持。 图 8.2.4：graph-overview Kiali 提供以下功能： 服务拓扑图 健康检查 指标度量收集 分布式跟踪 配置&配置校验 目前，Kiali 只能作为个人开发使用，功能上不够完善，在实际生产上可能还需结合实际需求开发更加完善的 Istio 可视化管理平台。 Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2022-08-13 13:54:16 "},"observability/tracing.html":{"url":"observability/tracing.html","title":"分布式追踪","keywords":"","body":"分布式追踪 分布式追踪通过监控流经网格的单个请求，提供了一种监控和理解行为的方法。追踪使网格的运维人员能够理解服务的依赖关系以及在服务网格中的延迟源。 Istio 支持通过 Envoy 代理进行分布式追踪。代理自动为其应用程序生成追踪 span，只需要应用程序转发适当的请求上下文即可。 Istio 支持很多追踪系统，包括 Zipkin， Jaeger，Lightstep和 Datadog。 1、Jaeger Jaeger 是由 Uber 开源的分布式追踪系统，它采用 Go 语言编写，主要借鉴了 Google Dapper 论文和 Zipkin 的设计，兼容 OpenTracing 以及 Zipkin 追踪格式，目前已成为 CNCF 基金会的开源项目。 Jaeger 主要包括以下三部分： Trace：用来描述在分布式系统中一个完整的调用链，每一个 Trace 会有一个独有的 Trace ID。一个端到端的 Trace 由一个或多个 Span 组成。 Span ：Span 是 Jaeger 的逻辑工作单元，可以是一个微服务中的 service，也可以是一次方法调用，甚至一个简单的代码块调用。具有请求名称、请求开始时间、请求持续时间。每一个 Span 会有一个独有的 Span ID。Span 会被嵌套并排序以展示服务间的关系。 图 8.3.1：jaeger-spans-traces Span Context：含额外 Trace 信息的数据结构，span context 可以包含 Trace ID、Span ID，以及其他任何需要向下游服务传递的 Trace 信息。 1.1 Envoy 分布式追踪 Jaeger 的实现 Envoy 原生支持 Jaeger，追踪所需 x-b3 开头的 Header 和 x-request-id 在不同的服务之间由业务逻辑进行传递，并由 Envoy 上报给 Jaeger，最终 Jaeger 生成完整的追踪信息。 为了将各种追踪 span 整合在一起以获得完整的追踪图，应用程序必须在传入和传出请求之间传播追踪上下文信息。特别是，Istio 依赖于应用程序传播 b3 追踪 Header 以及由 Envoy 生成的请求 ID，即应用程序服务请求时需携带这些 Header。这些 Header 包括： x-request-id x-b3-traceid x-b3-spanId x-b3-parentspanid x-b3-sampled x-b3-flags b3 如果请求中没有 B3 HTTP Header，Istio Sidecar 代理(Envoy) 会自动生成初始化的 Headers。 在 Istio 中，Envoy 和 Jaeger 的关系如下： 图 8.3.2：envoy-jaeger 上图中 Front Envoy 指的是第一个接收到请求的 Envoy Sidecar，它会负责创建 Root Span 并追加到请求 Header 内，请求到达不同的服务时，Envoy Sidecar 会将追踪信息进行上报。 Jaeger 的内部组件架构与 EFK 日志系统架构有一定相似性： 图 8.3.3：jaeger 架构图 Jaeger 主要由以下几个组件构成： Client：Jaeger 客户端，是 OpenTracing API 的具体语言实现，可以为各种开源框架提供分布式追踪工具。 Agent：监听在 UDP 端口的守护进程，以 Daemonset 的方式部署在宿主机或以 Sidecar 方式注入容器内，屏蔽了 Client 和 Collector 之间的细节以及服务发现。用于接收 Client 发送过来的追踪数据，并将数据批量发送至 Collector。 Collector：用来接收 Agent 发送的数据，验证追踪数据，并建立索引，最后异步地写入后端存储，Collector 是无状态的。 DataBase：后端存储组件，支持内存、Cassandra、Elasticsearch、Kafka 的存储方式。 Query：用于接收查询请求，从数据库检索数据并通过 UI 展示。 UI：使用 React 编写，用于 UI 界面展示。 在 Istio 提供“开箱即用”的追踪环境中，Jaeger 的部署方式是 all-in-one 的方式。该模式下部署的 Pod 为 jaeger，使用的是 jaegertracing/all-in-one 镜像，包含：Jaeger-agent、Jaeger-collector、Jaeger-query(UI) 几个组件。 不同的是，Bookinfo 的业务代码并没有集成 Jaeger-client ，而是由 Envoy 将追踪信息直接上报到 Jaeger-collector，另外，存储方式默认为内存，随着 Pod 销毁，追踪数据将会被删除。 1.2 Jaeger 部署和验证 Jaeger 的部署方式主要有以下几种： all-in-one 部署：适用于快速体验 Jaeger ，所有追踪数据存储在内存中，不适用于生产环境。在 Istio 的 demo 环境中，就是采用该种部署方式。 Kubernetes 部署：通过在集群独立部署 Jaeger 各组件 manifest 完成，定制化程度高，可使用已有的 Elasticsearch、Kafka 服务，适用于生产环境。 OpenTelemetry 部署：适用于使用 OpenTelemetry API 的部署方式。 Windows 部署：适用于 Windows 环境的部署方式，通过运行 exe 可执行文件安装和配置。 Jaeger 部署 Istio 的 demo 环境中，默认是没有部署 Jaeger，需要手动额外部署。（Istio 的老版本中，默认会自动部署） 确认 trace 采样率。 如使用 demo 配置安装 Istio，即：istioctl install --set profile=demo 方式安装， trace 采样率通过环境变量 PILOT_TRACE_SAMPLING 设置，demo 环境默认设置为 100，即：100% 采样所有请求。 demo 环境默认采样率可通过 mainfests\\proiles\\demo.yaml 文件，查看环境变量 PILOT_TRACE_SAMPLING 的配置值。 注意：你也可以通过 --set values.pilot.traceSampling= 来配置采样率。Value 范围在 0.0 到 100.0 之间，精度为 0.01 。例如，Value 配置 0.01 意味着 10000 请求中跟踪 1 个请求。 部署 Jaeger。 执行 kubectl apply -f samples/addons/jaeger.yaml 来安装 Jaeger，此时会在 istio-system 的命名空间中部署 Jaeger。 访问 Jaeger Dashboard。 执行 istioctl dashboard jaeger --address= 命令启动 dashboard，其中 是该宿主机的 IP 地址，方便外部直接根据该 IP 访问，默认端口为 16686。 浏览器输入：http://:16686，可直接访问 Jaeger Dashboard。 图 8.3.4：jaeger-dashboard 验证。 以 BookInfo 示例，访问 /productpage 页面，使得生成并上报调用链数据，默认 100% 的采样率，则请求一次即可产生追踪链路数据。 请求完，在 Jaeger Dashboard，选择一个 Service ，如 productpage.default ，点击 \"Find Traces\" 按钮查询追踪结果。 （如果查询不到数据，可将 \"Lookback\" 选项的时间调大些） 图 8.3.5：jaeger-dashboard-1 点击其中的列表进入追踪详情，详细记录了一次请求涉及到的 Services、深度、Span 总数、请求总时长等信息。也可以对下方的单项服务展开，观察每一个服务的请求耗时和详情。 图 8.3.6：jaeger-dashboard-2 Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2022-08-13 13:54:16 "},"observability/logs.html":{"url":"observability/logs.html","title":"日志","keywords":"","body":"日志 当今的容器化技术可以直接以 stdout 标准输出的形式来控制日志，这是目前比较推崇的做法。但在实际情况下，有可能会涉及到存量服务的迁移，而这些存量服务会将日志输出到文件，处理这些日志文件就没有那么简单，因为文件位于容器内部，从宿主机上不易访问。最简单的处理方式，就是在容器当中再单独 run 一个日志采集进程，这个进程就可以读取日志文件，以增量的形式将日志发送到日志中心，再进行聚合与存储，这是目前主流的处理方式，例如，ELK、EFK。 Istio 的核心设计理念之一就是为网格内的服务提供良好的可观测性，使开发与运维人员能够更好地监测到网格内的服务运行情况。Istio 可以监测到网格内的服务通信的流转情况，并生成详细的遥测日志数据，任何请求与事件的元信息都可以获取到。在 Istio 中，可以自定义 schema 来获取具有一定格式的日志信息，日志信息可以经过容器 stdout 标准输出，也可以通过第三方插件导出到特定的收集器，一切取决于实际情况。 本节主要讲解关于 Istio 中 Sidecar 的访问日志，以了解网格内服务通信的日志情况。 1、Envoy 访问日志 Istio 可以为网格内服务通信生成访问日志，并支持配置日志格式，其中包括时间、内容等，例如： [2019-03-06T09:31:27.360Z] \"GET /status/418 HTTP/1.1\" 418 - \"-\" 0 135 5 2 \"-\" \"curl/7.60.0\" \"d209e46f-9ed5-9b61-bbdd-43e22662702a\" \"httpbin:8000\" \"127.0.0.1:80\" inbound|8000|http|httpbin.default.svc.cluster.local - 172.30.146.73:80 172.30.146.82:38618 outbound_.8000_._.httpbin.default.svc.cluster.local 开启 Envoy 访问日志： 修改 Istio 配置文件： （Istio 安装时，demo 环境配置中已默认设置开启） istioctl install --set meshConfig.accessLogFile=/dev/stdout 此外，还可以通过 meshConfig.accessLogEncoding 为 JSON 或 TEXT 的日志编码格式，默认为 TEXT 格式，即：单行格式，通过 meshConfig.accessLogFormat 来自定义访问日志的格式。 （也可通过 kubectl edit 命令修改。） 日志格式： envoy 允许定制日志格式， 格式通过若干「Command Operators」组合，用于提取请求信息，istio 没有使用 envoy 默认的日志格式， istio 定制的访问日志格式如下： [%START_TIME%] \\\"%REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\\\" %RESPONSE_CODE% %RESPONSE_FLAGS% %RESPONSE_CODE_DETAILS% %CONNECTION_TERMINATION_DETAILS% \\\"%UPSTREAM_TRANSPORT_FAILURE_REASON%\\\" %BYTES_RECEIVED% %BYTES_SENT% %DURATION% %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)% \\\"%REQ(X-FORWARDED-FOR)%\\\" \\\"%REQ(USER-AGENT)%\\\" \\\"%REQ(X-REQUEST-ID)%\\\" \\\"%REQ(:AUTHORITY)%\\\" \\\"%UPSTREAM_HOST%\\\" %UPSTREAM_CLUSTER% %UPSTREAM_LOCAL_ADDRESS% %DOWNSTREAM_LOCAL_ADDRESS% %DOWNSTREAM_REMOTE_ADDRESS% %REQUESTED_SERVER_NAME% %ROUTE_NAME%\\n 其中，比较关键项如下： RESPONSE_CODE：响应状态码，如，200。 RESPONSE_FLAGS：很重要的信息，envoy 中自定义的响应标志位，可以认为是 envoy 附加的流量状态码。如 NR 表示找不到路由，UH 表示 upstream cluster 中没有健康的 host，RL 表示触发 rate limit，UO 触发断路器。RESPONSE_FLAGS 可选值有十几个，这些信息在调试中非常关键。 X-REQUEST-ID：一次 C 端到 S 端的 http 请求，Envoy 会在 C 端生产 request id，并附加到 header 中，传递到 S 端，在 2 端的日志中都会记录该值， 因此可以通过这个 ID 关联请求的上下游。注意不要和全链路跟踪中的 trace id 混淆。 ROUTE_NAME：匹配执行的路由名称。 通过访问日志可以快速的进行问题排查、分析。 Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2022-08-13 13:54:16 "},"extensibility/extending-envoy-proxy-with-webassembly.html":{"url":"extensibility/extending-envoy-proxy-with-webassembly.html","title":"基于 WASM 扩展 Envoy","keywords":"","body":"基于 WASM 扩展 Envoy Envoy WASM 介绍 WebAssembly 是一种沙盒技术，可用于扩展 Istio 代理（Envoy）的能力。Proxy-Wasm 沙盒 API 取代了 Mixer 作为 Istio 主要的扩展机制。 WebAssembly 沙盒的目标： 效率：这是一种低延迟，低 CPU 和低内存开销的扩展机制。 功能：这是一种可以执行策略，收集遥测数据和执行有效负载变更的扩展机制。 隔离：一个插件中程序的错误或是崩溃不会影响其它插件。 配置：插件的使用与其它 Istio API 一致的 API 进行配置，可以动态的配置扩展。 运维：扩展可以仅日志，故障打开或者故障关闭的方式进行访问和部署。 扩展开发者：可以用多种编程语言编写。 基于 Go 语言实现 Istio Envoy 的扩展 本示例是基于http_headers示例，来学习如何基于 Go 语言实现 Envoy WASM 的扩展，并应用于服务网格 Istio。 环境准备 安装 go 链接：https://golang.org/doc/install 安装 tinygo 链接：https://tinygo.org/getting-started/linux/ 提示：如果已有 go 环境，则不需要重复安装，tinygo 用于编译成 wasm 插件。tinygo 也可以从 github 直接下载解压，把解压后的 bin 目录加入到 PATH 目录。 下面以 MACOS 环境来安装 tinygo： % brew tap tinygo-org/tools % brew install tinygo 开发 WASM 开发 WASM 插件，理论上可以采用任何开发语言。目前已有不同语言实现的 Envoy Proxy WASM SDK 可供使用，如： proxy-wasm-cpp-sdk proxy-wasm-rust-sdk AssemblyScript proxy-wasm-go-sdk 本文示例采用由 tetrate 开发的 Go SDK，以http_headers示例进行举例。 下载http_headers示例代码。 通过 TinyGo 编译生成 WASM 文件。 在 http_headers 目录下执行 tinygo 命令编译，生成 http-headers.wasm。 % tinygo build -o ./http-headers.wasm -scheduler=none -target=wasi ./main.go 挂载 WSAM 文件 将 WASM 文件挂载到目标 Pod 的 Sidecar(即：istio-proxy)容器中。 以文件的方式，创建 ConfigMap。 % kubectl create cm http-headers-wasm --from-file=http-headers.wasm 将 WASM 文件挂载到目标 Pod 的 Sidecar 容器中，即：修改 Deployment，为其添加如下注解： sidecar.istio.io/userVolume: '[{\"name\":\"wasmfilters-dir\",\"configMap\": {\"name\": \"http-headers-wasm\"}}]' sidecar.istio.io/userVolumeMount: '[{\"mountPath\":\"/var/local/lib/wasm-filters\",\"name\":\"wasmfilters-dir\"}]' istiod 会依据这 2 个注解，将名为 http-headers-wasm 的 configmap，挂载到 istio-proxy 容器的 /var/local/lib/wasm-filters 目录下。 % kubectl patch deployment go-httpbin -p '{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"sidecar.istio.io/userVolume\":\"[{\\\"name\\\":\\\"wasmfilters-dir\\\",\\\"configMap\\\": {\\\"name\\\": \\\"http-headers-wasm\\\"}}]\",\"sidecar.istio.io/userVolumeMount\":\"[{\\\"mountPath\\\":\\\"/var/local/lib/wasm-filters\\\",\\\"name\\\":\\\"wasmfilters-dir\\\"}]\"}}}}}' Pod 重新创建后，在 istio-proxy 的 /var/local/lib/wasm-filters 下可以查看到 http-headers.wasm 文件。 创建 EnvoyFilter 创建 EnvoyFilter 资源，用于加载 WASM 插件。 编写的 EnvoyFilter 如下： apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: http-headers-filter spec: configPatches: - applyTo: HTTP_FILTER match: context: SIDECAR_INBOUND proxy: proxyVersion: ^1\\.11.* listener: portNumber: 8080 filterChain: filter: name: envoy.filters.network.http_connection_manager subFilter: name: envoy.filters.http.router patch: operation: INSERT_BEFORE value: name: envoy.filters.http.wasm typed_config: \"@type\": type.googleapis.com/udpa.type.v1.TypedStruct type_url: type.googleapis.com/envoy.extensions.filters.http.wasm.v3.Wasm value: config: # root_id: add_header vm_config: code: local: filename: /var/local/lib/wasm-filters/http-headers.wasm runtime: envoy.wasm.runtime.v8 # vm_id: \"my_vm_id\" allow_precompiled: false workloadSelector: labels: app: go-httpbin 其中： proxyVersion 与 istio-proxy 版本保持一致。 filename 需要与前面Deployment中的 Annotation 保持一致。 workloadSelector 设置为目标 Pod 的 label。 验证 验证 Envoy扩展的WASM插件是否生效。 登录到其它服务网格的服务容器中，请求该服务中存在的URL，查看该服务的istio-proxy容器的日志。 参考资料： https://istio.io/latest/docs/concepts/wasm/ proxy-wasm-go-sdk Extending Envoy Proxy with Golang WebAssembly Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-09-23 20:01:33 "},"integration/registry/integration-registry.html":{"url":"integration/registry/integration-registry.html","title":"集成注册中心","keywords":"","body":"集成注册中心 Istio 作为服务网格领域主流的开源框架，为微服务提供了零侵入的流量管理、服务可观测性等方面的服务治理能力，解决了传统微服务架构体系（如：Spring Cloud 技术体系）存在的高侵入性问题，彻底释放出业务开发人员无需过度关注服务治理的烦恼。 随着服务网格的推进，越来越多的项目尝试向服务网格转型，将传统微服务架构下的服务向服务网格迁移，为了降低迁移风险，大多采取阶段性、平滑迁移。一般先将所有服务容器化，迁移到 Kubernetes 上管理，再将服务纳入到网格管理。但面对庞大的存量微服务项目来说，往往为了能够快速迁移，以享受 Istio 提供的各种服务治理能力，采取服务上容器、Kubernetes、纳入网格，但服务注册仍采用原有的注册方式，如：Eureka、Consul、Nacos 等。为此，就需要考虑在 Istio 中如何集成第三方注册中心的问题，本节将针对 Istio 中的服务注册展开讨论，并为集成注册中心提供思路和方案，供参考使用。 1、Istio 服务注册发现机制 Istio 中服务的注册与发现是在控制平面实现，主要由 Pilot 组件完成，准确来说应该是pilot-discovery,负责所有服务的注册发现。 2、服务注册中心对接方式 3、总结 参考资料： 如何将第三方服务中心注册集成到 Istio ？ Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-09-30 16:50:38 "},"integration/registry/integration-consul.html":{"url":"integration/registry/integration-consul.html","title":"集成 Consul","keywords":"","body":"集成 Consul Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-09-30 16:50:38 "},"appendix/ports-used-by-istio.html":{"url":"appendix/ports-used-by-istio.html","title":"使用端口/协议","keywords":"","body":"使用端口和协议 在 istio 中，会默认占用一些端口，这些已占用端口在应用程序中应避免使用，否则将会发生端口冲突。 1、Sidecar 使用的端口和协议 Istio sidecar 代理（Envoy）使用以下端口和协议： 端口 协议 描述 仅限 Pod 内部 15000 TCP Envoy 管理端口 Yes 15001 TCP Envoy 出站端口 No 15004 HTTP 调试端口 Yes 15006 TCP Envoy 入站端口 No 15008 H2 HBONE mTLS 隧道端口 No 15009 H2C 用于安全网络的 HBONE 端口 No 15020 HTTP 从 Istio agent、Envoy 和应用程序合并的 Prometheus 指标采集端口 No 15021 HTTP 健康检查端口 No 15053 DNS DNS 端口，如果启用了将会占用 Yes 15090 HTTP Envoy Prometheus 遥测端口 No 2、控制平面（istiod）使用的端口和协议 Istio 控制平面 (istiod) 使用以下端口和协议： 端口 协议 描述 仅限本地主机 443 HTTPS Webhooks 服务端口 No 8080 HTTP 调试接口（已弃用，仅限容器端口） No 15010 GRPC XDS 和 CA 服务（纯文本，仅用于安全网络） No 15012 GRPC XDS 和 CA 服务（TLS 和 mTLS，推荐用于生产） No 15014 HTTP 控制平面监控 No 15017 HTTPS Webhook 容器端口，从 443 转发 No 3、服务器协议 协议 端口 SMTP 25 DNS 53 MySQL 3306 MongoDB 27017 参考：Ports used by Istio Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2022-08-13 15:19:28 "},"faq/how-does-the-specification-define-the-protocol-type-of-the-service.html":{"url":"faq/how-does-the-specification-define-the-protocol-type-of-the-service.html","title":"如何规范定义服务的协议类型","keywords":"","body":"如何规范定义服务的协议类型 istio 包括 HTTP、HTTPS、gRPC 和原始 TCP 协议等，默认可以自动检测 HTTP 和 HTTP2 流量。对于无法自动识别的协议，将被视为普通 TCP 流量。为了正确识别协议类型，提供额外的功能，在数据平面中必须规范定义服务的协议类型。 本文介绍如何规范定义服务的协议类型。 背景 istio 常见的协议类型包括 HTTP、HTTP2、HTTPS、TCP、TLS、gRPC、gRPC-Web、Mongo、MySQL 和 Redis。 为了规范定义服务协议，可以采取以下两种定义方式。 方式一：使用服务端口名称时指定协议类型 在 Service 的 ports 中，port 的 name 需设置为 {协议名称}或{协议名称}-{自定义后缀}。例如：服务的 9090 端口是 gRPC 协议类型，可以设置 port 的 name 为 grpc-demo；服务的 3306 端口是 MySQL 数据库协议，可以设置 port 的 name 为 mysql。 YAML 示例如下： kind: Service metadata: name: myservice spec: ports: - port: 9090 name: grpc-demo - port: 3306 name: mysql 方式二：使用服务端口的appProtocol指定协议类型 可以使用 Service 的 appProtocol 指定协议类型。 指定协议类型为 HTTPS 的 YAML 示例如下： kind: Service metadata: name: myservice spec: ports: -port: 3306 name: database appProtocol: https 注：ports.appProtocol 的生效优先级高于 ports.name。 参考： Protocol Selection Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2023-04-15 22:39:33 "},"faq/redis-mysql-cannot-connect-to-the-sidecar-after-injection.html":{"url":"faq/redis-mysql-cannot-connect-to-the-sidecar-after-injection.html","title":"Redis、MySQL等注入边车后，无法访问问题","keywords":"","body":"Redis、MySQL等注入边车后，无法访问问题 针对一些特殊协议的服务，注入网格后，是无法直接访问。 可以通过 DestinationRule 禁用 Redis Service 的 mTLS： apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: redis-disable-mtls spec: host: redis.default.svc.cluster.local trafficPolicy: tls: mode: DISABLE 参考： Server First Protocols Istio 运维实战系列（2）：让人头大的『无头服务』-上 Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2023-03-31 22:46:54 "},"faq/tpc-fault-injection.html":{"url":"faq/tpc-fault-injection.html","title":"TCP协议服务故障注入（如：redis）","keywords":"","body":"TCP协议服务故障注入（如：redis） 目前istio故障注入是通过VS实现对http协议的故障，针对TCP协议还未直接支持。 为了解决TCP协议类服务的故障注入，可以采取以下两种方式实现： 基于 Envoy 的 RedisProxy能力 ，网格内的 Redis 流量将经由 Envoy 代理，通过配置EnvoyFilter来实现。(待验证) 通过 VS 将匹配到redis端口, 路由到一个未知的service 来实现。 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: redis-route namespace: redis spec: hosts: - devops-redis.redis.svc.cluster.local tcp: match: - port: 6379 route: destination: host: devops-redis-unknown.redis.svc.cluster.local port: number: 6379 参考： Help: Is it possible to inject faults in Redis with Envoy Redis Proxy? Redis 流量管理 https://stackoverflow.com/questions/66941477/redis-fault-injection-using-istio-and-envoy-filter How to Fault Injection for redis Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2023-03-31 22:55:05 "}}