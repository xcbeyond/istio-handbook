{"./":{"url":"./","title":"序言","keywords":"","body":"istio-handbook — Istio 实践手册 Istio 实践手册，从服务网格概念出发，将逐步渗透到 Istio 具体细节中来，旨在帮助 Istio 学习者、使用者快速掌握相关知识点，可作为 Istio 学习、实践手册，建议收藏！ （不断更新中……） 如果你在学习过程中，有什么建议、或者需要帮助，欢迎通过 Issues 方式一起讨论。 Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-11-14 00:48:58 "},"microservice/new-generation-microservices-architecture.html":{"url":"microservice/new-generation-microservices-architecture.html","title":"迎接新一代微服务架构","keywords":"","body":"迎接新一代微服务架构 微服务是近些年来软件架构中的热名词，也是一个很大的概念，不同人对它的理解都各不相同，甚至在早期微服务架构中出现了一批四不像的微服务架构产品，有人把单纯引入 Spring Boot、Spring Cloud 等框架的应用服务也称之为微服务架构，但这却只是将它作为服务的 Web 容器而已。 随着微服务的火热，越来越多的团队开始实践，将微服务纷纷落地，并投入生产。但随着微服务规模的不断壮大，每增加一个微服务，就可能会增加一些依赖的基础设施和第三方的配置，比如 Kafka 、Redis 实例等，相应 CI/CD 的配置也会增加或调整。 同时随着微服务数量增多、业务复杂性的提升及需求的多样性等（如，对接第三方异构系统等），服务间通信的错综复杂，一步步地将微服务变得更加臃肿，服务治理也是难上加难，而这些问题在单体架构中是很容易解决的。为此，有人开始怀疑当初微服务化是否是明智之选，甚至考虑回归到传统单体应用。 正如下图所示，PPT 中的微服务总是美好的，但现实中的微服务却是一团糟糕，想甩甩不掉，越看越糟心。难道就没有办法了么？ 图 2.1.1：现实中和PPT中的微服务对比 1、传统微服务架构面临的挑战 面对上述暴露出的问题，并在传统微服务架构下，经过实践的不断冲击，面临了更多新的挑战，综上所述，产生这些问题的原因有以下这几点： 过于绑定特定技术栈。 当面对异构系统时，需要花费大量精力来进行代码的改造，不同异构系统可能面临不同的改造。 代码侵入度过高。 开发者往往需要花费大量的精力来考虑如何与框架或 SDK 结合，并在业务中更好的深度融合，对于大部分开发者而言都是一个高曲线的学习过程。 多语言支持受限。 微服务提倡不同组件可以使用最适合它的语言开发，但是传统微服务框架，如 Spring Cloud 则是 Java 的天下，多语言的支持难度很大。这也就导致在面对异构系统对接时的无奈，或选择退而求其次的方案了。 老旧系统维护难。 面对老旧系统，很难做到统一维护、治理、监控等，在过度时期往往需要多个团队分而管之，维护难度加大。 上述这些问题在传统微服务架构中都是在所难免，我们都知道技术演进来源于实践中不断的摸索，将功能抽象、解耦、封装、服务化。 随着传统微服务架构暴露出的这些问题，将迎来新的挑战，让大家纷纷寻找其他解决方案。 2、迎来新一代微服务架构 为了解决传统微服务面临的问题，以应对全新的挑战，微服务架构也进一步演化，最终催生了Service Mesh 的出现，迎来了新一代微服务架构，也被称为“下一代微服务”。为了更好地理解 Service Mesh 的概念和存在的意义，我们来回顾一下这一演进过程。 1.1 耦合阶段 在微服务架构中，服务发现、负载均衡、熔断等能力是微服务架构中重要的组成部分。微服务化之后，服务更加的分散，复杂度变得更高，起初开发者将诸如熔断、超时等功能和业务代码封装在一起，使服务具备了网络管控的能力，如下图所示。 图 2.1.2：耦合阶段 这种方案虽然易于实现，但从设计角度来讲却存在一定的缺陷。 基础设施功能（如，服务发现，负载均衡、熔断等）和业务逻辑高度耦合。 每个微服务都重复实现了相同功能的代码。 管理困难。如果某个服务的负载均衡发生变化，则调用它的相关服务都需要更新变化。 开发者不能集中精力只关注于业务逻辑开发。 1.2 公共库 SDK 基于上面存在的问题，很容易会想到将基础设施功能设计为一个公共库 SDK，让服务的业务逻辑与这些公共功能降低耦合度，提高重复利用率，更重要的是开发者只需要关注公共库 SDK 的依赖及使用，而不必关注实现这些公共功能，从而更加专注于业务逻辑的开发，比如 Spring Cloud 框架是类似的方式。如下图所示： 图 2.1.3：公共库SDK阶段 实际上即便如此，它仍然有一些不足之处。 这些公共库 SDK 存在较为陡峭的学习成本，需要耗费开发人员一定的时间和人力与现有系统集成，甚至需要考虑修改现有代码进行整合。 这些公共库 SDK 一般都是通过特定语言实现，缺乏多语言的支持，在对现有系统整合时有一定的局限性。 公共库 SDK 的管理和维护依然需要耗费开发者的大量精力，并需专门人员来进行管理维护。 1.3 Sidecar 模式 有了上面公共库 SDK 的启发，加上跨语言问题、更新后的发布和维护等问题，人们发现更好的解决方案是把它作为一个代理，服务通过这个透明的代理完成所有流量的控制。 这就是典型的 Sidecar 代理模式，也被翻译为\"边车\"代理，它作为与其他服务通信的桥梁，为服务提供额外的网络特性，并与服务独立部署，对服务零侵入，更不会受限于服务的开发语言和技术栈，如下图所示。 图 2.1.4：Sidecar模式阶段 以 Sidecar 模式进行通信代理，实现了基础实施层与业务逻辑的完全隔离，在部署、升级时带来了便利，做到了真正的基础设施层与业务逻辑层的彻底解耦。另一方面，Sidecar 可以更加快速地为应用服务提供更灵活的扩展，而不需要应用服务的大量改造。Sidecar 可以实现以下主要功能： 服务注册。 帮助服务注册到相应的服务注册中心，并对服务做相关的健康检查。 服务路由。 当应用服务调用其它服务时，Sidecar 可以帮助从服务发现中找到相应的服务地址，完成服务路由功能。 服务治理。 Sidecar 可以完全拦截服务进出的流量，并对其进行相应的调用链跟踪、熔断、降级、日志监控等操作，将服务治理功能集中在 Sidecar 中实现。 集中管控。 整个微服务架构体系下的所有服务完全可以通过 Sidecar 来进行集中管控，完成对服务的流控、下线等。 于是，应用服务终于可以做到跨语言开发、并更专注于业务逻辑的开发。 1.4 Service Mesh 把 Sidecar 模式充分应用于一个庞大的微服务架构系统，为每个应用服务配套部署一个 Sidecar 代理，完成服务间复杂的通信，最终就会得到一个如下图所示的网络拓扑结构，这就是 Service Mesh，又称之为“服务网格“。 图 2.1.5：Service Mesh阶段 至此，迎来了新一代微服务架构——Service Mesh，它彻底解决了传统微服务架构所面临的问题。 Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-09-05 13:52:03 "},"servicemesh/introduction.html":{"url":"servicemesh/introduction.html","title":"服务网格介绍","keywords":"","body":"服务网格介绍 在上一节《迎接新一代微服务架构》中，我们知道服务网格经历了 4 个重要阶段： 图 3.1.1：服务网格的演进历程 耦合阶段：高度耦合、重复实现、维护困难，在耦合架构设计中体现的最为突出，单体架构就是典型的代表。 公共 SDK：让基础设施功能设计成为公共 SDK，提高利用率，是解藕最有效的途径，比如 Spring Cloud 就是类似的方式。但学习成本高、特定语言实现，却将一部分人拦在了门外。 Sidecar 模式：再次深度解藕，不单单功能解藕，更从跨语言、更新发布和运维等方面入手，实现对业务服务的零侵入，更解藕于开发语言和单一技术栈，实现了完全隔离，为部署、升级带来了便利，做到了真正的基础设施层与业务逻辑层的彻底解耦。另一方面，Sidecar 可以更加快速地为应用服务提供更灵活的扩展，而不需要应用服务的大量改造。 Service Mesh：把 Sidecar 模式充分应用到一个庞大的微服务架构系统中来，为每个应用服务配套部署一个 Sidecar 代理，完成服务间复杂的通信，最终就会得到一个的网络拓扑结构，这就是服务网格，又称之为“Service Mesh“。它从本质上解决了传统微服务所面临的问题。 接下来，让我们一起全面、真正的开始了解服务网格吧！ 1、云原生定义 在正式开始服务网格了解之前，我们先来看看另外一个与之相关的名词——“云原生”，因为在服务网格的技术圈子里，与之密不可分。 CNCF（Cloud Native Computing Foundation（云原生计算基金会））对云原生的定义： 云原生技术有利于各组织在公有云、私有云和混合云等新型动态环境中，构建和运行可弹性扩展的应用。云原生的代表技术包括容器、服务网格、微服务、不可变基础设施和声明式API。 图 3.1.2：云原生技术 从上图可知，服务网格是云原生体系的具体实现，是承载微服务架构理念的云原生技术形态。 微服务源自服务化架构设计理念，与敏捷开发 DevOps 理念的结合：微、小、快、独。 经过四代的技术演进，随着云计算发展到云原生阶段，服务网格则成为承载微服务理念的新一代技术形态。 当你开启服务网格的学习之路后，也就意味着已经踏入了云原生的领域。在这里，服务治理与业务逻辑逐步解耦，服务治理能力下沉到基础设施，服务网格以基础设施的方式提供无侵入的连接控制、安全、可监测性、灰度发布等治理能力，如华为云的 ASM、蚂蚁金服的 SOFAMesh 等，都是对服务网格的最佳实践。 2、服务网格定义 服务网格，又称之为 Service Mesh，作为服务间通信的基础设施层。轻量级高性能网络代理，提供安全的、快速的、可靠地服务间通讯，与实际应用部署一起，但对应用透明。应用作为服务的发起方，只需要用最简单的方式将请求发送给本地的服务网格代理，然后网格代理会进行后续的操作，如服务发现，负载均衡，最后将请求转发给目标服务。 服务网格目的是解决系统架构微服务化后的服务间通信和治理问题。服务网格由 Sidecar 节点组成，这个模式的精髓在于实现了数据面（业务逻辑）和控制面的解耦。具体到微服务架构中，即给每一个微服务实例同步部署一个 Sidecar。 图 3.1.3：服务网格部署网络结构图 在服务网格部署网络结构图中，绿色方块为应用服务，蓝色方块为 Sidecar，应用服务之间通过 Sidecar 进行通信，整个服务通信形成图中的蓝色网络连线，图中所有蓝色部分就形成了。其具备如下主要特点： 应用程序间通讯的中间层 轻量级网络代理 应用程序无感知 解耦应用程序的重试/超时、监控、追踪和服务发现 服务网格的出现解决了传统微服务框架中的痛点，使得开发人员专注于业务本身，同时，将服务通信及相关管控功能从业务中分离到基础设施层。 从云原生的视角来看，服务网格是一种云原生的、应用层的网络技术，具体体现如下： 云原生：面向弹性、（微）服务化、去中心化业务场景。 应用层：以应用为中心，关注应用的发布、监控、恢复等。 网络：关注应用组件之间的接口、流量、数据、访问安全等。 3、服务网格的功能 服务网格作为微服务架构中负责网络通信的基础设施层，具备网络处理的大部分功能。下面列举了一些主要的功能： 动态路由。 可通过路由规则来动态路由到所请求的服务，便于不同环境、不同版本等的动态路由调整。 故障注入。 通过引入故障来模拟网络传输中的问题（如延迟）来验证系统的健壮性，方便完成系统的各类故障测试。 熔断。 通过服务降级来终止潜在的关联性错误。 安全。 在服务网格上实现安全机制（如 TLS），并且很容易在基础设施层完成安全机制更新。 多语言支持。 作为独立运行且对业务透明的 Sideca 代理，很轻松地支持多语言的异构系统。 多协议支持。 同多语言一样，也支持多协议。 指标和分布式链路追踪。 概括起来，服务网格主要体现在以下 4 个方面： 可见性： 运行时指标遥测、分布式跟踪。 可管理性： 服务发现、负载均衡、运行时动态路由等。 健壮性： 超时、重试、熔断等弹性能力。 安全性： 服务间访问控制、TLS 加密通信。 4、服务网格解决的问题 从上述服务网格的定义看： 基础设施层是服务网格的定位，致力于解决微服务基础设施标准化、配置化、服务化和产品化的问题。 服务间通信是服务网格技术层面对的问题，对微服务屏蔽通信的复杂度，解决微服务的通信治理问题。 请求可靠传递是服务网格的目标。 轻量级网络代理是服务网格的部署方式。 对应用程序透明是服务网格的亮点和特色，实现对业务无侵入。 综合上述，服务网格主要解决用户如下 3 个维度的痛点需求： 完善的微服务基础设施 通过将微服务通信下沉到基础设施层，屏蔽了微服务处理各种通信问题的复杂度，形成微服务之间的抽象协议层。开发者无需关心通信层的具体实现，也无需关注 RPC 通信（包含服务发现、负载均衡、流量调度、流量降级、监控统计等）的一切细节，真正像本地调用一样使用微服务，通信相关的一起工作直接交给服务网格。 语言无关的通信和链路治理 功能上，服务网格并没有提供任何新的特性和能力，服务网格提供的所有通信和服务治理能力在服务网格之前的技术中均能找到，比如 Spring Cloud 就实现了完善的微服务 RPC 通信和服务治理支持。 服务网格改变的是通信和服务治理能力提供的方式，通过将这些能力实现从各语言业务实现中解耦，下沉到基础设施层面，以一种更加通用和标准化的方式提供，屏蔽不同语言、不同平台的差异性，有利于通信和服务治理能力的迭代和创新，使得业务实现更加方便。 服务网格避免了多语言服务治理上的重复建设，通过服务网格语言无关的通信和服务治理能力，助力于多语言技术栈的效率提升。 通信和服务治理的标准化 微服务治理层面，服务网格是标准化、体系化、无侵入的分布式治理平台。 标准化方面，Sidecar 成为所有微服务流量通信的约束标准，同时服务网格的数据平台和控制平面也通过标准协议进行交互。 体系化方面，从全局考虑，提供多维度立体的微服务可观测能力（Metric、Trace、Logging），并提供体系化的服务治理能力，如限流、熔断、安全、灰度等。 通过标准化，带来一致的服务治理体验，减少多业务之间由于服务治理标准不一致带来的沟通和转换成本，提升全局服务治理的效率。 5、服务网格的原理 服务网格的核心是数据平面（Sidecar）与控制平面（Control Plane），如下图： 图 3.1.4：Service Mesh架构图 数据平面： Sidecar，与服务部署在一起的轻量级网络代理，用于实现服务框架的各项功能（如，服务发现、负载均衡、限流熔断等），让服务回归业务本质。 数据平台可以认为是将 Spring Cloud、Dubbo 等相关的微服务框架中通信和服务治理能力独立出来的一个语言无法的进程，并且更注重通用性和扩展性。在 服务网格 中，不再将数据平面代理视为一个个独立的组件，而是将这些代理连接在一起形成一个全局的分布式网格。 在传统的微服务架构中，各种服务框架的功能（如，服务发现、负载均衡、限流熔断等）代码逻辑或多或少的都需要耦合到服务实例的代码中，给服务实例增加了很多无关业务的代码，同时带来了一定的复杂度。 有了 Sidecar 之后，服务节点只做业务逻辑自身的功能，服务之间的调用只需交给 Sidecar，由 Sidecar 完成注册服务、服务发现、请求路由、熔断限流、日志统计等业务无关功能。 在这种新的微服务架构中，所有的 Sidecar 组成在一起，就形成了服务网格。那么这个大型的服务网格并不是完全自治的，它还需要一个统一的控制节点 Control Plane。 控制平面： 是用来从全局的角度上控制 Sidecar，相当于服务网格整体的大脑，控制着 Sidecar 来实现服务治理的各项功能。比如，它负责所有 Sidecar 的注册，存储统一的路由表，帮助各个 Sidecar 进行负载均衡和请求调度；它收集所有 Sidecar 的监控信息和日志数据。 6、总结 至此，关于服务网格的介绍就到此结束，更深入的理解可结合后续具体应用、实践来加深吧。 参考资料： 全方位解读服务网格（Service Mesh）的背景和概念 云原生第9课：Istio服务网格快速入门 Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-11-21 01:22:02 "},"servicemesh/framework-contrast.html":{"url":"servicemesh/framework-contrast.html","title":"服务网格框架对比","keywords":"","body":"服务网格框架对比 当前，业界主要有以下主要几种 Service Mesh 框架，下面进行详细的说明及对比。 1、Linkerd Linkerd 是 Buoyant 公司 2016 年率先开源的高性能网络代理，是业界的第一款 Service Mesh 框架。其主要用于解决分布式环境中服务之间通信面临的一些问题，如网络不可靠、不安全、延迟丢包等问题。 Linkerd 使用Scala 语言编写，运行于 JVM，底层基于 Twitter 的 Finagle 库，并对其做了相应的扩展。最主要的是 Linkerd 具有快速、轻量级、高性能等特点，每秒以最小的延迟及负载处理万级请求，易于水平扩展。除此之外，还有以下功能： 支持多平台：可运行于多种平台，比如 Kubernetes、DC/OS、Docker，甚至虚拟机或物理机。 无缝集成多种服务发现工具。 支持多协议，如 gRPC、HTTP/1.x、HTTP/2，甚至可通过 linkerd-tcp 支持 TCP 协议。 支持与第三方分布式追踪系统 Zipkin 集成。 灵活性、扩展性高，可通过其提供的接口开发自定义插件。 目前，Linkerd和Linkerd2并行开发，其情况如下： Linkerd：Linkerd使用Scala语言编写，运行于JVM，底层基于 Twitter 的Finagle库，并对其做了相应的扩展。 Linkerd2：使用Go语言和Rust语言完全重写了Linkerd，专门用于Kubernetes。 Linkerd本身是数据平面，负责将数据路由到目标服务，同时保证数据在分布式环境中传输是安全、可靠、快速的。另外，Linkerd还包括控制平面组件Namerd，通过控制平面Namerd实现中心化管理和存储路由规则、中心化管理服务发现配置、支持运行时动态路由以及暴露Namerd API管理接口。 图 3.2.1：Linkerd架构图 控制平面 是在Kubernetes特定命名空间中运行的一组服务。这些服务可以完成各种事情：聚集遥测数据，提供面向用户的 API，向数据平面代理提供控制数据等。 由以下部分组成： Controller：由public-api容器组成，该容器为CLI和dashboard提供接口 API。 Destination：数据平面中的每个代理都使用此组件来查找将请求发送到哪里。还用于获取服务配置信息，如：路由指标，重试和超时等。 Identity：该组件提供了证书的颁发，接受来自代理的CSRs并返回正确身份签名的证书。这些证书由代理在启动时获取，并且必须在代理准备就绪之前发出。随后，它们可用于Linkerd代理之间的任何连接以实现mTLS。 Proxy Injector：是一个注入程序，每次创建一个pod时，它都会接收一个webhook请求。该注入程序检查资源以查找特定于Linkerd的注释（linkerd.io/inject: enabled）。当存在该注释时，注入器将更改容器的规范，并添加 initContainer包含代理本身的以及附属工具。 Service Profile Validator：用于在保存新服务配置文件之前先对其进行验证。 Tap：从CLI和dashboard接收请求，以实时监视请求和响应。 数据平面 由轻量级代理组成，这些代理作为sidecar容器与服务代码的每个实例一起部署。为了将服务“添加”到Linkerd服务网格，必须重新部署该服务的Pod，以在每个 Pod 中包含数据平面代理。 2、Envoy 同Linkerd一样，Envoy也是一款高性能的网络代理，于 2016 年 10 月份有 Lyft 公司开源，为云原生应用而设计，可作为边界入口，处理外部流量，此外，也作为内部服务间通信代理，实现服务间可靠通信。Envoy的实现借鉴现有生产级代理及负载均衡器，如Nginx、HAProxy、硬件负载均衡器及云负载均衡器的实践经验，同时基于C++编写及 Lyft 公司生产实践证明，Envoy性能非常优秀、稳定。 Envoy既可用作独立代理层运行，也可作为Service Mesh架构中数据平面层，因此通常Envoy跟服务运行在一起，将应用的网络功能抽象化，Envoy提供通用网络功能，实现平台及语言无法性。除此之外，还有以下功能： 优先支持HTTP/2和gRPC，同时支持Websocket和 TCP 代理。 API 驱动的配置管理方式，支持动态管理、更新配置以及无连接和请求丢失的热重启功能。 L3/L4层过滤器形成Envoy核心的连接管理功能。 通过与多种指标收集工具及分布式追踪系统集成，实现运行时指标收集、分布式追踪，提供整个系统及服务的运行时可见性。 内存资源使用率低，Sidecar是Envoy最常用的部署模式。 3、Istio Istio是由Google、IBM和Lyft发起的开源的Service Mesh框架。该项目在 2017 年推出，并在 2018 年 7 月发布了 1.0 版本。 Istio是Service Mesh目前的实现的典型代表，如果Sidecar是整个Service Mesh的数据面，那么Istio主要在控制面上做了更多的改进，Istio使用Envoy作为Sidecar，控制面相关全部使用Golang编写，性能上有了很大的提升。 Istio 首先是一个服务网格，但是Istio又不仅仅是服务网格：在 Linkerd，Envoy 这样的典型服务网格之上，Istio提供了一个完整的解决方案，为整个服务网格提供行为洞察和操作控制，以满足微服务应用程序的多样化需求。 Istio在服务网络中统一提供了许多关键功能： 流量管理：控制服务之间的流量和 API 调用的流向，使得调用更可靠，并使网络在恶劣情况下更加健壮。 可观察性：了解服务之间的依赖关系，以及它们之间流量的本质和流向，从而提供快速识别问题的能力。 策略执行：将组织策略应用于服务之间的互动，确保访问策略得以执行，资源在消费者之间良好分配。策略的更改是通过配置网格而不是修改应用程序代码。 服务身份和安全：为网格中的服务提供可验证身份，并提供保护服务流量的能力，使其可以在不同可信度的网络上流转。 除此之外，Istio针对可扩展性进行了设计，以满足不同的部署需要。 平台支持：Istio旨在在各种环境中运行，包括跨云， 预置，Kubernetes，Mesos等。最初专注于Kubernetes，但很快将支持其他环境。 集成和定制：策略执行组件可以扩展和定制，以便与现有的ACL，日志，监控，配额，审核等解决方案集成。 这些功能极大的减少了应用程序代码，底层平台和策略之间的耦合，使微服务更容易实现。 图 3.2.2：Istio架构图 Istio架构图中各个子模块功能如下： Envoy：负责各个应用服务之间通信。 Pilot：管理和配置Envoy，提供服务发现、负载均衡和智能路由，保证弹性服务（服务超时次数、重试、熔断策略）。 Mixer：信息监控检查。 Istio-Auth：提供服务和服务、用户和服务之间的认证服务，实现访问控制，解决是谁访问的是哪个 API 的问题。 其中，图中的通信代理组件为Envoy，这是Istio原生引入的，但Linkerd也能够集成对接Istio。 4、Conduit Conduit于 2017 年 12 月发布，作为由 Buoyant 继Linkerd后赞助的另外一个开源项目，作为Linkerd面向Kubernetes的独立版本。Conduit旨在彻底简化用户在Kubernetes使用服务网格的复杂度，提高用户体验，而不是像Linkerd一样针对各种平台进行优化。 Conduit的主要目标是轻量级、高性能、安全并且非常容易理解和使用。同Linkerd和Istio，Conduit也包含数据平面和控制平面，其中数据平面由Rust语言开发，使得Conduit使用极少的内存资源，而控制平面由Go语言开发。Conduit依然支持Service Mesh要求的功能，而且还包括以下功能： 超级轻量级和极快的性能。 专注于支持Kubernetes平台，提高运行在Kubernetes平台上服务的可靠性、可见性及安全性。 支持gRPC、HTTP/2和HTTP/1.x请求及所有 TCP 流量。 Conduit以极简主义架构，以零配置理念为中心，旨在减少用户与Conduit的交互，实现开箱即用。 5、对比总结 下面对上述各种 Service Mesh 框架进行简单的比较汇总，见下表所示： 功能 Linkerd Envoy Istio Conduit 代理 Finagle + Jetty Envoy Envoy Conduit 熔断 支持。基于连接的熔断器Fast Fail和基于请求的熔断器Failure Accrual。 支持。通过特定准则，如最大连接数、 最大请求数、最大挂起请求数或者最大重试数的设定。 支持。通过特定准则，如最大连接数和最大请求数等的设定。 暂不支持。 动态路由 支持。通过设置Linkerd的dtab规则实现不同版本服务请求的动态路由。 支持。通过服务的版本或环境信息实现。 支持。通过服务的版本或环境信息实现。 暂不支持。 流量分流 支持。以增量和受控的方式实现分流。 支持。以增量和受控的方式实现分流。 支持。以增量和受控的方式实现分流。 暂不支持。 服务发现 支持。支持多种服务发现机制，如基于文件的服务发现、Consul、Zookeeper、Kubernetes等。 支持。通过提供平台无关的服务发现接口实现与不同服务发现工具集成。 支持。通过提供平台无关的服务发现接口实现与不同服务发现工具集成。 只支持Kubernetes。 负载均衡 支持。提供多种负载均衡算法。 支持。提供多种负载均衡算法，如Round Robin、加权最小请求、哈希环、Maglev等。 支持。提供多种负载均衡算法，如Round Robin、加权最小请求、哈希环、Maglev等。 支持。当前只有 HTTP 请求支持基于P2C + least-loaded的负载均衡算法。 安全通信 支持 TLS。 支持 TLS。 支持 TLS。 支持TLS。 访问控制 不支持。 不支持。 支持。基于RBAC的访问控制。 暂不支持。 可见性 分布式追踪(Zipkin)、运行时指标(InfluxDB、Prometheus、statsd) 分布式追踪(Zipkin)、运行时指标(statsd) 分布式追踪(Zipkin)、运行时指标(Prometheus、statsd)、监控(NewRepic、Stackdriver) 运行时指标(Prometheus) 部署模式 Sidecar 或者 per-host 模式 Sidecar 模式 Sidecar 模式 Sidecar 模式 控制平面 Namerd 没有，但可通过 API 实现。 Pilot、Mixer、Citadel Conduit 协议支持 HTTP/1.x、HTTP/2、gRPC HTTP/1.x、HTTP/2、gRPC、TCP HTTP/1.x、HTTP/2、gRPC、TCP HTTP/1.x、HTTP/2、gRPC、TCP 运行平台 平台无关 平台无关 目前支持Kubernetes，平台无关是最终实现目标。 只支持Kubernetes。 上述任何一个 Service Mesh 框架都能够满足您的基本需求。到⽬前为⽌，Istio 具有这几个服务⽹格框架中最多的功能和灵活性，灵活性意味着复杂性，因此需要团队更为充⾜的准备。如果只想使⽤基本的 Service Mesh 治理功能，Linkerd 可能是最佳选择。如果您想⽀持同时包含 Kubernetes 和 VM 的异构环境，并且不需要 Istio 的复杂性，那么 Conduit 可能是您的最佳选择，⽬前 Istio 也提供了同时包含 Kubernetes 和 VM 的异构环境的⽀持。 Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-11-21 01:22:02 "},"architecture/istio-architecture.html":{"url":"architecture/istio-architecture.html","title":"Istio 整体架构","keywords":"","body":"Istio 整体架构 Istio 从逻辑上分为数据平面（Data Plane）和控制平面（Control Plane）。 数据平面：由整个网格内的 Sidecar 代理组成，这些代理以 Sidecar 的形式和应用服务一起部署。这些代理负责协调和控制应用服务之间的所有网络通信。每一个 Sidecar会接管进入和离开服务的流量，并配合控制平面完成流量控制等方面的功能。 控制平面：控制和管理数据平面中的 Sidecar 代理，完成配置分发、服务发现、流量路由、授权鉴权等功能，以达到对数据平面的统一管理。在 Istio 1.5 版本中，控制平面由原来分散的、独立部署的几个组件整合为一个独立的 istiod，变成了一个单进程、多模块的组织形态。同时，在 Istio 1.5 版本中，基于性能、部署方便考虑，废弃了Mixer，转而将这些功能放到了 Sidecar 中。 下图展示了组成每个平面的不同组件： 图 4.1.1：Istio架构图 1、核心组件 下面将简单的介绍一下 Istio 架构中几个核心组件的主要功能。 1.1 Proxy Proxy 位于数据平面，即：常说的 Sidecar 代理，与应用服务以 Sidecar 方式部署在同一个 Pod 中。Proxy 实际上包括 istio-proxy 和 pilot-agent 两部分，它们以两个不同的进程部署在同一个容器 istio-proxy 中。 istio-proxy Istio 的数据平面默认使用 Envoy 的扩展版本作为 Sidecar 代理（即：istio-proxy），istio-proxy 是基于 Envoy 新增了一些扩展，其代码仓库位于 istio/proxy。 注：理论上，Istio 是支持多种 Sidecar 代理，其中 Envoy 作为默认提供的数据平面，如无特殊说明在 Istio 中通常所说的 Envoy 就是 istio-proxy。 Envoy 是用 C++ 开发的高性能代理，用于协调服务网格中所有服务的入站和出站流量，是唯一与数据平面流量交互的组件。主要包括三部分能力： 动态服务发现、负载均衡、路由、流量转移。 弹性能力：如超时重试、熔断等。 调试功能：如故障注入、流量镜像等。 polit-agent pilot-agent，负责管理 istio-proxy 的整个生命周期，具体包括 istio-proxy 准备启动参数和配置文件，负责管理 istio-proxy 的启动过程、运行状态监控以及重启等。其代码仓库位于 istio/istio/pilot/cmd/pilot-agent。 部署上，isito-proxy 不是单独构建镜像，而是和 polit-agent 一起打包构建成一个镜像 istio/proxyv2，poilt-agent 将会以子进程的方式启动 istio-proxy，并监控 istio-proxy 的运行状态。 1.2 Istiod 自 Istio 1.5 版本开始，控制平面由原来分散、独立部署的三个组件（Pilot、Citadel、Galley）整合为一个独立的 istiod，变成了一个单进程、多模块的组织形态，极大的降低了原来部署的复杂度。 Pilot 负责 Istio 数据平面的 xDS 配置管理，具体包括： 服务发现、配置规则发现：为 Sidecar 提供服务发现、用于智能路由的流量管理功能（例如，A/B 测试、金丝雀发布等）以及弹性功能（超时、重试、熔断器等）。通过提供通用的流量管理模型和服务发现适配器（Service Discovery Adapter），来对接不同平台的适配层。 xDS 配置下发：提供统一的 xDS API，供 Sidecar 调用。将路由规则等配置信息转换为 Sidecar 可以识别的信息，并下发给数据平面。 注：这里实际上是指 pilot-discovery，代码仓库位于 istio/istio/pilot/cmd/pilot-discovery Citadel 负责安全证书的管理和发放，可以实现授权和认证等操作。 Citadel 并不是唯一的证书管理方式，Istio 当前支持 Citadel、Vault 和 Google 等多种证书管理方式，Citadel 是当前默认的证书管理方式。 Galley Galley 是 Istio 1.1 版本中新引入的配置管理组件，主要负责配置的验证、提取和处理等功能。其目的是将 Istio 和底层平台（如 Kubernetes）进行解耦。 在引入 Galley 之前，Istio 控制平面的各个组件需要分别对 Kubernetes 资源进行管理，包括资源的配置验证，监控资源配置变化，并针对配置变更采取相应的处理等。 2、设计目标 几个关键的设计目标形成了 Istio 的架构，这些目标对于使系统能够大规模和高性能地处理服务是至关重要的。 对应用透明性：从本质上说，对应用透明是 Service Mesh 的特性，一个合格的 Service Mesh 产品都应该具有这一特性，否则也就失去了网格产品的核心竞争力。为此，Istio 自动将自己注入到服务之间的所有网络路径中，做到对应用的透明性。Istio 使用 Sidecar 代理来捕获流量，并在不更改已部署应用程序代码的情况下，自动对网络层进行配置，以实现通过这些代理来路由流量。 可扩展性：Istio 认为，运维和开发人员随着深入使用 Istio 提供的功能，会逐渐涌现更多的需求，主要集中在策略方面。因此，为策略系统提供足够的扩展性，成为了 Istio 的一个主要的设计目标。 可移植性：考虑到现有云生态的多样性，Istio 被设计为可以支持不同的底层平台，也支持本地、虚拟机、云平台等不同的部署环境。不过从目前的情况来看，Istio 和 Kubernetes 还是有着较为紧密的依赖关系，平台无关性、可移植性将是 Istio 最终实现目标。 策略一致性：Istio 使用自己的 API 将策略系统独立出来，而不是集成到 Sidecar 中，从而允许服务根据需要直接与之集成。同时，Istio 在配置方面也注重统一和用户体验的一致性。一个典型的例子是路由规则都统一由虚拟服务来配置，可在网格内、外以及边界的流量控制中复用。 3、Istio 架构演进 从 2017 年 5 月发布以来，Istio 经历了四个重要的版本和由此划分的三个发展阶段。在不到三年的产品迭代过程中，出现了两次重大的架构变动。 0.1 版本：2017 年 5 月发布。作为第二代 Service Mesh 的开创者，宣告了 Istio 的诞生，也燃起了网格市场的硝烟与战火。 1.0 版本：发布于 2018 年 7 月，对外宣传生产环境可用。从 0.1 到 1.0 版本，开发时间经历了一年多，但持续的发布了多个 0.x 版本，这一阶段处于快速迭代期。 1.1 版本：发布于 2019 年 3 月，号称企业级可用的版本。一个小的版本号变化居然耗费了半年之久，其主要原因是出现了第一次架构重构，这一阶段算是调整期。 1.5 版本：发布于 2020 年 3 月，再次进行架构的重建，将多组件整合为单体形态的 istiod。从 1.1 到 1.5 版本的一年中，Istio 开始遵循季节性发布，进入了产品的稳定发展期。 图 4.1.2：Istio架构演进 接下来，我们将会针对 Istio 数据平面和控制平面展开具体说明。 Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-09-30 17:35:45 "},"architecture/dataplane.html":{"url":"architecture/dataplane.html","title":"数据平面","keywords":"","body":"数据平面 Istio 数据平面的核心是以 Sidecar 方式存在的代理，该 Sidecar 将数据平面核心组件部署到单独的流程或容器中，以提供隔离和封装。Sidecar 与应用服务共享相同的生命周期，与应用服务一起创建、退出。Sidecar 与参与服务网格的所有应用服务实例一起运行，但不在同一个容器进程中，形成了服务网格的数据平面。只要应用服务想要与其他服务通信，就会通过服务 Sidecar 代理进行。 如下图所示，数据平面的 Sidecar 代理可以调节和控制应用服务之间所有的网络通信，每个应用服务 Pod 启动时会伴随启动 istio-init 和 istio-proxy 容器。其中 istio-init 容器主要功能是初始化 Pod 网络和对 Pod 设置 iptable 规则，设置完成后自动结束。istio-proxy 容器会启动两个进程：pilot-agent 以及 Sidecar 代理（如：Envoy）。pilot-agent 的作用是同步管理数据，启动并管理 Sidecar 代理服务进程，上报遥测数据，Sidecar 代理则根据管理策略完成流量管控、生成遥测数据。 图 4.2.1：Istio数据平面架构图 在 Istio 中，数据平面主要负责提供以下能力： 服务发现：探测所有可用的上游或下游服务实例。 健康检测：探测上游或下游服务实例是否健康，是否准备好接收网络流量。 流量路由：将网络请求路由到正确的上游或下游服务。 负载均衡：在对上游或下游服务进行请求时，选择合适的服务实例接收请求，同时负责处理超时、断路、重试等情况。 身份验证和授权：在 istio-agent 与 istiod 配合下，对网络请求进行身份验证、权限验证，以决定是否响应以及如何响应，使用 mTLS 或其他机制对链路进行加密等。 链路追踪：对于每个请求，生成详细的统计信息、日志记录和分布式追踪数据，以便操作人员能够理解调用路径并在出现问题时进行调试。 目前常见的数据平面实现有： Envoy：Istio 默认使用的开箱即用 Sidecar 代理，使用 C++ 开发，性能较高。 MOSN：阿里巴巴公司开源，设计类似 Envoy，使用 Go 语言开发，对多协议进行了支持。 Linkerd：一个提供弹性云端原生应用服务网格的开源项目，也是面向微服务的开源 RPC 代理，使用 Scala 开发。它的核心是一个透明代理，因此也可作为典型的数据平面的实现。 下面将对数据平面涉及到的容器或组件进行具体说明。 1、isito-init 2、istio-proxy 和 pilot-agent 3、Envoy Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-09-30 17:35:45 "},"architecture/controlplane.html":{"url":"architecture/controlplane.html","title":"控制平面","keywords":"","body":"控制平面 控制平面是控制和管理数据平面中的 Sidecar 代理，完成配置分发、服务发现、流量路由、授权鉴权等功能，以达到对数据平面的统一管理。 在 Istio 1.5 版本之前，Mixer 组件负责遥测统计（istio-telemetry）和策略控制（istio-policy），其中提供了两个接口 check 和 report，在每次 Sidecar 代理发送请求时都会去 Mixer 里 check 一次，然后 report 观测信息给 Mixer，Mixer 加工后发送给 Prometheus。因此，基于性能、部署方便的考虑，在 Istio 1.5 版本中废弃了 Mixer 组件，转而将这些功能放到了 Sidecar 中。 在 Istio 1.5 版本中，控制平面由原来分散、独立部署的几个组件整合为一个独立的 istiod，变成了一个单进程、多模块的组织形态。istiod 是新版本中最大的变化，以一个单体组件替代了原有的架构，在降低复杂度和维护难度的同时，也让易用性得到提升。需要注意的一点是，原有的多组件并不是被完全移除，而是在重构后以模块的形式整合在一起组成了 istiod。 目前，控制平面依旧延续之前组件的功能，但以模块的形式呈现在 istiod 中，包括 Pilot、Citadel、Galley 三个模块，本节将会详细介绍它们。 1、Pilot Pilot 是 Istio 中的核心组件，用于管理和配置部署在特定 Istio 服务网格中的所有 Sidecar 代理实例。它管理 Sidecar 代理之间的路由流量规则，并配置故障恢复功能，如超时、重试和熔断等。 1.1 Pilot 架构 图 4.3.1：Pilot架构图 根据上图，Pilot 有几个关键模块如下： 抽象模型（Abstract model） 为了实现对不同服务注册中心 （如，Kubernetes、Consul） 的支持，Pilot 需要对不同输入来源的数据有一个统一的存储格式，也就是抽象模型。 抽象模型中定义的关键成员包括 HostName（service 名称）、Ports（service 端口）、Address（service ClusterIP）、Resolution （负载均衡策略） 等。 平台适配器 （Platform adapters） Pilot 的实现是基于平台适配器（Platform adapters） 的，借助平台适配器 Pilot 可以实现服务注册中心数据到抽象模型之间的数据转换。 例如，Pilot 中的 Kubernetes 适配器通过 Kubernetes API Server 获得 Kubernetes 中 service 和 Pod 的相关信息，然后翻译为抽象模型提供给 Pilot 使用。 通过平台适配器模式，Pilot 还可以从 Consul 等平台中获取服务信息，还可以开发适配器将其他提供服务发现的组件集成到 Pilot 中。 xDS API Pilot 使用了一套源于 Envoy 项目的标准数据平面 API 来将服务信息和流量规则下发到数据面的 Sidecar 中。这套标准数据平面 API，也叫 xDS。 Sidecar 通过 xDS API 可以动态获取 Listener （监听器）、Route （路由）、Cluster（集群）及 Endpoint （集群成员）及 Secret（安全）配置： LDS：Listener 发现服务。Listener 监听器控制 Sidecar 启动端口的监听（目前只支持 TCP 协议），并配置 L3/L4 层过滤器，当网络连接达到后，配置好的网络过滤器堆栈开始处理后续事件。 RDS：Route 发现服务，用于 HTTP 连接管理过滤器动态获取路由配置，路由配置包含 HTTP 头部修改（增加、删除 HTTP 头部键值），virtual hosts （虚拟主机），以及 virtual hosts 定义的各个路由条目。 CDS：Cluster 发现服务，用于动态获取 Cluster 信息。 EDS：Endpoint 发现服务。用于动态维护端点信息，端点信息中还包括负载均衡权重、金丝雀状态等，基于这些信息，Sidecar 可以做出智能的负载均衡决策。 SDS：Secret 发现服务，用于运行时动态获取 TLS 证书。 通过采用该标准 API， Istio 将控制平面和数据平面进行了解耦，为多种数据平面 Sidecar 实现提供了可能性。例如蚂蚁金服开源的 Golang 版本的 Sidecar MOSN (Modular Observable Smart Network)。 用户 API（User API） Pilot 还定义了一套用户 API， 用户 API 提供了面向业务的高层抽象，可以被运维人员理解和使用。 运维人员使用该 API 定义流量规则并下发到 Pilot ，这些规则被 Pilot 翻译成数据平面的配置，再通过 xDS API 分发到 Sidecar 实例，可以在运行期对微服务的流量进行控制和调整。 通过运用不同的流量规则，可以对网格中微服务进行精细化的流量控制，如按版本分流、断路器、故障注入、灰度发布等。 1.2 Pilot 实现 图 4.3.2：Pilot实现 图中实线连线表示控制流，虚线连线表示数据流。带 [pilot] 的组件表示为 Pilot 组件，图中关键的组件如下： Discovery service：即 pilot-discovery，主要功能是从注册中心（如 Kubernetes 或者 Consul ）中获取服务信息，从 Kubernetes API Server 中获取流量规则（Kubernetes CRD Resource），并将服务信息和流量规则转化为数据平面可以处理的格式，通过标准的数据平面 API 下发到网格中的各个 Sidecar 中。 Agent：即 pilot-agent 组件，该进程根据 Kubernetes API Server 中的配置信息生成 Envoy 的配置文件，负责启动、监控 Sidecar 进程。 Proxy：既 Sidecar Proxy，是所有服务的流量代理，直接连接 pilot-discovery ，间接地从 Kubernetes 等服务注册中心获取集群中微服务的注册情况。 Service A/B：使用了 Istio 的应用，如 Service A/B，进出网络流量会被 Proxy 接管。 其中，Pilot 实际上包括 pilot-discovery 和 pilot-agent 两个组件，分别位于控制平面和数据平面。 pilot-discovery pilot-discovery 位于控制平面，扮演服务发现、控制平面到 Sidecar 之间的桥梁作用。pilot-discovery 的主要功能如下： 监控服务注册中心（如 Kubernetes）的服务注册情况。在 Kubernetes 环境下，会监控 service、endpoint、pod、node 等资源信息。 监控 Istio 控制平面信息变化，在 Kubernetes 环境下，会监控包括 RouteRule、 VirtualService、Gateway、EgressRule、ServiceEntry 等以 Kubernetes CRD 形式存在的 Istio 控制面配置信息。 将上述两类信息合并组合为 Sidecar 可以理解的配置信息，并将这些信息以 gRPC 协议提供给 Sidecar。 pilot-agent pilot-agent 位于数据平面，是一个本地代理，与 Sidecar 代理部署在一起，负责 Sidecar 服务的整个生命周期。pilot-agent 的主要功能如下： 生成 Sidecar 的配置。 负责 Sidecar 的启动与监控。 2、Citadel Citadel 是 Istio 中负责身份认证和证书管理的核心安全组件，1.5 版本之后取消了独立进程，作为一个功能模块被整合在 istiod 中。 2.1 Citadel 基本功能 总体来说，Istio 在安全架构方面主要包括以下内容： 证书签发机构（CA）负责密钥和证书管理。 API 服务器将安全配置分发给数据平面。 客户端、服务端通过代理安全通信。 Istio 的身份标识模型使用一级服务标识来确定请求的来源，它可以灵活的标识终端用户、工作负载等。在平台层面，Istio 可以使用类似于服务名称来标识身份，或直接使用平台提供的服务标识。比如 Kubernetes 的 ServiceAccount，AWS IAM 用户、角色账户等。 在身份和证书管理方面，Istio 使用 X.509 证书，并支持密钥和证书的自动轮换。从 1.1 版本开始，Istio 开始支持安全发现服务器（SDS），随着不断的完善和增强，1.5 版本 SDS 已经成为默认开启的组件。Citadel 以前有两个功能：将证书以 Secret 的方式挂载到命名空间里；通过 SDS gRPC 接口与 nodeagent（已废弃）通信。目前 Citadel 只需要完成与 SDS 相关的工作，其他功能被移动到了 istiod 中。 2.2 Citadel 工作原理 Citadel 主要包括 CA 服务器、SDS 服务器、证书密钥控制器等模块，它们的工作原理如下： CA 服务器 Citadel 中的 CA 签发机构是一个 gRPC 服务器，启动时会注册两个 gRPC 服务，一个是 CA 服务，用来处理 CSR 请求（certificate signing request）；另外一个是证书服务，用来签发证书。CA 首先通过 HandleCSR 接口处理来自客户端的 CSR 请求，对客户端进行身份验证（包括 TLS 认证和 JWT 认证），验证成功后会调用 CreateCertificate 进行证书签发。 SDS 服务器 SDS， 即安全发现服务（Secret Discovery Service），它是一种在运行时动态获取证书私钥的 API，Envoy 代理通过 SDS 动态获取证书私钥。Istio 中的 SDS 服务器负责证书管理，并实现了安全配置的自动化。相比传统的方式，使用 SDS 主要有以下优点： 无需挂载 Secret 卷； 动态更新证书，无需重启； 可以监听多个证书密钥对。 图 4.3.3：Citadel身份认证流程 目前的版本中，SDS 是默认开启的，它的工作流程如下： Envoy 通过 SDS API 发送证书和密钥请求； istio-agent 作为 Envoy 的代理，创建一个私钥和证书签名请求（CSR），并发送给 istiod； CA 机构验证收到的 CSR 并生成证书； istio-agent 将私钥和从 istiod 收到的证书通过 SDS API 发送给 Envoy； 以上流程周期性执行实现密钥和证书轮换。 证书密钥控制器 证书密钥控制器（CaSecretController）监听 istio.io/key-and-cert 类型的 Secret 资源，它会周期性的检查证书是否过期，并更新证书。 证书轮换 如果没有自动证书轮换功能，当证书过期时，就不得不重启签发，并重启代理。证书轮换解决了这一问题，提高了服务的可用性。Istio 里通过一个轮换器（Rotator）自动检查自签名的根证书，并在证书即将过期时进行更新，它本质上是一个协程（goroutine）在后台轮询实现的： 获取当前证书，解析证书的有效期并获取下一次轮换时间。 启动定时器，如果发现证书到达轮换时间，从 CA 获取最新的证书密钥对。 更新证书。 3、Galley Galley 是不直接向数据平面提供业务能力，而是在控制平面上向其他模块提供支持。 Galley 作为负责配置管理的模块，验证配置信息的格式和内容的正确性，并将这些配置信息提供给控制平面的 Pilot 使用，这样控制平面的其他模块只用和 Galley 交互，从而与底层平台解耦。使用简单的接口和 Galley 进行交互，由 Galley 负责配置验证、配置变更管理、配置源管理、多平台适配等工作。 Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-09-30 17:35:45 "},"install/istio-install.html":{"url":"install/istio-install.html","title":"Istio 安装","keywords":"","body":"Istio 安装 Istio 安装方式很多，本文采用 istioctl 命令安装，更多安装方式参考 Installation Guides 以 Istio 1.9.0 版本为例说明。 1、Istio 下载 在 Istio release 页面 https://github.com/istio/istio/releases/tag/1.9.0 下载 Istio 安装文件。 将 istio-1.9.0-linux-amd64.tar.gz 上传到安装服务器上，并解压。 tar -xvf istio-1.9.0-linux-amd64.tar.gz 将 istioctl 客户端路径增加到 path 环境变量中，使得能够直接执行 istioctl 命令。 修改当前用户的 .bash_profile 文件，将 istio 目录下的 bin 文件夹添加到 path 环境变量中，并使其生效（source .bash_profile）： # 进入当前用户目录 $ cd ~ # 修改.bash_profile文件，将istio目录下的bin文件夹添加到path中 $ vi .bash_profile # 使其生效 $ source .bash_profile 2、Istio 安装 对于本次安装，我们采用 demo 安装配置。 选择它是因为它包含了一组专为测试准备的功能集合，另外还有用于生产或性能测试的配置组合。 2.1 在线安装 安装命令如下： istioctl install --set profile=demo -y istioctl install 安装过程中需要下载相关镜像（最好能够科学上网），需耐心等待安装完成即可。 （安装失败，大多都是下载镜像失败所致，可确保能够正常下载镜像的情况下，再次执行上述安装命令。） 安装配置： 在安装 Istio 时所能够使用的内置配置文件，通过命令 istioctl profile list 可以查看有哪些内置配置。这些配置文件提供了对 Istio 控制平面和 Istio 数据平面 Sidecar 的定制内容。 您可以从 Istio 内置配置文件的其中一个开始入手，然后根据您的特定需求进一步自定义配置文件。当前提供以下几种内置配置文件： default: 根据默认的安装选项启用组件 (建议用于生产部署)。 demo: 这一配置具有适度的资源需求，旨在展示 Istio 的功能。它适合运行 Bookinfo 应用程序和相关任务。 这是通过快速开始指导安装的配置，但是您以后可以通过自定义配置 启用其他功能来探索更高级的任务。此配置文件启用了高级别的追踪和访问日志，因此不适合进行性能测试。 minimum：与默认配置文件相同，但仅安装控制平面组件。这允许您使用单独的配置文件配置控制平面和数据平面组件（例如，网关）。 external: 用于配置一个远程集群，由一个外部控制平面或通过控制平面主集群的多集群网格。 empty：不部署任何东西。这可以用作自定义配置的基本配置文件。 preview：预览配置文件包含实验性功能。这是为了探索 Istio 的新功能。不保证稳定性、安全性和性能 - 使用风险自负。 组件对应关系表： default demo minimal external empty preview istio-egressgateway ✅ istio-ingressgateway ✅ ✅ ✅ istiod ✅ ✅ ✅ ✅ 2.2 离线安装 在受网络限制的环境下，需进行离线安装。 准备所需镜像 tar 包。 在具备下载镜像的环境下，通过 docker pull 、docker save 命令制作 docker 镜像 tar 包。 导入镜像。 在 Docker 私有镜像仓库，通过 docker load 、 docker tag 、 docker push 命令，将镜像 tar 包导入私有镜像仓库。 离线安装。 执行 istioctl install 命令，并指定镜像仓库参数： istioctl install --set profile=demo --set values.global.hub=192.168.161.100/istio -y 参数 --set values.global.hub=xx，用于设置 istio 安装所需镜像的私有仓库。更多 --set 参数值参考https://istio.io/latest/docs/reference/config/istio.operator.v1alpha1/#IstioOperatorSpec 对于ARM架构，截止2022年8月，isito 官方暂未提供相应的镜像，可将 values.global.hub 设置为 ghcr.io/resf/istio 进行安装。 参考资料： Install with Istioctl Installation Configuration Profiles Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2022-08-08 18:24:11 "},"install/istio-uninstall.html":{"url":"install/istio-uninstall.html","title":"Istio 卸载","keywords":"","body":"Istio 卸载 在某些场景下，我们需要卸载 Istio,可参考本文进行卸载。 要从集群中完整卸载 Istio，运行下面卸载命令： istioctl x uninstall --purge 可选的 --purge 参数将删除所有的 Istio 资源，包括可能被其他 Istio 控制平面共享的、集群范围的资源。 或者，只删除指定的 Istio 控制平面，运行以下命令： istioctl x uninstall 或 istioctl manifest generate | kubectl delete -f - 控制平面的命名空间（例如：istio-system）默认不会删除， 如果确认不再需要，用下面命令删除它： kubectl delete namespace istio-system Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-10-12 23:03:24 "},"install/deploy-bookinfo-sample.html":{"url":"install/deploy-bookinfo-sample.html","title":"部署 Bookinfo 示例","keywords":"","body":"部署 Bookinfo 示例 部署官方 Bookinfo 示例应用。 该示例部署了一个用于演示多种 Istio 特性的应用，该应用由四个单独的微服务构成。 这个应用模仿在线书店的一个分类，显示一本书的信息。 页面上会显示一本书的描述，书籍的细节（ISBN、页数等），以及关于这本书的一些评论。 Bookinfo 应用分为四个单独的微服务： productpage：这个微服务会调用 details 和 reviews 两个微服务，用来生成页面。 details：这个微服务中包含了书籍的信息。 reviews：这个微服务中包含了书籍相关的评论。它还会调用 ratings 微服务。 ratings：这个微服务中包含了由书籍评价组成的评级信息。 reviews 微服务有 3 个版本： v1 版本不会调用 ratings 服务。 v2 版本会调用 ratings 服务，并使用 1 到 5 个黑色星形图标来显示评分信息。 v3 版本会调用 ratings 服务，并使用 1 到 5 个红色星形图标来显示评分信息。 下图展示了这个应用的端到端架构。 图 5.3.1：Bookinfo部署图 1、部署服务 进入 Istio 安装目录。 Istio 默认 自动注入 sidecar。请为 default 命名空间打上标签 istio-injection=enabled： $ kubectl label namespace default istio-injection=enabled namespace/default labeled 使用 kubectl apply -f 命令部署应用： kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml 确认所有的服务和 Pod 都已经正确的定义和启动： kubectl get service kubectl get pod 要确认 Bookinfo 应用是否正在运行，请在某个 Pod 中用 curl 命令对应用发送请求，例如 ratings： kubectl exec -it $(kubectl get pod -l app=ratings -o jsonpath='{.items[0].metadata.name}') -c ratings -- curl productpage:9080/productpage | grep -o \".*\" 2、确定 Ingress 的 IP 和端口 现在 Bookinfo 中的所有服务都启动并运行中，您需要使应用程序可以从外部访问，例如使用浏览器。可以通过 Istio Gateway 和 istio-ingress 来实现。 为应用程序定义 Ingress 网关 kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml 确认网关创建完成 $ kubectl get gateway NAME AGE bookinfo-gateway 32s 确认 Ingress 的 IP 和端口 执行如下命令，明确自身 Kubernetes 集群环境支持外部负载均衡： $ kubectl get svc istio-ingressgateway -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-ingressgateway LoadBalancer 10.108.112.232 localhost 15021:32133/TCP,80:31412/TCP,443:31507/TCP,31400:32717/TCP,15443:32369/TCP 22h 默认端口为 80。 3、确认可以从集群外部访问应用 用浏览器打开网址 http:///productpage，来浏览应用的 Web 页面。如果刷新几次应用的页面，就会看到 productpage 页面中会随机展示 reviews 服务的不同版本的效果（红色、黑色的星形或者没有显示）。reviews 服务出现这种情况是因为我们还没有使用 Istio 来控制版本的路由。 图 5.3.2：BookInfo应用页面 接下来的 Istio 学习中，可以使用此示例来验证 Istio 的流量路由、故障注入等功能。 4、卸载示例应用 当完成 Bookinfo 示例的实验后，如有需要可按照以下说明进行卸载和清理： 删除路由规则，并终止应用程序容器 samples/bookinfo/platform/kube/cleanup.sh 确认卸载 kubectl get virtualservices #-- there should be no virtual services kubectl get destinationrules #-- there should be no destination rules kubectl get gateway #-- there should be no gateway kubectl get pods #-- the Bookinfo pods should be deleted Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2022-05-24 20:43:22 "},"install/deploy-kiali.html":{"url":"install/deploy-kiali.html","title":"部署 Kiali","keywords":"","body":"部署 Kiali Istio 和几个遥测应用做了集成。 能帮你了解服务网格的结构、展示网络的拓扑结构、分析网格的健康状态等。 使用下面说明部署 Kiali 仪表板、 以及 Prometheus、 Grafana、 还有 Jaeger。 安装 Kiali 和其他插件，等待部署完成。 如果在安装插件时出错，再运行一次命令。 有一些和时间相关的问题，再运行就能解决。 $ kubectl apply -f samples/addons $ kubectl rollout status deployment/kiali -n istio-system Waiting for deployment \"kiali\" rollout to finish: 0 of 1 updated replicas are available... deployment \"kiali\" successfully rolled out 访问 Kiali 仪表板。 istioctl dashboard kiali --address 192.168.161.233 --address：Kiali 仪表板监听地址，即：用于访问的IP。 用浏览器打开网址 http://:20001/kiali，就可以访问 Kiali 仪表板。在左侧的导航菜单，选择 Graph ，然后在 Namespace 下拉列表中，选择 default 。 Kiali 仪表板展示了网格的概览、以及 Bookinfo 示例应用的各个服务之间的关系。 它还提供过滤器来可视化流量的流动。 图 5.4.1：Kiali Graph Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2022-05-24 20:43:22 "},"install/istio-upgrade.html":{"url":"install/istio-upgrade.html","title":"升级","keywords":"","body":"升级 众所周知，Istio 目前属于快速发展时期，版本的更新也是很快，Istio 框架升级也是必须要考虑的一个重要环节。目前，Istio 官方也给出多种升级方法供大家根据实际情况选择。 以 Istio 1.9.0 版本向 1.10.0 版本升级为例进行说明。 1、金丝雀升级 金丝雀升级，是一种渐进式的升级方式，可以让新老版本的 istiod 同时存在，并可通过流量控制先将一小部分流量路由到新版本的 istiod 上管控，逐步完成新版本的升级。 该种方式比较安全，也是大家比较推荐的升级方法。 首先需 下载新版本的 Istio，将其上传至服务器，并切换到新版本的目录。 注意：接下来一定要使用新版本的 istioctl 命令，否则将会升级失败！（可参考 Istio 安装 中修改 istioctl path 环境变量，或根据 istioctl 的路径使用，如：./bin/istioctl） 1.1 控制平面升级 安装灰度 canary 版本，将 revision 字段设置为 canary： istioctl install --set revision=canary --set values.global.hub=192.168.162.47/istio -y 根据 istiotcl 版本来决定升级的 canary 版本，所以需确保执行 istiotcl 命令的版本为要升级 istio 的版本。 上述执行成功后，会部署一个新的 istiod-canary，即：新版本的控制平面，该新的控制平面并不会对原有的控制平面产生影响，此时会有新、旧两个控制平面同时存在： $ kubectl get pods -n istio-system -l app=istiod NAME READY STATUS RESTARTS AGE istiod-786779888b-p9s5n 1/1 Running 0 114m istiod-canary-6956db645c-vwhsk 1/1 Running 0 1m 此外，也会同时存在 2 个 service 和 sidecar-injector： $ kubectl get svc -n istio-system -l app=istiod NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istiod ClusterIP 10.32.5.247 15010/TCP,15012/TCP,443/TCP,15014/TCP 33d istiod-canary ClusterIP 10.32.6.58 15010/TCP,15012/TCP,443/TCP,15014/TCP,53/UDP,853/TCP 12m $ kubectl get mutatingwebhookconfigurations NAME WEBHOOKS AGE istio-sidecar-injector 1 7m56s istio-sidecar-injector-canary 1 3m18s 1.2 数据平面升级 只安装 canary 版本的控制平面 istio-canary 并不会对现有的代理造成影响，要升级数据平面，需将他们指向新的控制平面，需要在 namespace 中插入 istio.io/rev 标签。 例如，想要升级 default namespace 的数据平面，需要添加 istio.io/rev 标签以指向 canary 版本的控制平面，并删除 istio-injection 标签： kubectl label namespace default istio-injection- istio.io/rev=canary 注意：istio-injection 标签必须删除，因为该标签的优先级高于 istio.io/rev 标签，保留该标签将导致无法升级数据平面。 在 namespace 的标签更新成功后，需要重启 Pod 来重新注入 Sidecar： kubectl rollout restart deployment -n default 当重启成功后，该 namespace 的 Pod 将被配置指向新的 istiod-canary 控制平面，使用如下命令查看启用新代理的 Pod： kubectl get pods -n default -l istio.io/rev=canary 同时可以使用如下命令 istioctl proxy-status 查看新 Pod 的控制平面是否为 istiod-canary 及是否为新版本号： istioctl proxy-status 目前 Istio 1.10.0 版本在金丝雀升级时，istio-egressgateway 并未升级，可能是该版本存在的 bug，期待后续版本更新。 1.3 卸载旧的控制平面 升级控制平面和数据平面之后，您可以卸载旧的控制平面。例如，以下命令可以卸载旧的控制平面 1-9-0： istioctl x uninstall --revision 1-9-0 卸载前提是，旧版本的控制平面没有被使用。 如果旧的控制平面没有 revision 版本标签，请使用其原始安装选项将其卸载，例如： istioctl x uninstall -f manifests/profiles/default.yaml 通过以下方式可以确认旧的控制平面已被移除，并且集群中仅存在新的控制平面： $ kubectl get pods -n istio-system -l app=istiod NAME READY STATUS RESTARTS AGE istiod-canary-55887f699c-t8bh8 1/1 Running 0 27m 请注意，以上说明仅删除了用于指定控制平面修订版的资源，而未删除与其他控制平面共享的群集作用域资源。要完全卸载 istio，请参阅卸载。 1.4 卸载金丝雀控制平面（回滚） 如果您想要回滚到旧的控制平面，而不是完成金丝雀升级，则可以卸载 Canary 版本： istioctl x uninstall --revision=canary 但是，在这种情况下，您必须首先手动重新安装先前版本的网关，因为卸载命令不会自动还原先前升级的网关。 确保使用与 istioctl 旧控制平面相对应的版本来重新安装旧网关，并且为避免停机，请确保旧网关已启动并正在运行，然后再进行金丝雀卸载。 2、原地升级 通过 istioctl upgrade 命令将对 Istio 进行升级。在执行升级之前，它会检查 Istio 安装是否满足升级资格标准。另外，如果它检测到 Istio 版本之间的配置文件默认值有任何更改，也会警告用户。 目前原地升级有很大的概率通不过升级检测，导致无法升级，不推荐这种升级方式。期待后续版本更好的支持。 官方原地升级文档：https://istio.io/latest/docs/setup/upgrade/in-place/ Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-10-27 00:01:31 "},"traffic/":{"url":"traffic/","title":"概述","keywords":"","body":"流量管理 流量管理，是对整个系统流量的管控，其中包括了对服务网格的入口流量、出口流量以及网格内部服务间的流量管控。 Istio 的流量路由规则可以让您轻松地控制服务之间的流量和 API 调用。 Istio 简化了服务级别属性的配置(如，熔断器、超时和重试等)，并使设置重要任务(如，A/B 测试、canary 部署和基于百分比的流量分割的分阶段部署)变得容易。 它还提供了开箱即用的故障恢复特性，帮助您的应用程序更健壮地应对依赖服务或网络的故障。 本章节将针对 Istio 中流量管理展开详细说明，通过 Istio 提供丰富的资源配置项来解决流量管控方面的需求。 Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-10-27 00:50:10 "},"traffic/crd/traffic-crd.html":{"url":"traffic/crd/traffic-crd.html","title":"资源配置","keywords":"","body":"资源配置 Istio 的流量管理是通过一系列的 CRD（Kubernetes 的自定义资源）来实现的，包括以下这些资源： VirtualService：虚拟服务，用来定义路由规则，控制请求如何被路由到某个服务。 DestinationRule：目标规则，用来配置请求策略。 Gateway：网关，在网格的入口设置负载、控制流量等。 ServiceEntry：服务入口，用来定义外部如何访问服务网格。 EnvoyFilter： Sidecar： Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-10-23 21:28:30 "},"traffic/crd/virtual-service.html":{"url":"traffic/crd/virtual-service.html","title":"VirtualService","keywords":"","body":"VirtualService VirtualService 与 DestinationRule 是流量控制最关键的两个自定义资源。在 VirtualService 中定义了一组路由规则，当流量进入时，逐个规则进行匹配，直到匹配成功后将流量转发给指定的路由地址。 图 6.2.1.1：VirtualService流程图 配置项 下图是 VirtualService 的资源配置项： 图 6.2.1.2：VirtualService 资源配置项 示例 （以 Bookinfo 示例，将 Reviews 服务路由到v1版本） apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - route: - destination: host: reviews subset: v1 --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: reviews spec: host: reviews subsets: - name: v1 labels: version: v1 配置项说明： hosts：用来配置下游访问的可寻址地址。配置一个 String[] 类型的值，可以配置多个。指定了发送流量的目标主机， 可以使用FQDN（Fully Qualified Domain Name - 全限定域名）或者短域名， 也可以一个前缀匹配的域名格式或者一个具体的 IP 地址。 match：这部分用来配置路由规则，通常情况下配置一组路由规则，当请求到来时，自上而下依次进行匹配，直到匹配成功后跳出匹配。它可以对请求的 uri、method、authority、headers、port、queryParams 以及是否对 uri 大小写敏感等进行配置。 route：用来配置路由转发目标规则，可以指定需要访问的 subset （服务子集），同时可以对请求权重进行设置、对请求头、响应头中数据进行增删改等操作。subset （服务子集）是指同源服务而不同版本的 Pod，通常在 Deployment 资源中设置不同的 label 来标识。 更多详细配置项说明参考：https://istio.io/latest/zh/docs/reference/config/networking/virtual-service/#VirtualService Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2022-05-28 21:47:56 "},"traffic/crd/destination-rule.html":{"url":"traffic/crd/destination-rule.html","title":"DestinationRule","keywords":"","body":"DestinationRule DestinationRule 是 Istio 中定义的另外一个比较重要的资源，它定义了网格中某个 Service 对外提供服务的策略及规则，包括负载均衡策略、异常点检测、熔断控制、访问连接池等。 负载均衡策略支持简单的负载策略（ROUND_ROBIN、LEAST_CONN、RANDOM、PASSTHROUGH）、一致性 Hash 策略和区域性负载均衡策略。 异常点检测配置在服务连续返回了5xx的错误时进行及时的熔断保护，避免引起雪崩效应。DestinationRule 也可以同 VirtualService 配合使用实现对同源服务不同子集服务的访问配置。 Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-10-23 21:28:30 "},"traffic/crd/gateway.html":{"url":"traffic/crd/gateway.html","title":"Gateway","keywords":"","body":"Gateway Gateway，一个运行在网格边缘的负载均衡器，定义了所有 HTTP/TCP 流量进出服务网格的统一进出口。它描述了一组对外公开的端口、协议、负载均衡、以及 SNI 配置。 Istio Gateway 包括 Ingress Gateway 与 Egress Gateway，分别用来配置网格的入口流量与出口流量。Ingress Gateway 使用 istio-ingressgateway 负载均衡器来代理流量，而 istio-ingressgateway 实际上是一个 Envoy 代理。 图 6.2.3.1：Gateway流程图 示例 一个简单示例如下： apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: my-gateway namespace: some-config-namespace spec: selector: app: my-gateway-controller servers: - port: number: 80 name: http protocol: HTTP hosts: - uk.bookinfo.com - eu.bookinfo.com tls: httpsRedirect: true # sends 301 redirect for http requests - port: number: 443 name: https-443 protocol: HTTPS hosts: - uk.bookinfo.com - eu.bookinfo.com tls: mode: SIMPLE # enables HTTPS on this port serverCertificate: /etc/certs/servercert.pem privateKey: /etc/certs/privatekey.pem - port: number: 9443 name: https-9443 protocol: HTTPS hosts: - \"bookinfo-namespace/*.bookinfo.com\" tls: mode: SIMPLE # enables HTTPS on this port credentialName: bookinfo-secret # fetches certs from Kubernetes secret - port: number: 9080 name: http-wildcard protocol: HTTP hosts: - \"ns1/*\" - \"ns2/foo.bar.com\" - port: number: 2379 # to expose internal service via external port 2379 name: mongo protocol: MONGO hosts: - \"*\" 该示例中， Gateway 被引用在 some-config-namespace 这个 Namespace 下，并使用 label my-gateway-controller 来关联部署网络代理的 Pod ，对外公开了 80、443、9443、9080、2379 端口的服务。 80 端口：附属配置的 host 为uk.bookinfo.com，eu.bookinfo.com，同时在 tls 中配置了 httpsRedirect。如果使用 HTTP1.1 协议访问将会返回 301，要求使用 HTTPS 访问，通过这种配置变相的禁止了对 uk.bookinfo.com，eu.bookinfo.com 域名的 HTTP1.1 协议的访问入口。 443 端口：提供TLS/HTTPS 的访问，表示接受 uk.bookinfo.com，eu.bookinfo.com 域名的 HTTPS 协议的访问，protocol 属性指定了协议类型。在 tls 的配置中指定了会话模式为单向 TLS（mode: SIMPLE） ，同时指定了服务端证书和私钥的存放地址。 9443 端口：提供TLS/HTTPS 的访问，与 443 端口不同的是证书不是指定存放证书文件的地址，而是通过 credentialName 属性配置从 Kubernetes 的证书管理中心拉取证书。 9080 端口：提供简单的 HTTP1.1 协议的访问。 hosts 中配置了 ns1/* 与 ns2/foo.bar.com，表示只允许ns1 Namespace 下的 VirtualService 绑定它以及 ns2 Namespace下配置了 host 为 foo.bar.com 的 VirtualService 绑定它。 2379 端口：提供 MONGO 协议的访问，允许所有 host 绑定它。 Egress Gateway 提供了对网格的出口流量进行统一管控的功能，在安装 Istio 时默认是不开启的。可以使用以下命令查看是否开启： kubectl get pod -l istio=egressgateway -n istio-system 若没有开启，使用以下命令添加。 istioctl manifest apply --set values.global.istioNamespace=istio-system \\ --set values.gateways.istio-egressgateway.enabled=true Egress Gateway 的一个简单示例如下： apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: istio-egressgateway spec: selector: istio: egressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - edition.cnn.com 可以看出，与 Ingress Gateway不同，Egress Gateway 使用有 istio: egressgateway 标签的 Pod 来代理流量，实际上这是一个 Envoy 代理。当网格内部需要访问 edition.cnn.com 这个地址时，流量将会统一先转发到 Egress Gateway 上，再由 Egress Gateway 将流量转发到 edition.cnn.com 上。 Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-10-23 21:28:30 "},"traffic/crd/service-entry.html":{"url":"traffic/crd/service-entry.html","title":"ServiceEntry","keywords":"","body":"ServiceEntry ServiceEntry ，将网格外的服务注册到 Istio 的注册表中，这样就可以把外部服务当做网格内部的服务一样进行管理和操作。包括服务发现、路由控制等，在 ServiceEntry 中可以配置 hosts，vips，ports，protocols，endpoints等。 图 6.2.4.1：ServiceEntry流程图 示例 它的一个简单示例如下： apiVersion: networking.istio.io/v1alpha3 kind: ServiceEntry metadata: name: external-svc-https spec: hosts: - api.dropboxapi.com - www.googleapis.com - api.facebook.com location: MESH_EXTERNAL ports: - number: 443 name: https protocol: TLS resolution: DNS 该示例中，定义了在网格内部使用 HTTPS 协议访问外部的几个服务的配置。通过上面配置，网格内部的服务就可以把 api.dropboxapi.com，www.googleapis.com, www.googleapis.com 这几个外部的服务当做网格内部服务去访问。MESH_EXTERNAL 表示是网格外服务，该参数会影响到服务间调用的 mTLS 认证、策略执行等。 Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-10-23 21:28:30 "},"traffic/crd/envoy-filter.html":{"url":"traffic/crd/envoy-filter.html","title":"EnvoyFilter","keywords":"","body":"EnvoyFilter EnvoyFilter 提供了一种机制来定制 Istio Pilot 生成的 Envoy 配置。使用 EnvoyFilter 来修改某些字段的值，添加特定的过滤器，甚至添加全新的 listener、cluster 等。这个功能必须谨慎使用，因为不正确的配置可能破坏整个网格的稳定性。与其他 Istio 网络对象不同，EnvoyFilter 对应用是累加生效的。对于特定命名空间中的特定工作负载，可以存在任意数量的 EnvoyFilter，并累加生效。这些 EnvoyFilter 的生效顺序如下：配置根命名空间中的所有 EnvoyFilter，其次是工作负载命名空间中的所有匹配 EnvoyFilter。 注意： 该 API 的某些方面与 Istio 网络子系统的内部实现以及 Envoy 的 xDS API 有很深的关系。虽然 EnvoyFilter API 本身将保持向后兼容，但通过该机制提供的任何 Envoy 配置应在 Istio 代理版本升级时仔细审查，以确保废弃的字段被适当地删除和替换。 当多个 EnvoyFilter 被绑定到特定命名空间的同一个工作负载时，所有补丁将按照创建顺序处理。如果多个 EnvoyFilter 的配置相互冲突，则其行为将无法确定。 要将 EnvoyFilter 资源应用于系统中的所有工作负载（sidecar 和 gateway）上，请在 config 根命名空间中定义该资源，不要使用 workloadSelector。 1、配置项 下图是 EnvoyFilter 的资源配置项： 图 6.2.5.1：EnvoyFilter 资源配置项 2、示例 2.1 前提准备 2.1.1 开启注入 服务需注入边车，对其运行命名空间添加自动注入标签 istio-injection=enabled： kubectl label namespace samples istio-injection=enabled 2.1.2 服务准备 通过服务 sleep 发起请求，调用服务 helloworld 的 /hello 完成相应功能的验证。 sleep。 kubectl apply -f samples/sleep/sleep.yaml -n samples helloworld。 1）部署服务 helloworld。 kubectl apply -f samples/helloworld/helloworld.yaml -n samples 2）部署 helloworld gateway。 kubectl apply -f samples/helloworld/helloworld-gateway.yaml -n samples 2.2 添加 HTTP 响应头 在应用程序中添加 HTTP 响应头可以提高 Web 应用程序的安全性。本示例介绍如何通过定义 EnvoyFilter 添加HTTP 响应头。 OWASP(Open Web Application Security Project) 提供了最佳实践指南和编程框架，描述了如何使用安全响应头保护应用程序的安全。HTTP 响应头的基准配置如下： HTTP响应头 默认值 描述 Content-Security-Policy frame-ancestors none; 防止其他网站进行Clickjacking攻击。 X-XSS-Protection 1;mode=block 激活浏览器的XSS过滤器（如果可用），检测到XSS时阻止渲染。 X-Content-Type-Options Nosniff 禁用浏览器的内容嗅探。 Referrer-Policy no-referrer 禁用自动发送引荐来源。 X-Download-Options noopen 禁用旧版本IE中的自动打开下载功能。 X-DNS-Prefetch-Control off 禁用对页面上的外部链接的推测性DNS解析。 Server envoy 由Istio的入口网关自动设置。 X-Powered-by 无默认值 去掉该值来隐藏潜在易受攻击的应用程序服务器的名称和版本。 Feature-Policy camera ‘none’; microphone ‘none’;geolocation ‘none’;encrypted-media ‘none’;payment ‘none’;speaker ‘none’;usb ‘none’; 控制可以在浏览器中使用的功能和API。 2.2.1 边车 具体服务上生效。 创建 Envoyfilter。 kubectl apply -f samples/envoyfilter/ef-add-response-headers-into-sidecar.yaml -n samples 注：与服务在相同的 namespace。 apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: ef-add-response-headers-into-sidecar spec: workloadSelector: # select by label in the same namespace labels: app: helloworld configPatches: # The Envoy config you want to modify - applyTo: HTTP_FILTER match: context: SIDECAR_INBOUND listener: filterChain: filter: name: \"envoy.filters.network.http_connection_manager\" subFilter: name: \"envoy.filters.http.router\" patch: operation: INSERT_BEFORE value: # lua filter specification name: envoy.lua typed_config: \"@type\": \"type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\" inlineCode: |- function envoy_on_response(response_handle) function hasFrameAncestors(rh) s = rh:headers():get(\"Content-Security-Policy\"); delimiter = \";\"; defined = false; for match in (s..delimiter):gmatch(\"(.-)\"..delimiter) do match = match:gsub(\"%s+\", \"\"); if match:sub(1, 15)==\"frame-ancestors\" then return true; end end return false; end if not response_handle:headers():get(\"Content-Security-Policy\") then csp = \"frame-ancestors none;\"; response_handle:headers():add(\"Content-Security-Policy\", csp); elseif response_handle:headers():get(\"Content-Security-Policy\") then if not hasFrameAncestors(response_handle) then csp = response_handle:headers():get(\"Content-Security-Policy\"); csp = csp .. \";frame-ancestors none;\"; response_handle:headers():replace(\"Content-Security-Policy\", csp); end end if not response_handle:headers():get(\"X-Frame-Options\") then response_handle:headers():add(\"X-Frame-Options\", \"deny\"); end if not response_handle:headers():get(\"X-XSS-Protection\") then response_handle:headers():add(\"X-XSS-Protection\", \"1; mode=block\"); end if not response_handle:headers():get(\"X-Content-Type-Options\") then response_handle:headers():add(\"X-Content-Type-Options\", \"nosniff\"); end if not response_handle:headers():get(\"Referrer-Policy\") then response_handle:headers():add(\"Referrer-Policy\", \"no-referrer\"); end if not response_handle:headers():get(\"X-Download-Options\") then response_handle:headers():add(\"X-Download-Options\", \"noopen\"); end if not response_handle:headers():get(\"X-DNS-Prefetch-Control\") then response_handle:headers():add(\"X-DNS-Prefetch-Control\", \"off\"); end if not response_handle:headers():get(\"Feature-Policy\") then response_handle:headers():add(\"Feature-Policy\", \"camera 'none';\".. \"microphone 'none';\".. \"geolocation 'none';\".. \"encrypted-media 'none';\".. \"payment 'none';\".. \"speaker 'none';\".. \"usb 'none';\"); end if response_handle:headers():get(\"X-Powered-By\") then response_handle:headers():remove(\"X-Powered-By\"); end end 验证。 1）进入 sleep 服务容器内。 $ kubectl exec -it $(kubectl get pod -n samples -l app=sleep -o jsonpath='{.items[0].metadata.name}') -c sleep -n samples sh 2）调用 /hello 接口。接口响应头中包含额外添加的响应头，则说明创建的 EnvoyFilter 生效。 $ curl -i helloworld:5000/hello HTTP/1.1 200 OK content-type: text/html; charset=utf-8 content-length: 60 server: envoy date: Wed, 03 Aug 2022 08:06:59 GMT x-envoy-upstream-service-time: 163 content-security-policy: frame-ancestors none; x-frame-options: deny x-xss-protection: 1; mode=block x-content-type-options: nosniff referrer-policy: no-referrer x-download-options: noopen x-dns-prefetch-control: off feature-policy: camera 'none';microphone 'none';geolocation 'none';encrypted-media 'none';payment 'none';speaker 'none';usb 'none'; Hello version: v1, instance: helloworld-v1-6874cd9dcd-ddnrh 2.2.2 istio-ingress ingress 上生效。 创建 Envoyfilter。 kubectl apply -f samples/envoyfilter/ef-add-response-headers-into-ingressgateway.yaml -n istio-system apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: ef-add-response-headers-into-ingressgateway spec: workloadSelector: # select by label in the same namespace labels: istio: ingressgateway configPatches: # The Envoy config you want to modify - applyTo: HTTP_FILTER match: context: GATEWAY listener: filterChain: filter: name: \"envoy.filters.network.http_connection_manager\" subFilter: name: \"envoy.filters.http.router\" patch: operation: INSERT_BEFORE value: # lua filter specification name: envoy.lua typed_config: \"@type\": \"type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\" inlineCode: |- function envoy_on_response(response_handle) function hasFrameAncestors(rh) s = rh:headers():get(\"Content-Security-Policy\"); delimiter = \";\"; defined = false; for match in (s..delimiter):gmatch(\"(.-)\"..delimiter) do match = match:gsub(\"%s+\", \"\"); if match:sub(1, 15)==\"frame-ancestors\" then return true; end end return false; end if not response_handle:headers():get(\"Content-Security-Policy\") then csp = \"frame-ancestors none;\"; response_handle:headers():add(\"Content-Security-Policy\", csp); elseif response_handle:headers():get(\"Content-Security-Policy\") then if not hasFrameAncestors(response_handle) then csp = response_handle:headers():get(\"Content-Security-Policy\"); csp = csp .. \";frame-ancestors none;\"; response_handle:headers():replace(\"Content-Security-Policy\", csp); end end if not response_handle:headers():get(\"X-Frame-Options\") then response_handle:headers():add(\"X-Frame-Options\", \"deny\"); end if not response_handle:headers():get(\"X-XSS-Protection\") then response_handle:headers():add(\"X-XSS-Protection\", \"1; mode=block\"); end if not response_handle:headers():get(\"X-Content-Type-Options\") then response_handle:headers():add(\"X-Content-Type-Options\", \"nosniff\"); end if not response_handle:headers():get(\"Referrer-Policy\") then response_handle:headers():add(\"Referrer-Policy\", \"no-referrer\"); end if not response_handle:headers():get(\"X-Download-Options\") then response_handle:headers():add(\"X-Download-Options\", \"noopen\"); end if not response_handle:headers():get(\"X-DNS-Prefetch-Control\") then response_handle:headers():add(\"X-DNS-Prefetch-Control\", \"off\"); end if not response_handle:headers():get(\"Feature-Policy\") then response_handle:headers():add(\"Feature-Policy\", \"camera 'none';\".. \"microphone 'none';\".. \"geolocation 'none';\".. \"encrypted-media 'none';\".. \"payment 'none';\".. \"speaker 'none';\".. \"usb 'none';\"); end if response_handle:headers():get(\"X-Powered-By\") then response_handle:headers():remove(\"X-Powered-By\"); end end 验证。 1）确认 ingress 地址。 $ kubectl cluster-info Kubernetes control plane is running at https://192.168.1.1:16443 To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. $ kubectl get svc -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-egressgateway ClusterIP 10.111.176.85 80/TCP,443/TCP,15443/TCP 5d4h istio-ingressgateway LoadBalancer 10.101.141.144 15021:31873/TCP,80:31064/TCP,443:30191/TCP,31400:30943/TCP,15443:31029/TCP 5d4h istiod ClusterIP 10.105.147.56 15010/TCP,15012/TCP,443/TCP,15014/TCP 5d4h 从上述结果中得知，ingress 地址为 http://192.168.1.1:31064。 2）通过 ingress 访问 hello 接口。接口响应头中包含额外添加的响应头，则说明创建的 EnvoyFilter 生效。 $ curl -i http://192.168.1.1:31064/hello HTTP/1.1 200 OK content-type: text/html; charset=utf-8 content-length: 60 server: istio-envoy date: Wed, 03 Aug 2022 07:43:03 GMT x-envoy-upstream-service-time: 159 content-security-policy: frame-ancestors none; x-frame-options: deny x-xss-protection: 1; mode=block x-content-type-options: nosniff referrer-policy: no-referrer x-download-options: noopen x-dns-prefetch-control: off feature-policy: camera 'none';microphone 'none';geolocation 'none';encrypted-media 'none';payment 'none';speaker 'none';usb 'none'; Hello version: v1, instance: helloworld-v1-6874cd9dcd-ddnrh 2.3 添加直接响应 对于发往指定服务的指定路径的http请求，不再向服务转发请求，而是立即返回固定的响应内容。 创建EnvoyFilter。 kubectl apply -f samples/envoyfilter/ef-envoy-direct-response.yaml -n samples apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: ef-envoy-direct-response spec: workloadSelector: labels: app: helloworld configPatches: - applyTo: NETWORK_FILTER match: context: SIDECAR_INBOUND listener: filterChain: filter: name: \"envoy.filters.network.http_connection_manager\" patch: operation: REPLACE value: name: envoy.filters.network.http_connection_manager typed_config: \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager stat_prefix: hello route_config: name: my_first_route virtual_hosts: - name: direct_response_service domains: [\"helloworld.samples\"] routes: - match: prefix: \"/\" direct_response: status: 200 body: inline_string: \"envoy direct response.\" http_filters: - name: envoy.filters.http.router typed_config: \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router 验证。 1）进入 sleep 服务容器内。 $ kubectl exec -it $(kubectl get pod -n samples -l app=sleep -o jsonpath='{.items[0].metadata.name}') -c sleep -n samples sh 2）调用 /hello 接口。返回结果为 envoy direct response，则说明创建的 EnvoyFilter 生效。 $ curl -i http://192.168.1.1:31064/hello HTTP/1.1 200 OK content-type: text/html; charset=utf-8 content-length: 60 server: istio-envoy date: Wed, 03 Aug 2022 07:43:03 GMT x-envoy-upstream-service-time: 3 envoy direct response. 参考 Envoy Filter 在ASM中通过EnvoyFilter添加HTTP响应头 Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2022-08-13 12:59:03 "},"traffic/crd/sidecar.html":{"url":"traffic/crd/sidecar.html","title":"Sidecar","keywords":"","body":"Sidecar 默认情况下，Istio 让每个 Envoy 代理都可以访问来自和它关联的应用服务的所有端口请求，然后转发到对应的应用服务。而通过 Sidecar 资源配置可以做更多的事情，如： 调整 Envoy 代理接受的端口和协议集。 限制 Envoy 代理可以访问的服务集合。 例如，下面的 Sidecar 配置将 bookinfo 命名空间中的所有服务配置为仅能访问运行在相同命名空间和 Istio 控制平面中的服务： apiVersion: networking.istio.io/v1alpha3 kind: Sidecar metadata: name: default namespace: bookinfo spec: egress: - hosts: - \"./*\" - \"istio-system/*\" Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-10-23 21:28:30 "},"traffic/route.html":{"url":"traffic/route.html","title":"路由","keywords":"","body":"路由 本章节用来验证服务的路由功能。 在 Istio 中服务路由完全由 VirtualService 资源完成，通过定义请求匹配方式、路由规则来完成。 基于请求内容路由 基于多版本权重路由 Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2022-06-06 14:36:49 "},"traffic/load-balancing.html":{"url":"traffic/load-balancing.html","title":"负载均衡","keywords":"","body":"负载均衡 Istio 中的负载均衡是基于 Sidecar 实现，并通过 DestinationRule 中的 loadBalancer 完成负载均衡的配置，目前支持以下负载均衡算法： 标准算法： ROUND_ROBIN：轮询算法，默认。 LEAST_CONN：权重最小请求算法。该算法选择两个随机的健康主机，并选择活动请求较少的主机。 RANDOM：随机算法。该算法选择一个随机的健康主机。如果未配置运行状况检查策略，则随机负载均衡器的性能通常比轮询更好。 PASSTHROUGH：该算法将连接转发到调用者请求的原始 IP 地址，而不进行任何形式的负载平衡。需谨慎使用，它适用于特殊场景。 consistentHash：一致 Hash 算法。该算法可提供基于 HTTP 头、Cookie 等一致 Hash 算法，仅适用于HTTP 请求，常作为基于会话保持的负载均衡算法。 localityLbSetting：地域负载均衡。提供了地域感知的能力，简单说来，就是在分区部署的较大规模的集群，或者公有云上，Istio 负载均衡可以根据节点的区域标签，对调用目标做出就近选择。这些区域是使用任意标签指定的，这些标签以{region} / {zone} / {sub-zone}形式指定区域的层次结构。 更多关于 DestinationRule 中的 loadBalancer 具体配置说明可参考：LoadBalancerSettings。 例如，采用 ROUND_ROBIN 轮询负载均衡策略将流量转发到 ratings 服务。 apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: bookinfo-ratings spec: host: ratings.prod.svc.cluster.local trafficPolicy: loadBalancer: simple: ROUND_ROBIN Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-10-30 22:37:19 "},"traffic/traffic-shadow.html":{"url":"traffic/traffic-shadow.html","title":"流量镜像","keywords":"","body":"流量镜像 流量镜像（Mirroring / traffic-shadow），也称为影子流量，是一个以尽可能低的风险为生产带来变化的强大的功能。镜像会将实时流量的副本发送到镜像服务。镜像流量发生在主服务的关键请求路径之外。 …… Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-10-30 22:37:19 "},"traffic/ingress.html":{"url":"traffic/ingress.html","title":"Ingress","keywords":"","body":"Ingress Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-10-27 00:50:10 "},"traffic/egress.html":{"url":"traffic/egress.html","title":"Egress","keywords":"","body":"Egress Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-10-27 00:50:10 "},"traffic/circuit-breaking.html":{"url":"traffic/circuit-breaking.html","title":"熔断","keywords":"","body":"熔断 Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-10-27 00:50:10 "},"traffic/fault-injection.html":{"url":"traffic/fault-injection.html","title":"故障注入","keywords":"","body":"故障注入 故障注入（Fault Injection），即：故障测试，是 Istio 提供了一种无侵入式的故障注入机制，让开发测试人员在不用调整服务程序的前提下，通过配置即可完成对服务的异常模拟，完成对系统的定向错误测试。 通过引入故障来模拟网络传输中的问题（如延迟）来验证系统的健壮性，方便完成系统的各类故障测试。通过配置上游主机的 VirtualService 来实现，当我们在 VirtualService 中配置了故障注入时，上游服务的 Sidecar 代理在拦截到请求之后就会做出相应的响应。 目前，Istio 提供两种类型的故障注入，abort 类型与 delay 类型。 abort：非必配项，配置一个 Abort 类型的对象。用来注入请求异常类故障。简单的说，就是用来模拟上游服务对请求返回指定异常码时，当前的服务是否具备处理能力。它对应于 Envoy 过滤器中的 config.filter.http.fault.v2.FaultAbort 配置项，当 VirtualService 资源应用时，Envoy 将会该配置加载到过滤器中并处理接收到的流量。 delay：非必配项，配置一个 Delay 类型的对象。用来注入延时类故障。通俗一点讲，就是人为模拟上游服务的响应时间，测试在高延迟的情况下，当前的服务是否具备容错容灾的能力。它对应于 Envoy 过滤器中的 config.filter.fault.v2.FaultDelay 配置型，同样也是在应用 Istio 的 VirtualService 资源时，Envoy 将该配置加入到过滤器中。 实际上，Istio 的故障注入是基于 Envoy 的 config.filter.http.fault.v2.HTTPFault 过滤器实现的，它的局限性也来自于 Envoy 故障注入机制的局限性。对于 Envoy 的 HttpFault 的详细介绍请参考Envoy 文档。对比 Istio 故障注入的配置项与 Envoy 故障注入的配置项，不难发现，Istio 简化了对于故障控制的手段，去掉了 Envoy 中通过 HTTP header 控制故障注入的配置。 HTTPFaultInjection.Abort： httpStatus：必配项，是一个整型的值。表示注入 HTTP 请求的故障状态码。 percentage：非必配项，是一个 Percent 类型的值。表示对多少请求进行故障注入。如果不指定该配置，那么所有请求都将会被注入故障。 HTTPFaultInjection.Delay： fixedDelay：必配项，表示请求响应的模拟处理时间。格式为：1h/1m/1s/1ms， 不能小于 1ms。 percentage：非必配项，是一个 Percent 类型的值。表示对多少请求进行故障注入。如果不指定该配置，那么所有请求都将会被注入故障。 Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-10-30 22:37:19 "},"traffic/gray-release.html":{"url":"traffic/gray-release.html","title":"灰度发布","keywords":"","body":"灰度发布 灰度发布，又叫做“金丝雀发布”，是指选择一小部分用户作为新版本的测试对象，当新版本运行稳定后，再逐步将更多的流量切换到新版本，直到将 100% 的流量都切换到新版本上，最后关闭剩下的老版本服务，完成灰度发布。 灰度发布，用来实现业务从老版本到新版本的平滑过渡，并避免升级过程中出现的问题对用户造成的影响。 “金丝雀发布”，来源于矿工们用金丝雀对矿井进行空气测试的做法。以前矿工挖煤的时候，矿工下矿井前会先把金丝雀放进去，或者挖煤的时候一直带着金丝雀。金丝雀对甲烷和一氧化碳浓度比较敏感，会先报警。所以大家都用“金丝雀”来搞最先的测试。 例如，下图中，左下方的少部分用户就被当作“金丝雀”来用于测试新上线的 1.1 版本。如果新版本出现问题，“金丝雀”们会报警，但不会影响其他用户业务的正常运行。 图 6.13.1：灰度发布 灰度发布的流程如下： 准备和生产环境隔离的“金丝雀”服务器。 将新版本的服务部署到“金丝雀”服务器上。 对“金丝雀”服务器上的服务进行自动化和人工测试。 测试通过后，将“金丝雀”服务器连接到生产环境，将少量生产流量导入到“金丝雀”服务器中。 如果在线测试出现问题，则通过把生产流量从“金丝雀”服务器中重新路由到老版本的服务的方式进行回退，修复问题后重新进行发布。 如果在线测试顺利，则逐渐把生产流量按一定策略逐渐导入到新版本服务器中。 待新版本服务稳定运行后，删除老版本服务。 1、Istio 中灰度发布的实现 灰度发布的实现，核心技术是要提供一种机制满足多不版本同时在线，并能够灵活配置规则给不同的版本分配流量。 Istio 本身并没有关于灰度发布的规则定义，灰度发布只是流量管控规则的一种典型应用，在进行灰度发布时，只要写个简单的流量规则配置（ VirtualService 规则配置）即可。 Istio 在每个 Pod 里都注入了一个 Envoy，因而只要在控制面配置分流策略，对目标服务发起访问的每个 Envoy 便都可以执行流量策略，完成灰度发布功能。 Istio 灰度发布支持两类灰度策略： 基于流量比例的策略：配置 VirtualService 中的路由权重 weight 来实现基于流量比例的灰度发布。 基于请求内容的策略：配置 VirtualService 中的路由请求头 headers 来实现基于请求内容的策略。该策略是非常灵活的，比如某个特性是专门为 Mac 操作系统开发的，则在该版本的流量策略中需要匹配请求方的操作系统。浏览器、请求的 Headers 等请求内容在 Istio 中都可以作为灰度发布的特征条件。 采用 kubernetes 的滚动升级(rolling update)功能也可以实现不中断业务的应用升级，但滚动升级是通过逐渐使用新版本的服务来替换老版本服务的方式对应用进行升级，在滚动升级不能对应用的流量分发进行控制，因此无法采用受控地把生产流量逐渐导流到新版本服务中，也就无法控制服务升级对用户造成的影响。 2、灰度发布示例 下面采用 Istio 官方提供的 BookInfo 示例来验证灰度发布的流程，实现将 reviews 服务从 v1 版本逐渐灰度替换到 v2 版本，如将流量从 20% 逐步路由到 v2 版本，如下图所示： 图 6.13.2：灰度发布示例图 部署 v1 版本的服务 部署 BookInfo 示例中所有服务（productpage、details、reviews-v1、ratings） 1）只部署 revices 服务的 v1 版本，则需将 samples/bookinfo/platform/kube/bookinfo.yaml 中关于 reviews 服务的 v2 、v3版本去除，通过 kubectl apply 命令部署： kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml 2）通过 kubectl 命令行确认 pod 部署，可以看到只有 V1 版本的服务： $ kubectl get pods …… 3）为了能够外部访问，则为应用程序定义 Ingress 网关： kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml 4）用浏览器打开网址 http:///productpage，（EXTERNAL-IP 是 istio-ingress 的 External IP）来浏览应用的 Web 页面。由于 V1 版本的 reviews 服务并不会调用 rating 服务，因此可以看到 Product 页面显示的是不带星级的评价信息。 部署 v2 版本的 reviews 服务 1）部署 reviews 服务的 v2 版本之前，需要创建路由规则（VirtualService、DestinationRule），确保将所有流量都路由到 v1，避免新版本 v2 对线上用户造成影响。 ① 根据 samples/bookinfo/networking/virtual-service-all-v1.yaml 创建 VirtualService，将请求都路由到所有服务的 v1 版本： apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: productpage spec: hosts: - productpage http: - route: - destination: host: productpage subset: v1 --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - route: - destination: host: reviews subset: v1 --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: ratings spec: hosts: - ratings http: - route: - destination: host: ratings subset: v1 --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: details spec: hosts: - details http: - route: - destination: host: details subset: v1 --- 通过 kubectl apply 命令创建该 VirtualService： kubectl apply -f samples/bookinfo/networking/virtual-service-all-v1.yaml ② 根据 samples/bookinfo/networking/destination-rule-all.yaml 创建 DestinationRule，配合 VirtualService 实现流量路由到 reviews 服务的 v2 版本，将其中无关内容删除，最终配置如下： apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: productpage spec: host: productpage subsets: - name: v1 labels: version: v1 --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: reviews spec: host: reviews subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: ratings spec: host: ratings subsets: - name: v1 labels: version: v1 --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: details spec: host: details subsets: - name: v1 labels: version: v1 --- 通过 kubectl apply 命令创建该 DestinationRule： kubectl apply -f samples/bookinfo/networking/destination-rule-all.yaml 2）部署 reviews 服务的 v2 版本： kubectl apply -f samples/bookinfo/platform/kube/bookinfo-reviews-v2.yaml 此时系统中部署了 v1 和 v2 两个版本的 reviews 服务，但所有的业务流量都被路由到 reviews 服务的 v1 版本。 将测试流量导入到 v2 版本的 reviews 服务 基于 istio 请求内容的路由规则策略，将部分用户的流量切换到 reviews 服务的 v2 版本，以最小化模拟测试对已上线业务的影响。 1）修改 reviews 的 VirtualService的路由规则，将用户名为 jason 的流量路由到 reviews 服务的 v2 版本，即：可使用示例中的 samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml，内容如下： apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - match: - headers: end-user: exact: jason route: - destination: host: reviews subset: v2 - route: - destination: host: reviews subset: v1 通过 kubectl apply 命令创建该 VirtualService： kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml 2）以用户 jason 登录 productpage 页面，可以看到 reviews 服务的 V2 版本带星级的评价页面。而注销用户 jason，即：不登录用户，则是 v1 版本无星级的评价页面。由此，说明基于 istio 请求内容的路由规则策略生效，实现了将部分用户的流量切换到了 v2 版本。 将部分流量路由到 v2 版本的 reviews 服务 在线上模拟测试完成后，如果系统测试情况良好，可以通过规则将一部分用户流量导入到 v2 版本的服务中，进行小规模的灰度测试。 修改 reviews 的 VirtualService的路由规则，将 20% 的流量路由到 v2 版本。 备注：本例只是描述原理，因此为简单起见，将 20% 流量路由v2 版本，在实际操作中，更可能是先导入较少流量，然后根据监控的新版本运行情况将流量逐渐导入，如采用 5%，10%，20%，50% …的比例逐渐导入。 即：可使用示例中的 samples/bookinfo/networking/virtual-service-reviews-80-20.yaml，内容如下： apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - route: - destination: host: reviews subset: v1 weight: 80 - destination: host: reviews subset: v2 weight: 20 通过 kubectl apply 命令创建该 VirtualService： kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-80-20.yaml 此时，会有 20% 的流量路由到 v2 版本。 将所有生产流量路由到 v2 版本的 reviews 服务 如果新版本 v2 的服务运行正常，则可以将所有流量路由到 v2 版本。 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - route: - destination: host: reviews subset: v2 删除 v1 版本的 reviews 服务 待 v2 版本上线稳定运行后，删除 v1 版本的 reviews 服务。 （ 指 v1 版本的 reviews 服务的 Pod 名。 ） kubectl delete pod Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-10-30 22:37:19 "},"security/crd/security-crd.html":{"url":"security/crd/security-crd.html","title":"资源配置","keywords":"","body":"资源配置 Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-10-23 21:28:30 "},"security/crd/request-authentication.html":{"url":"security/crd/request-authentication.html","title":"RequestAuthentication","keywords":"","body":"RequestAuthentication Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-10-23 21:28:30 "},"security/crd/peer-authentication.html":{"url":"security/crd/peer-authentication.html","title":"PeerAuthentication","keywords":"","body":"PeerAuthentication Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-10-23 21:28:30 "},"security/crd/authorization-policy.html":{"url":"security/crd/authorization-policy.html","title":"AuthorizationPolicy","keywords":"","body":"AuthorizationPolicy Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-10-23 21:28:30 "},"extensibility/extending-envoy-proxy-with-webassembly.html":{"url":"extensibility/extending-envoy-proxy-with-webassembly.html","title":"基于 WASM 扩展 Envoy","keywords":"","body":"基于 WASM 扩展 Envoy Envoy WASM 介绍 WebAssembly 是一种沙盒技术，可用于扩展 Istio 代理（Envoy）的能力。Proxy-Wasm 沙盒 API 取代了 Mixer 作为 Istio 主要的扩展机制。 WebAssembly 沙盒的目标： 效率：这是一种低延迟，低 CPU 和低内存开销的扩展机制。 功能：这是一种可以执行策略，收集遥测数据和执行有效负载变更的扩展机制。 隔离：一个插件中程序的错误或是崩溃不会影响其它插件。 配置：插件的使用与其它 Istio API 一致的 API 进行配置，可以动态的配置扩展。 运维：扩展可以仅日志，故障打开或者故障关闭的方式进行访问和部署。 扩展开发者：可以用多种编程语言编写。 基于 Go 语言实现 Istio Envoy 的扩展 本示例是基于http_headers示例，来学习如何基于 Go 语言实现 Envoy WASM 的扩展，并应用于服务网格 Istio。 环境准备 安装 go 链接：https://golang.org/doc/install 安装 tinygo 链接：https://tinygo.org/getting-started/linux/ 提示：如果已有 go 环境，则不需要重复安装，tinygo 用于编译成 wasm 插件。tinygo 也可以从 github 直接下载解压，把解压后的 bin 目录加入到 PATH 目录。 下面以 MACOS 环境来安装 tinygo： % brew tap tinygo-org/tools % brew install tinygo 开发 WASM 开发 WASM 插件，理论上可以采用任何开发语言。目前已有不同语言实现的 Envoy Proxy WASM SDK 可供使用，如： proxy-wasm-cpp-sdk proxy-wasm-rust-sdk AssemblyScript proxy-wasm-go-sdk 本文示例采用由 tetrate 开发的 Go SDK，以http_headers示例进行举例。 下载http_headers示例代码。 通过 TinyGo 编译生成 WASM 文件。 在 http_headers 目录下执行 tinygo 命令编译，生成 http-headers.wasm。 % tinygo build -o ./http-headers.wasm -scheduler=none -target=wasi ./main.go 挂载 WSAM 文件 将 WASM 文件挂载到目标 Pod 的 Sidecar(即：istio-proxy)容器中。 以文件的方式，创建 ConfigMap。 % kubectl create cm http-headers-wasm --from-file=http-headers.wasm 将 WASM 文件挂载到目标 Pod 的 Sidecar 容器中，即：修改 Deployment，为其添加如下注解： sidecar.istio.io/userVolume: '[{\"name\":\"wasmfilters-dir\",\"configMap\": {\"name\": \"http-headers-wasm\"}}]' sidecar.istio.io/userVolumeMount: '[{\"mountPath\":\"/var/local/lib/wasm-filters\",\"name\":\"wasmfilters-dir\"}]' istiod 会依据这 2 个注解，将名为 http-headers-wasm 的 configmap，挂载到 istio-proxy 容器的 /var/local/lib/wasm-filters 目录下。 % kubectl patch deployment go-httpbin -p '{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"sidecar.istio.io/userVolume\":\"[{\\\"name\\\":\\\"wasmfilters-dir\\\",\\\"configMap\\\": {\\\"name\\\": \\\"http-headers-wasm\\\"}}]\",\"sidecar.istio.io/userVolumeMount\":\"[{\\\"mountPath\\\":\\\"/var/local/lib/wasm-filters\\\",\\\"name\\\":\\\"wasmfilters-dir\\\"}]\"}}}}}' Pod 重新创建后，在 istio-proxy 的 /var/local/lib/wasm-filters 下可以查看到 http-headers.wasm 文件。 创建 EnvoyFilter 创建 EnvoyFilter 资源，用于加载 WASM 插件。 编写的 EnvoyFilter 如下： apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: http-headers-filter spec: configPatches: - applyTo: HTTP_FILTER match: context: SIDECAR_INBOUND proxy: proxyVersion: ^1\\.11.* listener: portNumber: 8080 filterChain: filter: name: envoy.filters.network.http_connection_manager subFilter: name: envoy.filters.http.router patch: operation: INSERT_BEFORE value: name: envoy.filters.http.wasm typed_config: \"@type\": type.googleapis.com/udpa.type.v1.TypedStruct type_url: type.googleapis.com/envoy.extensions.filters.http.wasm.v3.Wasm value: config: # root_id: add_header vm_config: code: local: filename: /var/local/lib/wasm-filters/http-headers.wasm runtime: envoy.wasm.runtime.v8 # vm_id: \"my_vm_id\" allow_precompiled: false workloadSelector: labels: app: go-httpbin 其中： proxyVersion 与 istio-proxy 版本保持一致。 filename 需要与前面Deployment中的 Annotation 保持一致。 workloadSelector 设置为目标 Pod 的 label。 验证 验证 Envoy扩展的WASM插件是否生效。 登录到其它服务网格的服务容器中，请求该服务中存在的URL，查看该服务的istio-proxy容器的日志。 参考资料： https://istio.io/latest/docs/concepts/wasm/ proxy-wasm-go-sdk Extending Envoy Proxy with Golang WebAssembly Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-09-23 20:01:33 "},"integration/registry/integration-registry.html":{"url":"integration/registry/integration-registry.html","title":"集成注册中心","keywords":"","body":"集成注册中心 Istio 作为服务网格领域主流的开源框架，为微服务提供了零侵入的流量管理、服务可观测性等方面的服务治理能力，解决了传统微服务架构体系（如：Spring Cloud 技术体系）存在的高侵入性问题，彻底释放出业务开发人员无需过度关注服务治理的烦恼。 随着服务网格的推进，越来越多的项目尝试向服务网格转型，将传统微服务架构下的服务向服务网格迁移，为了降低迁移风险，大多采取阶段性、平滑迁移。一般先将所有服务容器化，迁移到 Kubernetes 上管理，再将服务纳入到网格管理。但面对庞大的存量微服务项目来说，往往为了能够快速迁移，以享受 Istio 提供的各种服务治理能力，采取服务上容器、Kubernetes、纳入网格，但服务注册仍采用原有的注册方式，如：Eureka、Consul、Nacos 等。为此，就需要考虑在 Istio 中如何集成第三方注册中心的问题，本节将针对 Istio 中的服务注册展开讨论，并为集成注册中心提供思路和方案，供参考使用。 1、Istio 服务注册发现机制 Istio 中服务的注册与发现是在控制平面实现，主要由 Pilot 组件完成，准确来说应该是pilot-discovery,负责所有服务的注册发现。 2、服务注册中心对接方式 3、总结 参考资料： 如何将第三方服务中心注册集成到 Istio ？ Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-09-30 16:50:38 "},"integration/registry/integration-consul.html":{"url":"integration/registry/integration-consul.html","title":"集成 Consul","keywords":"","body":"集成 Consul Copyright © xcbeyond.cn 2021 all right reserved，powered by Gitbook Updated at 2021-09-30 16:50:38 "}}